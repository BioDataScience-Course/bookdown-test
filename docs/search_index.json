[
["index.html", "Science des données biologiques, UMONS Préambule", " Science des données biologiques, UMONS Philippe Grosjean &amp; Guyliann Engels 2018-11-21 Préambule Cet ouvrage couvrira, à terme, la matière des cinq cours de science des données enseignés aux biologistes de la Faculté des Sciences de l’Université de Mons (Belgique). La matière sera complétée progressivement à partir du premier cours prévu pour l’année académique 2018-2019. Cet ouvrage est conçu pour être utilisé de manière interactive en ligne. En effet, nous prévoyons d’y adjoindre des capsules (unités d’enseignement ciblant un et un seul concept) sous forme de vidéos, des démonstrations interactives, et des exercices sous forme de questionnaires interactifs également. Ces différents éléments ne sont, bien évidemment, utilisables qu’en ligne. "],
["vue-generale-du-cours.html", "Vue générale du cours", " Vue générale du cours Le premier cours intitulé Science des données I: visualisation et inférence qui est dispensé aux biologistes de second Bachelier en Faculté des Sciences de l’Université de Mons à partir de l’année académique 2018-2019 contient 25h de cours et 50h d’exercices en presentiel. Il nécessitera environ un tiers de ce temps (voir plus, en fonction de votre rythme et de votre technique d’apprentissage) en travail à domicile. Cette matière est divisée en 12 modules de sessions de 6h chacune en présentiel. Une première séance de 2h précèdera ces 12 modules afin d’installer les logiciels (SciViews Box, R, RStudio, Github Desktop), et de se familiariser avec eux. "],
["materiel-pedagogique.html", "Matériel pédagogique", " Matériel pédagogique Le matériel pédagogique, rassemblé dans ce syllabus interactif est aussi varié que possible. Vous pourrez ainsi piocher dans l’offre en fonction de vos envies et de votre profil d’apprenant pour optimiser votre travail. Vous trouverez: le présent ouvrage en ligne, des capsules, essentiellement sous forme de vidéos &lt; 10 min qui ciblent chacune un concept particulier (en cours de développement), des tutoriaux interactifs (réalisés avec un logiciel appelé learnr). Vous pourrez exécuter ces tutoriaux directement sur votre ordinateur, et vous aurez alors accès à des pages Web réactives contenant des explications, des exercices et des quizzs en ligne, des slides de présentations, des dépôts Github Classroom dans la section BioDataScience-Course (vous apprendrez ce que c’est très rapidement dès le premier module) pour réaliser et documenter vos travaux personnels. des renvois vers des documents externes en ligne, types vidéos youtube ou vimeo, des ouvrages en ligne en anglais ou en français, des blogs, des tutoriaux, des parties gratuites de cours Datacamp ou équivalents, des questions sur des sites comme “Stackoverflow” ou issues des “mailing lists” R, … Tout ce matériel est accessible à partir du site Web du cours, du présent syllabus interactif (et de Moodle pour les étudiants de l’UMONS). Ces derniers ont aussi accès au dossier SDD sur StudentTemp en Intranet à l’UMONS. Les aspects pratiques seront à réaliser en utilisant la ‘SciViews Box’, une machine virtuelle préconfigurée que nous installerons ensemble lors du premier cours1. Il vous faudra donc avoir accès à un ordinateur (sous Windows, MacOS, ou Linux peu importe, suffisamment puissant et connecté à Internet ou à l’Intranet UMONS). Enfin, vous pourrez poser vos questions par mail à l’adresse sdd@sciviews.org. Il existe tout de même des outils plus pointus pour obtenir de l’aide sur le logiciel R comme rseek.org, rdocumentation.org ou rdrr.io. Rien ne sert de chercher ’R’ dans Goggle.↩ "],
["comment-apprendre.html", "Comment apprendre?", " Comment apprendre? fortunes::fortune(&quot;brain surgery&quot;) # # I wish to perform brain surgery this afternoon at 4pm and don&#39;t know where # to start. My background is the history of great statistician sports # legends but I am willing to learn. I know there are courses and numerous # books on brain surgery but I don&#39;t have the time for those. Please direct # me to the appropriate HowTos, and be on standby for solving any problem I # may encounter while in the operating room. Some of you might ask for # specifics of the case, but that would require my following the posting # guide and spending even more time than I am already taking to write this # note. # -- I. Ben Fooled (aka Frank Harrell) # R-help (April 1, 2005) Version courte: en pratiquant, en faisant des erreurs ! Version longue: aujourd’hui –et encore plus à l’avenir– les données sont complexes et ne se manipulent plus simplement avec un tableur comme Microsoft Excel. Vous allez apprendre à maitriser des outils professionnels, ce qui sous-entend qu’ils sont très puissants mais aussi relativement complexes. La méthode d’apprentissage que nous vous proposons a pour objectif prioritaire de vous faciliter la tâche, quelles que soient vos aptitudes au départ. Envisagez votre voyage en science des données comme l’apprentissage d’une nouvelle langue. C’est en pratiquant, et en pratiquant encore sur le long terme que vous allez progresser. La formation s’étale sur quatre années, et est répartie en cinq cours de difficulté croissante pour vous aider dans cet apprentissage progressif et sur la durée. N’hésitez pas à expérimenter, tester, essayer des nouvelles idées (même au delà de ce qui sera demandé dans les exercices) et n’ayez pas peur de faire des erreurs. Vous en ferez, … beaucoup … nous vous le souhaitons! En fait, la meilleure manière d’apprendre, c’est justement en faisant des erreurs, et puis en mettant tout en oeuvre pour les comprendre et les corriger. Donc, si un message d’erreur, ou un “warning” apparait, ne soyez pas intimidé. Prenez une bonne respiration, lisez-le attentivement, essayez de le comprendre, et au besoin faites-vous aider: la solution est sur le Net, ‘Google1 est votre ami’! Il existe tout de même des outils plus pointus pour obtenir de l’aide sur le logiciel R comme rseek.org, rdocumentation.org ou rdrr.io. Rien ne sert de chercher ’R’ dans Goggle.↩ "],
["evaluation.html", "Evaluation", " Evaluation L’évaluation sera basée sur une somme de petites contributions qui matérialiseront votre progression sur le long terme. Avec cette évaluation, nous souhaitons vous gratifier chaque fois que vous franchirez des étapes, plutôt que de vous sanctionner lorsque vous bloquez. Donc, pour une note finale sur 20: 3 points pour la restitution des capsules et votre participation en présentiel. Au début de chaque séance, nous discuterons des notions que vous aurez à préparer par avance, et votre participation sera évaluée. 6 points pour un quizz final. 11 points pour l’évaluation d’un des rapports d’analyse de données (choisi au hasard en fin de cours). Enfin, vous pourrez éventuellement encore gagner un point bonus pour une participation remarquable, ou tout autre élément à valoriser (site web personnel et/ou blog exceptionnel, aide des autres étudiants, etc.). Ceci étant à l’appréciation des enseignants. Le matériel dans cet ouvrage est distribué sous licence CC BY-NC-SA 4.0. "],
["intro.html", "Module 1 Introduction", " Module 1 Introduction Objectifs Appréhender ce qu’est la science des données et les (bio)statistiques. S’initier à des outils de base (SciViews Box, RStudio, Markdown, Git, GitHub). Se sensibiliser à l’importance d’une présence web au niveau professionnel. "],
["donnees.html", "1.1 Le monde il y a 25 ans", " 1.1 Le monde il y a 25 ans Il y a 25 ans, pas d’internet, pas de smartphone. Essayez d’imaginer ce que serait votre vie aujourd’hui si ces outils qui font partie de votre quotidien n’existaient pas. Les révolutions industrielles: 1770 (1756) révolution 1: mécanisation 1870 révolution 2: maîtrise de l’énergie 1970 (1979) révolution 3: informatique 1990 révolution 4: internet (1990 Web, 1992 ISOC = Internet society, 1993 = premier navigateur web) 2000 révolution 5: numérique. GAFA = Google - Apple - Facebook - Amazon + Microsoft = GAFAM aux USA et BATX en Chine = Baidu - Alibaba - Tencent - Xiaomi. Aussi NATU = Netflix - Airbnb - Tesla - Uber. 2010 révolution 6: NBIC = nanotechnologies - biotechnologies - informatique - sciences cognitives. 2020 = date prévue pour que l’ordinateur ait la même puissance de traitement de l’information que le cerveau humain 2030 = trans-humanisme: ordinateur plus puissant que l’homme et le remplacera probablement dans de nombreuses tâches. Valeur estimée des données et informations mises à disposition par les utilisateurs du net: 1000 milliards de dollar par an (écrivez ce nombre en chiffres pour vous donner une meilleure idée de ce que cela représente) ! En 2020, quantité d’information ajoutée sur le net: 1000 milliards de milliards par semaine (écrivez ce nombre en chiffres également). Comparaison de puissance de traitement du cerveau humain versus un ordinateur: 89 milliards de neurones, mais travail en multitâche alors qu’un processeur est monotâche =&gt; difficile à comparer. Une étude a montré en 2017 que l’un des 5 ordinateurs les plus puissants a été capable de simuler le fonctionnement d’environ 1% du cerveau humain en 1 sec. Il lui a fallu 40 min de calcul pour y arriver. Intel estime que l’évolution permettra d’égaler le cerveau humain en terme de vitesse de traitement vers 2020. Consommation électrique du supercalculateur: se mesure en mégawatts, alors que le cerveau humain consomme 12-13W seulement! “Le transhumanisme est une approche interdisciplinaire qui nous amène à comprendre et à évaluer les avenues qui nous permettrons de surmonter nos limites biologiques par les progrès technologiques. Les trans-humanistes cherchent à développer les possibilités techniques afin que les gens vivent plus longtemps et …” Vous pouvez maintenant avoir un aperçu de l’importance d’avoir des outils performants afin d’appréhender les données dont le nombre croit de manière exponentielle. Pour ce cours de sciences des données, plusieurs outils puissants sont mis à votre disposition (Vous trouverez sur l’hyperlien suivant, un poster présentant la philosophie du cours https://github.com/BioDataScience-Course/RencontresRRennes2018) "],
["decouverte-des-outils.html", "1.2 Découverte des outils", " 1.2 Découverte des outils La science des données est complexe et requiert d’employer des outils performants. Nous avons sélectionné ces outils pour vous. 1.2.1 Machine virtuelle La SciViews Box est une machine virtuelle (un ordinateur complet, mais totalement indépendant du matériel -le hardware- et qui peut être déployé sur pratiquement n’importe quel ordinateur physique). Cette SciViews Box est complètement configurée et dédiée à la sciences des données biologiques. Elle contient tout ce qu’il faut pour importer et analyser vos données, et ensuite écrire des rapports ou d’autres documents prêts à publication ou à présentation. Elle vous servira également à collaborer avec d’autres chercheurs qui peuvent facilement utiliser exactement la même machine virtuelle (aspect reproductible de vos analyses). Des explications détaillées se trouvent dans l’annexe A dédiée à l’installation et la configuration de la SciViews Box. Figure 1.1: Logo de la SciViews Box Une fois connecté au compte sv dans la machine virtuelle, réalisez l’activité : Découverte de la machine virtuelle https://github.com/BioDataScience-Course/sdd_lesson/blob/master/sdd1_01/presentations/sdd1_01_svbox.pdf Après avoir réalisé l’activité, un document récapitulatif est mis à votre disposition : https://github.com/BioDataScience-Course/sdd_lesson/blob/master/sdd1_01/exercises/sdd1_01_svbox.Rmd Des explications détaillées se trouvent dans l’annexe @ref(svbox_use) dédiée à l’utilisation de la SciViews Box. 1.2.2 RStudio RStudio est l’outil au sein de la SciViews Box que vous allez utiliser le plus fréquemment durant ce cours. Il fournit un environnement complet et optimisé pour réaliser vos analyses, vos graphiques et vos rapports. RStudio travaille main dans la main avec le logiciel R qui effectue l’ensemble des traitements. L’interface utilisateur de RStudio est divisée en quatre zones importantes (A-D) avec une barre d’outils générale par dessus : A. Une zone d’édition B. Plusieurs onglets sont présents comme Environnement, History ou encore Connections. Par exemple, les différents items (on parle d’objets) chargés en mémoire dans R sont visibles dans l’onglet Environnement (mais pour l’instant, il n’y a encore rien). C. La Console est l’endroit où vous pouvez entrer des instructions dans R pour manipuler vos données D. Une zone multiusage où vous pouvez manipuler vos fichiers (Files), vos graphiques (Plots), les différents “addins” de R (on parle de Packages), accéder aux pages d’aide (Help) ou encore, visualiser le rendu final de vos rapports (Viewer). Des explications détaillées se trouvent dans l’annexe B.1 qui présente les bases de l’utilisation de RStudio. Vous avez également à votre disposition un aide-mémoire afin d’appréhender cette interface RStudio IDE Cheat Sheet. Pour en savoir plus RStudio. Site Web de RStudio comprenant un ensemble de ressources en anglais RStudio, un environnement de développement pour R. Brève explication de RStudio en français. RStudio : sa vie, son oeuvre, ses ressources. Un autre site Web consacré à RStudio en français. 1.2.3 Markdown Dans RStudio, les rapports sont rédigés en utilisant le langage Markdown dans la zone d’édition. Il permet de baliser le texte pour indiquer le sens des différentes parties (par exemple, pour indiquer les différents niveaux de titres). Il permet de se concentrer sur l’écriture dans un premier temps en dissociant le fond de la mise en forme. En effet, vous vous préoccupez de l’aspect final du document dans un second temps, et même, vous pouvez changer radicalement d’avis pratiquement sans rien changer dans le texte (par exemple, il est possible de passer d’une page Web à un document PDF ou Word, ou même encore à une présentation). Markdown est relativement simple et intuitif à l’usage, même si un petit effort est nécessaire, naturellement, au début. Quels sont les commandes et instructions indispensables lorsque l’on rédige un rapport ? Des titres et sous-titres, une mise en évidence (texte en italique ou en gras), des listes,… Il ne faut au final que très peu de commandes pour réaliser un rapport de qualité avec une mise en page sobre et épurée qui caractérise les travaux professionnels. Vous avez à votre disposition deux aide-mémoires pour apprendre Markdown : R Markdown Cheat Sheet et R Markdown Reference Guide plus détaillé. Après avoir rédiger votre document, vous devez cliquer sur le bouton Preview ou Knit (selon le type de document édité) dans la barre d’outils de la zone d’édition pour obtenir la version finale formatée. Une fois connecté au compte sv dans la machine virtuelle, réalisez l’activité : Découverte de RStudio et R Markdown https://github.com/BioDataScience-Course/sdd_lesson/blob/master/sdd1_01/presentations/sdd1_01_markdown.pdf Après avoir réalisé l’activité, un document récapitulatif est mis à votre disposition : https://github.com/BioDataScience-Course/sdd_lesson/blob/master/sdd1_01/exercises/sdd1_01_markdown.Rmd Pour en savoir plus Markdown. Explication en anglais de l’intérêt d’employer Markdown ainsi que la syntaxe à employer. Rédigez en Markdown ! Un guide pour bien commencer avec Markdown Le Markdown comme langage d’écriture universel ? Comment écrire confortablement et professionnellement ? Le Markdown !. Utilisation de Markdown afin de revenir à l’essence de la rédaction. Écrire tout simplement – Introduction à Markdown. Pourquoi utiliser Markdown ? 1.2.4 Gestionnaire de version Lors de la rédaction de travaux un petit peu conséquents, comme un travail de fin d’étude, une publication scientifique ou un rapport volumineux, on se retrouve rapidement avec plusieurs fichiers correspondant à des états d’avancements du travail : TFE_final TFE_final1 TFE_final2 TFE_final3 TFE_final… TFE_final99 Lors de différents essais, on va avoir tendance à tout garder dans différents fichiers afin de ne rien supprimer d’important. Cette pratique bien que très courante comporte le gros désavantage de prendre énormément de place sur le disque de votre ordinateur et de n’être pas pratique. Les questions suivantes peuvent se poser : Que se cache-t-il dans la version TFE_finalX ? Après un mois sans travailler sur le projet, seriez-vous encore capable de faire facilement la différence entre TFE_final2 et le TFE_final3 ? Cela se complique encore plus lorsque plusieurs personnes collaborent sur un même projet. Ils vont, par exemple, s’échanger par email différentes versions du travail avec chacun qui y place ses commentaires et modifie différentes parties du texte. Cela peut donner quelque chose comme ceci : TFE_final TFE_final1 TFE_final1_jacques TFE_final1_pierre TFE_final2 TFE_final2_jules TFE_final… TFE_final99 Dans quel fichier se trouve la dernière version de chaque personne ayant collaboré sur le projet ? Une petit peu dans différents fichiers, sans doute. Différents outils informatiques existent pour faciliter le travail collaboratif comme : Le partage de fichiers en ligne (Dropbox, Google Drive, One Drive). Ces espaces de stockage sur le “cloud” ne règlent toujours pas le problème de collaboration sur le même fichier. L’utilisation d’un programme d’édition collaboratif en temps réel (Etherpad, Google Drive - Docs, Gobby). Il est possible de travailler en même temps sur un même fichier. Cette option ne règle pas le problème du retour vers une ancienne version. Lorsqu’une modification a été réalisée l’ancienne version est tout simplement écrasée. La meilleure combinaison pour gérer ses versions et collaborer : Git et GitHub. Ces outils sont plutôt considérés comme écrits par et pour des geeks. Cependant, ils permettent de gérer et collaborer de manière efficace sur un même projet contenant du code ou non, et des interfaces facilitant leur utilisation apparaissent comme GitHub Desktop, ou même, les outils Git intégrés dans RStudio. 1.2.4.1 Git La gestion de versions est gérée par Git. Cet outil va remplacer les nombreuses copies d’un même fichier par une sorte d’arbre que l’on peut représenter schématiquement comme ci-dessous : Représentation de la gestion de fichiers via Git Comme vous pouvez le voir ci-dessus, on peut suivre la progression de notre projet via un nombre d’étapes successives représentées sur le schéma par des boules bleues. Chaque étape capture l’état de notre projet au moment où nous avons décidé de l’enregistrer. Pour enregistrer une nouvelle version de votre projet, vous réalisez un commit qui sera accompagné d’un message spécifiant les modifications apportées. Git comprend de nombreux outils très intéressant pour la gestion de versions que vous utiliserez par la suite. 1.2.4.2 GitHub Un réseau social a été conçu autour de Git pour sauvegarder vos projets sur le “cloud”, les partager et collaborer avec d’autres personnes. Ce système se nomme GitHub (tout comme Facebook ou LinkedIn). GitHub rassemble donc “Git”, la gestion de version et “Hub” relatif au réseau. D’autres réseaux équivalents existent comme Gitlab ou Bitbucket, mais dans ce cours, nous utiliserons GitHub ensemble, sachant que les notions apprises ici seront réutilisables ailleurs. Lorsque l’on travaille seul tout en utilisant GitHub, l’évolution de notre projet va ressembler à l’arbre ci-dessous : Représentation des versions successives d’un projet avec GitHub. On réalise un envoi (push) lorsque l’on souhaite synchroniser nos changements locaux avec la version sur le “cloud”. Plusieurs commits peuvent être envoyés avec un seul push sur le réseau, et c’est d’ailleurs généralement comme cela que l’on procède. L’inverse (rapatrier localement les changements que d’autres collaborateurs ont envoyé sur la version réseau de notre projet) s’appelle faire un “pull”. L’avantage principal de GitHub ne réside pas vraiment dans la possibilité de réaliser une sauvegarde en ligne mais plutôt dans la possibilité de collaborer avec d’autres personnes présentes sur ce réseau comme l’illustre la figure ci-dessous. Deux scientifiques (les versions représentées par des boules bleues et des boules vertes) collaborent sur un même projet que l’on appelle un dépôt (repository en anglais) lorsqu’il est en ligne. Le premier chercheur (boules bleues) va initier le dépôt et réaliser un “push”&quot; pour rendre son travail accessible sur le réseau (boules oranges). Son collaborateur (boules vertes) va clôner (clone en anglais) le dépôt sur son ordinateur afin d’y travailler également en local sur son PC. Après avoir fait des changements, il réalise également un push sur le réseau. Le premier scientifique, avant de travailler à nouveau sur le projet, va donc réaliser un pull afin d’obtenir en local l’ensemble des modifications fournies par son ou ses collaborateurs, et ensuite après modifications en local il effectuera à nouveau un “pus”. Différentes versions d’un projet sur GitHub lorsque deux personnes différentes collaborent sur le même dépôt. Vous venez d’apprendre le B-A-BA de la terminologie nécessaire à la bonne compréhension de Git et GitHub : repository : espace de stockage sous gestion de version Git. commit : enregistrer une version du projet. clone : créer un double local d’un dépôt GitHub. push : envoyer ses modifications locales vers le dépôt GitHub. pull : rapatrier les modifications que les autres utilisateurs ont appliqué dans le dépôt GitHub vers sa propre version locale. Ceci n’est qu’une explication très succincte. Vous trouverez plus de détails dans les liens ci-dessous et dans les Appendices. Il est, par exemple, possible de travailler sur une version en parallèle d’un dépôt original pour lequel on n’a pas de droits en écriture. Dans ce cas, il faudra faire une copie dans notre propre compte GitHub du dépôt. Cela s’appelle faire un fork. Il n’est pas possible de faire un push vers le dépôt d’origine puisqu’on n’a pas les droits en écriture. Dans ce cas, on fera un pull request, suggérant ainsi à l’auteur d’origine que nous avons fait des modifications qui pourraient l’intéresser. Si c’est effectivement le cas, il pourra accepter notre “pull request” et intégrer nos suggestions dans le dépôt d’origine. Vous serez amenés à “forker” des dépôts GitHub pour vos exercices, et vous effectuerez également un “pull request” lorsque vous serez suffisamment aguerris avec les autres techniques de gestion de vos projets sous Git et GitHub. Pour en savoir plus Gérez vos codes sources avec Git. Explication en français sur l’utilisation de Git. Quel logiciel de gestion de versions devriez-vous utiliser ?. Explication en français sur l’utilisation des logiciels de gestion de versions. Git : comprendre la gestion de versions. Explication en français sur ce qu’est Git et comment cela s’utilise en pratique. Introduction en anglais de GitHub dans RStudio à l’aide d’une vidéo. Happy Git and GitHub for the useR. Complet, mais un peu technique et en anglais. Installation et première utilisation de Git et GitHub dans R. En anglais. Git. Site en anglais comprenant toute la documentation de Git. GitHub pour les nuls : pas de panique, lancez-vous !. 1.2.4.3 GitHub Classroom GitHub Classroom est une extension de GitHub qui facilite le travail avec GitHub dans le contexte d’exercices à réaliser dans le cadre d’un cours. Vous serez amené à cloner et modifier des dépôts issus de GitHub Classroom pour réaliser vos exercices. Ces dépôts seront privés. Cela signifie que, seuls vous-mêmes et vos enseignants auront accès à ces dépôts. A la fin de la formation, tous ces dépôts seront détruits. Donc, si vous voulez les conserver, il faudra les “forker” sur votre propre compte. Rassurez-vous : nous vous préviendrons avant de faire le ménage ! Maintenant que vous comprenez mieux avec quels outils informatiques nous allons travaillez, vous pouvez passer à votre premier exercice pour découvrir la SciViews Box, RStudio, Markdown, Git et GitHub : vous allez réaliser un site web professionnel en ligne… "],
["site.html", "1.3 Site web professionnel", " 1.3 Site web professionnel De nos jours, un nombre important de données sont collectées via notre activité sur le web. A la fin de vos études, vous serez amenés à rechercher un travail (dans le milieu de la recherche universitaire, dans la recherche en entreprise, dans les métiers de l’éco-conseils, dans l’industrie, etc.). Tous les recruteurs utilisent actuellement les données que l’on a semé sur le net afin de réaliser un profil détaillé sur nous. Il est donc de votre intérêt d’avoir le meilleur profil possible sur Internet… et qu’il apparaisse comme à la fois sérieux, dynamique et professionnel. Nous allons vous y aider ! Les réseaux sociaux sont une mine d’or sur nos habitudes de vie comme Facebook, Twitter ou encore Instagram. On peut parler de l’image numérique d’une personne. Afin de mettre l’accent sur les compétences professionnelles, de plus en plus de personnes utilisent des réseaux dédiés comme LinkedIn ou encore, ont recours à un site web personnel professionel (par exemple : http://phgrosjean.sciviews.org/, http://www.guyliann.be/). Dans le cadre de ce premier module, vous allez réaliser votre premier site web professionnel (contenant également une section “blog” que vous pourrez alimenter, entre autres, avec vos considérations concernant vos cours, les analyse de données, et autres, …). Ce site sera immédiatement disponible sur le web et ce, de manière entièrement gratuite ! Afin de mener à bien ce premier projet, divers outils vous seront nécessaires. Ces outils seront complétés plus tard par d’autres pour que vous ayez une boite à outils complète pour vos futures analyses de données dans un contexte professionnel. A vous de jouer Maintenant que vous avez appréhendé les différents outils, lancez vous dans la création de votre site web professionnel via l’adresse suivante : https://github.com/BioDataScience-Course/blogdown.source. Les instructions détaillées sont dans le fichier README.md sur ce dépôt. Lorsque vous éditez votre site blogdown, faites attention à ceci : Hugo (le logiciel qui compile le site Web) n’est pas très tolérant. Vous devez respecter la syntaxe qu’il impose. Faites attention aux détails, comme les virgules indispensables pour séparer les items dans une liste. Par exemple, dans la liste [interests] qui apparaît dans content/home/about.md, si vous ajoutez des items comme ceci, Hugo ne compilera plus le site correctement. interests = [ &quot;Biology&quot;, &quot;Ecology&quot; &quot;Movies&quot; ] Tous les items doivent être séparés par une virgule, ce qui n’est pas le cas de &quot;Ecology&quot; et &quot;Movies&quot;. La forme correcte est présentée ci-dessous : interests = [ &quot;Biology&quot;, &quot;Ecology&quot;, &quot;Movies&quot; ] Ne cliquez pas sur le bouton Preview lorsque vous éditez un fichier .md. Normalement dans RStudio, c’est effectivement avec ce bouton qu’on obtient une visualisation du rendu final du document. Mais avec blogdown, il faut passer par Addins -&gt; BLOGDOWN -&gt; Serve Site. De plus, si vous avez cliqué sur Preview, le fichier .html généré peut empêcher la bonne construction du site. Donc, effacez tous les fichiers .html correspondant aux .md présents dans un sous-dossier de content. S’il vous semble que la compilation du site est bloquée, redémarrez R. Dans le menu de RStudio, faites Session -&gt; Restart R, ensuite relancez la compilation du site avec Addins -&gt; BLOGDOWN -&gt; Serve Site. Traquez un message d’erreur éventuel qui apparaîtrait dans l’onglet Console, et sinon, vérifiez la syntaxe dans config.toml, les entêtes de tous les fichiers .md, ainsi que l’absence de fichiers .htmlassociés. N’oubliez pas de “commiter” vos changements à partir de l’onglet Git dans RStudio. A ce stade, vous avez mis à jour les sources de votre site, mais pas votre site lui-même. Pour mettre à jour votre site en ligne, il faut aussi “commiter” les changements dans &lt;your_login&gt;.github.io. Ouvrez ce dernier dépôt dans Github Desktop et effectuez-y un “commit”. Ensuite, naviguez vers http://&lt;your_login&gt;.github.io pour vérifier ce qui apparaît en ligne. Attention, il se peut que le cache du navigateur ou d’un routeur vous renvoie encore une ancienne version. Rafraîchissez la page deux ou trois fois pour voir apparaître la dernière version de votre oeuvre. Pour en savoir plus Le manuel de blogdown (en anglais). Des exemples de sites web réalisés avec blogdown. "],
["visu1.html", "Module 2 Visualisation I", " Module 2 Visualisation I Objectifs Découvrir –et vous émerveiller de– ce que l’on peut faire avec le logiciel R (R Core Team 2018) Savoir réaliser différentes variantes d’un graphique en nuage de points dans R avec la fonction chart() Découvrir le format R Markdown (Allaire et al. 2018) et la recherche reproductible Intégrer ensuite des graphiques dans un rapport et y décrire ce que que vous observez Comparer de manière critique un flux de travail “classique” en biologie utilisant Microsoft Excel et Word avec une approche utilisant R et R Markdown ; Prendre conscience de l’énorme potentiel de R Prérequis Si ce n’est déjà fait, vous devez installer et vous familiariser avec la ‘SciViews Box’, RStudio, Markdown. Vous devez aussi maîtriser les bases de Git et de GitHub (avoir un compte GitHub, savoir cloner un dépôt localement, travailler avec GitHub Desktop pour faire ses “commits”, “push” et “pull”). L’ensemble de ces outils a été abordé lors de la création de votre site personnel professionnel du module 1. Avant de poursuivre, vous allez devoir découvrir les premiers rudiments de R afin de pouvoir réaliser par la suite vos premiers graphiques. Pour cela, vous aurez à lire attentivement et effectuer tous les exercices de deux tutoriels2. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience::run(&quot;02a_base&quot;) Ensuite, vous pouvez également parcourir le tutoriel qui vous permettra de découvrir R sur base d’une analyse concrète (cliquez dans la fenêtre Console de RStudio et appuyez sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel) : BioDataScience::run(&quot;02b_decouverte&quot;) (BioDataScience est un package R spécialement développé pour ce cours et que vous avez dû installer lors de la configuration de votre SciViews Box, voir Appendice A.3.3). Références "],
["nuage-de-points.html", "2.1 Nuage de points", " 2.1 Nuage de points Dès que vous vous sentez familiarisé avec les principes de base de R, vous allez pouvoir réaliser assez rapidement des beaux graphiques. Par exemple, si vous souhaitez représenter une variable numérique en fonction d’une autre variable numérique, vous pouvez exprimer cela sous la forme d’une formule3 \\[y \\sim x\\] que l’on peut lire “y en fonction de x”. Pour les deux variables numériques x et y, la représentation graphique la plus classique est le nuage de points (voir Fig. 2.1 pour un exemple). Figure 2.1: Exemple de graphique en nuage de points. Des éléments essentiels sont ici mis en évidence en couleurs (voir texte). Les éléments indispensables à la compréhension d’un graphique en nuage de points sont mis en évidence à la Fig. 2.1 : Les axes avec les graduations (en rouge), les labels et les unités des axes (en bleu). Les instructions dans R pour produire un tel nuage de point sont : # Chargement de SciViews::R SciViews::R # Importation du jeu de données (urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;)) # # A tibble: 421 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Fishe… 9.9 10.2 5 NA 0.522 0.478 # 2 Fishe… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Fishe… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Fishe… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Fishe… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Fishe… 10.5 11.1 5 NA 0.610 0.551 # 7 Fishe… 11 11 5.2 NA 0.672 0.605 # 8 Fishe… 11.1 11.2 5.7 NA 0.703 0.628 # 9 Fishe… 9.4 9.2 4.6 NA 0.413 0.375 # 10 Fishe… 10.1 9.5 4.7 NA 0.449 0.398 # # ... with 411 more rows, and 12 more variables: integuments &lt;dbl&gt;, # # dry_integuments &lt;dbl&gt;, digestive_tract &lt;dbl&gt;, # # dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, dry_gonads &lt;dbl&gt;, # # skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, spines &lt;dbl&gt;, # # maturity &lt;int&gt;, sex &lt;fct&gt; # Réalisation du graphique chart(data = urchin, height ~ weight) + geom_point() Figure 2.2: Taille (hauteur du test) d’oursins en fonction de leur masse. La fonction chart() n’est pas accessible dans R de base, mais l’extension chargée via l’instruction SciViews::R rend cette fonction disponible. Elle requiert comme argument le jeu de donnée (data = urchin, c’est un objet dataframe ou tibble dans le langage de R), ainsi que la formule à employer dans laquelle vous avez indiqué le nom des variables que vous voulez sur l’axe des ordonnées à gauche et des abscisses à droite de la formule, les deux membres étant séparés par un “tilde” (~). Vous voyez que le jeu de données contient beaucoup de variables (les titres des colonnes du tableau en sortie). Parmi toutes ces variables, nous avons choisi ici de représenter height en fonction de weight, la hauteur en fonction de la masse des oursins. Jusqu’ici, nous avons spécifié ce que nous voulons représenter, mais pas encore comment (sous quelle apparence), nous voulons les matérialiser sur le graphique. Pour un nuage de points, nous voulons les représenter sous forme de … points ! Donc, nous devons ajouter la fonction geom_point() pour indiquer cela. 2.1.1 Le nuage de points en vidéo Vous trouverez une vidéo ci-dessous vous expliquant la création du nuage de points dans R sur ce jeu de données, en analysant d’autres variables. Cette vidéo ne vous a montré que les principaux outils disponibles lors de la réalisation de graphiques. Soyez curieux et expérimentez par vous-même ! A vous de jouer Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant le nuage de points : BioDataScience::run(&quot;02c_nuage_de_points&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel) 2.1.2 Echelles d’un graphiques Vous devez être vigilant lors de la réalisation d’un nuage de point particulièrement sur l’étendue des valeurs présentées sur vos axes. Vous devez utilisez votre expertise de biologiste pour vous posez les deux questions suivantes : Est ce que l’axe représente des valeurs plausibles de hauteurs et de masses de ces oursins appartenant à l’espèce Paracentrotus lividus ? Quels est la précision des mesures effectuées ? Dans certains cas, la forme du nuage de points peut être distendu par la présence de valeurs aberrantes. Ce n’est pas le cas ici, mais nous pouvons le simuler en distendant artificiellement soit l’axe X, soit l’axe Y, soit les deux : Figure 2.3: Piège du nuage de points. A) graphique initial montrant la variation de la hauteur [mm] en fonction de la masse [g]. B) graphique A avec la modification de l’échelle de l’axe X. C) Graphique A avec une seconde modification de l’axe X. D) Graphique A avec modification simultanée des deux axes. 2.1.3 Transformation des données Vous avez la possibilité d’appliquer une transformation de vos données (il est même conseillé de le faire) afin qu’elles soient plus facilement analysables. Par exemple, il est possible d’utiliser des fonctions de puissance, racines, logarithmes, exponentielles4 pour modifier l’apparence du nuage de points dans le but de le rendre plus linéaire (car il est plus facile d’analyser statistiquement des données qui s’alignent le long d’une droite). Par exemple, sur nos données de hauteurs et masses d’oursins, la transformation double-logarithmique (log(x) et log(Y)) fonctionne très bien pour rendre le nuage de points plus linéaire : # Réalisation du graphique de la hauteur en fonction de la masse a &lt;- chart(urchin, height ~ weight) + geom_point() # Application du logarithme sur les deux variables représentées b &lt;- chart(urchin, log10(height) ~ log10(weight)) + geom_point() + labs(x = &quot;log(Masse totale [g])&quot;, y = &quot;log(Hauteur du test [mm])&quot;) # Assemblage des graphiques combine_charts(list(a, b)) Figure 2.4: A) Hauteur [mm] en fonction de la masse [g] d’oursins violets. B) Logarithme en base 10 de la hauteur [mm] en fonction du logarithme en base 10 de la masse [g] de ces mêmes oursins. Pièges et astuces RStudio permet de récupérer rapidement des instructions à partir d’une banque de solutions toutes prêtes. Cela s’appelle des snippets. Vous avez une série de snippets disponibles dans la SciViews Box. Celui qui vous permet de réaliser un graphique en nuage de points s’appelle .cbxy (pour chart -&gt; bivariate -&gt; xy-plot). Entrez ce code et appuyez ensuite sur la tabulation dans un script R, et vous verrez le code remplacé par ceci dans la fenêtre d’édition : chart(data = DF, YNUM ~ XNUM) + geom_point() Vous avez à votre disposition un ensemble de snippets que vous pouvez retrouver dans l’aide-mémoire consacré à SciViews. Vous avez également à votre disposition l’aide-mémoire sur la visualisation des données (Data Visualization Cheat Sheet) qui utilise la fonction ggplot() plutôt que chart() et une interface légèrement différente pour spécifier les variables à utiliser pour réaliser le graphique (aes(x = ..., y = ...)). A vous de jouer Une nouvelle tâche va vous être demandée ci-dessous en utilisant GitHub Classroom 1.2.4.3. Cette tâche est un travail individuel. Une fois votre assignation réalisée, faites un clone local de votre dépôt et placez-le dans le sous-dossier projects de votre dossier partagé avec la SciViews Box shared. Vous aurez alors un nouveau projet RStudio B.1.1 Les instructions R que vous expérimentez dans un learnR peuvent être employées également dans un script d’analyse. Sur base du jeu de données urchin_bio, explorez différents graphiques en nuages de points. Utilisez l’URL suivante pour accéder à votre tâche : https://classroom.github.com/a/eYrXLy_u Inspirez-vous du script dans le dépôt sdd1_iris. Vous devez commencer par faire un “fork” du dépôt, puis un clone sur votre ordinateur en local pour pouvoir l’utiliser. https://github.com/BioDataScience-Course/sdd1_iris Prêtez une attention toute particulière à l’organisation d’un script R. En plus des instructions R, il contient aussi sous forme de commentaires, un titre , la date de la dernière mise à jour, le nom de l’auteur, et des sections qui organisent de façon claire le contenu du script. A ce sujet, vous trouverez des explications détaillées concernant l’utilisation des scripts R dans l’annexe B.1.2. Pour en savoir plus Visualisation des données dans R for Data Science. Chapitre du livre portant sur la visualisation des données, en anglais. ggplot2 nuage de point. Tutoriel en français portant sur l’utilisation d’un nuage de points avec le package ggplot2 et la fonction geom_point(). Fundamentals of Data Visualization. Un livre en anglais sur les fondamentaux de la visualisation graphique. R Graphics Cookbook - Chapter 5: Scatter Plots. Un chapitre d’un livre en anglais sur l’utilisation du nuage de points. geom_point(). La fiche technique de la fonction (en anglais). Testez vos acquis Dans la fenêtre Console de RStudio, entrez l’instruction suivante et puis appuyez sur la touche Entrée pour ouvrir le tutoriel de challenge concernant le nuage de points : BioDataScience::run(&quot;02d_np_challenge&quot;) N’oubliez pas de vous enregistrer (login GitHub et email UMONS) au début, et d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel. Dans R, une formule permet de spécifier les variables avec lesquelles on souhaite travailler, et leur rôle. Par exemple ici, la variable x sur l’axe des abscisses et la variable y sur l’axe des ordonnées.↩ Pour les proportions (prop) ou les pourcentages (perc) (valeurs bornées entre 0 et 1 ou 0 et 100%) la transformation arc-sinus est souvent utilisée : \\(prop′ = \\arcsin \\sqrt{prop}\\) ou \\(perc′ = \\arcsin \\sqrt{perc / 100}\\).↩ "],
["graphiques-dans-r-markdown.html", "2.2 Graphiques dans R Markdown", " 2.2 Graphiques dans R Markdown Un fichier R Markdown est un fichier avec une extension .Rmd. Il permet de combiner le langage Markdown que vous avez déjà abordé au premier module avec du code R, tel que celui utilisé dans la première partie de ce module 2. 2.2.1 R Markdown en vidéo La vidéo ci-dessous vous montre ce qu’est R Markdown, un format hybride entre Markdown et R bien pratique pour inclure vos graphiques directement dans un rapport. Elle vous montre aussi comment transformer un script R en document R Markdown (ou R Notebook, qui en est une variante). Les balises spéciales R Markdown à retenir sont les suivantes : en entrée de chunk R : ```{r} seul sur une ligne. Il est aussi possible de rajouter un nom, par exemple, ```{r graphique1} et/ou des options, par exemple, ```{r, echo=FALSE, results='hide'} pour cacher et le code et le résultat dans le rapport), en sortie de chunk R : ``` seul sur une ligne. Vous devez bien entendu avoir autant de balises d’entrée que de balises de sortie. Des explications plus détaillées se trouvent dans l’annexe B.1.3 dédiée au R Markdown. De plus, l’écriture d’un rapport d’analyse scientifique doit respecter certaines conventions. Vous trouverez des explications à ce sujet dans l’annexe D. Vous ne devez bien évidemment pas commencer avec un script R. Vous pouvez commencer d’emblée avec un R Markdown/R Notebook et écrire vos instructions R directement dedans. Il vous est toujours possible d’exécuter ces instructions ligne après ligne dans la fenêtre Console pour les tester tout comme à partir d’un script R. Pour en savoir plus Communicating results with R Markdown explique la même chose que dans la vidéo, avec plus de détails et des liens vers d’autres documents utiles (en anglais). What is R Markdown?. Vidéo en anglais + site présentant les différentes possibilités, par les concepteurs de R Markdown (RStudio). Introduction to R Markdown. Tutoriel en anglais, par RStudio. R Markdown : the definitive guide est le manuel par excellence pour R Markdown (en anglais uniquement, malheureusement). Aide-mémoire R Markdown: dans les menus de RStudio : Help -&gt; Cheatsheets -&gt; R Markdown Cheat Sheet Référence rapide à Markdown : dans les menus RStudio Help -&gt; Markdown Quick Reference Introduction à R Markdown. Présentation en français par Agrocampus Ouest - Rennes. Le langage R Markdown. Introduction en français concise, mais relativement complète. Reproducible reports with R Markdown. Une explication en anglais de la raison d’être de R Markdown. Why I love R Notebooks explique (en anglais) pourquoi le format R Notebook est particulièrement bien adapté à la science des données. A vous de jouer Vous allez maintenant manipuler un R Notebook pour construire de manière interactive une analyse en même temps que le rapport associé. Partez du projet sdd1_urchin_bio que vous avez obtenu via le lien GitHub Classroom dans la première partie de ce module. Votre objectif est de comprendre les données proposées, en utilisant des visualisations graphiques appropriées et en documentant le fruit de votre étude dans un rapport R Notebook. Utilisez le graphique en nuage de points que vous venez d’étudier, bien sûr, mais vous êtes aussi encouragés à expérimenter d’autres formes de visualisation graphique. Flux de travail “classique” en biologie (Microsoft Excel et Word) comparé à R et R Markdown. Une nouvelle tâche va vous être demandée ci-dessous en utilisant GitHub Classroom 1.2.4.3. Cette tâche est un travail en équipe. Une fois votre assignation réalisée, faites un clone de votre dépôt et placez-le dans le dossier shared/projects. Comparez le workflow classique en biologie via Microsoft Office avec l’utilisation de R - R Markdown en suivant les explications dans le fichier README.mddu dépôt accessible depuis : https://classroom.github.com/g/2Cii2dws "],
["visu2.html", "Module 3 Visualisation II", " Module 3 Visualisation II Objectifs Savoir réaliser différentes variantes de graphiques visant à montrer comment les données se distribuent tel que les histogrammes, les graphes de densité ou encore les diagrammes en violon dans R avec la fonction chart() Intégrer ensuite des graphiques dans un rapport et y décrire ce que vous observez Gérer des conflits dans GitHub Prérequis Pour réaliser les exercices dans ce module, vous devez être capables de travailler dans la SciViews Box et dans RStudio. Vous devez également maîtriser les bases de Git et GitHub. Tout ceci est enseigné dans le module 1. Vous devez également être familiarisés avec les graphiques dans R et R Markdown, une matière qui fait l’objet du module 2. Préparatifs Une nouvelle tâche va vous être demandée ci-dessous en utilisant GitHub Classroom 1.2.4.3. Une fois votre assignation réalisée, faites un clone de votre dépôt et placez-le dans le dossier shared/projects. Pour cette tâche, vous démarrerez d’un projet RStudio B.1.1 que vous obtiendrez via une tâche GitHub Classroom. Un projet sur le zooplancton provenant de Madagascar est mis à votre disposition. Utilisez l’URL suivante qui va vous donner accès à votre tâche. Cette tâche est un travail en binôme https://classroom.github.com/g/7Ji4Aj9G Vous utiliserez à la fois votre projet sur la biométrie des oursins (du module précédent) et ce nouveau projet sur le zooplancton5 pour découvrir les nouveaux outils graphiques décrits dans ce module. Le mot zooplancton ne se décline jamais au pluriel. On parle du zooplancton pour désigner une large communauté d’organismes zooplanctoniques, et non pas des zooplanctons.↩ "],
["histogramme.html", "3.1 Histogramme", " 3.1 Histogramme Vous souhaitez visualiser l’étalement de vos données sur un axe (on parle de distribution6 en statistique) pour l’une des variables étudiées. L’histogramme est l’un des outils pouvant vous apporter cette information. Ce graphique va représenter sous forme de barres un découpage en plusieurs classes7 d’une variable numérique. Figure 2.2: Exemple d’histogramme montrant la distribution de la taille d’un échantillon de zooplancton. Outre l’histogramme lui-même, représenté par des barres de hauteur équivalentes au nombre de fois que les observations ont été réalisées dans les différentes classes, les éléments suivants sont également indispensables à la compréhension du graphique (ici mis en évidence en couleur) Les axes avec les graduations (en rouge). Su l’axe des abscisses, les classes de tailles, et sur l’axe des ordonnées, le nombre d’occurrence les labels des axes et l’unité (pour l’axe des abscisses uniquement ici) (en bleu) Les instructions dans R afin de produire un histogramme à l’aide de la fonction chart() sont : # Importation du jeu de données (zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;)) # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # ... with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; # Réalisation du graphique chart(data = zooplankton, ~ size) + geom_histogram(bins = 50) + ylab(&quot;Effectifs&quot;) Figure 3.1: Distribution des tailles au sein d’un échantillon de zooplancton # bins permet de préciser le nombre de classes souhaitées La fonction chart() requiert comme argument le jeu de donnée (zooplankton), ainsi que la formule à employer dans laquelle vous avez indiqué le nom de la variable que vous voulez sur l’axe des abscisses à droite de la formule, après le tilde ~. Vous voyez que le jeu de données contient beaucoup de variables (les titres des colonnes du tableau en sortie). Parmi toutes ces variables, nous avons choisi ici de représenter size, Jusqu’ici, nous avons spécifié ce que nous voulons représenter, mais pas encore comment (sous quelle apparence), nous voulons matérialiser cela sur le graphique. Pour un histogramme, nous devons ajouter la fonction geom_histogram(). L’argument bins dans cette fonction permet de préciser le nombre de classes souhaitées. Le découpage en classe se fait automatiquement dans R à partir de la variable size d’origine. Vous pouvez décrypter votre histogramme sur base des modes8 et de la symétrie9 de ces derniers. Un histogramme peut être unimodal (un seul mode), bimodal (deux modes) ou multimodal (plus de deux modes). En général, s’il y a plus d’un mode, nous pouvons suspecter que des sous-populations existent au sein de notre échantillon. Figure 3.2: Histogrammes montrant les modes et symétries : A. histogramme unimodal et symétrique, B. histogramme bimodal et asymétrique, C. histogramme unimodal et asymétrique, D. histogramme multimodal et symétrique. 3.1.1 Nombre de classes Vous devez être particulièrement vigilant lors de la réalisation d’un histogramme aux classes définies pour ce dernier. # Réalisation du graphique précédent a &lt;- chart(data = zooplankton, ~ size) + geom_histogram(bins = 50) + ylab(&quot;Effectifs&quot;) # Modification du nombre de classes b &lt;- chart(data = zooplankton, ~ size) + geom_histogram(bins = 20) + ylab(&quot;Effectifs&quot;) c &lt;- chart(data = zooplankton, ~ size) + geom_histogram(bins = 10) + ylab(&quot;Effectifs&quot;) d &lt;- chart(data = zooplankton, ~ size) + geom_histogram(bins = 5) + ylab(&quot;Effectifs&quot;) # Assemblage des graphiques combine_charts(list(a, b, c, d)) Figure 3.3: Choix des classes. A. histogramme initial montrant la répartition des tailles au sein d’organismes planctoniques. B., C., D. Même histogramme que A, mais en modifiant le nombres de classes. Comme vous pouvez le voir à la Fig. 3.3, le changement du nombre de classes peut modifier complètement la perception des données via l’histogramme. Le choix idéal est un compromis entre plus de classes (donc plus de détails), et un d“coupage raisonnable en fonction de la quantité de données disponibles. Si l’intervalle des classes est trop petit, l’histogramme sera illisible. Si l’intervalle des classes est trop grand, il sera impossible de visualiser correctement les différents modes. Dans la figure en exemple, les variantes A et B sont acceptables, mais les C et D manquent de détails. Pièges et astuces La SciViews Box propose un snippet RStudio pour réaliser un histogramme. Il s’appelle .cuhist (pour chart -&gt; univariate -&gt; histogram). Entrez ce code dans une zone d’édition R et appuyez ensuite sur la tabulation, et vous verrez le code remplacé par ceci : chart(data = DF, ~VARNUM) + geom_histogram(binwidth = 30) L’argument binwidth = permet de préciser la largeur des classes. C’est une autre façon de spécifier le découpage en classes, mais vous pouvez naturellement le remplacer par l’argument bins = si vous préférez. Vous avez à votre disposition un ensemble de snippets que vous pouvez retrouver dans l’aide-mémoire sur SciViews. N’oubliez pas que vous avez également à votre disposition l’aide-mémoire sur la visualisation des données (Data Visualization Cheat Sheet), via la fonction ggplot(). 3.1.2 Histogramme par facteur Lors de l’analyse de jeux de données, vous serez amené à réaliser un histogramme par facteur (c’est-à-dire, en fonction de différents niveaux d’une variable qualitative qui divise le jeu de données en sous-groupes). Par exemple, dans un jeu de données sur des fleurs d’iris, la variable species10 représente l’espèce d’iris étudiée (trois espèces différentes : I. setosa, I. versicolor et I. virginica). # Importation du jeu de données (iris &lt;- read(&quot;iris&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;)) # # A tibble: 150 x 5 # sepal_length sepal_width petal_length petal_width species # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 5.1 3.5 1.4 0.2 setosa # 2 4.9 3 1.4 0.2 setosa # 3 4.7 3.2 1.3 0.2 setosa # 4 4.6 3.1 1.5 0.2 setosa # 5 5 3.6 1.4 0.2 setosa # 6 5.4 3.9 1.7 0.4 setosa # 7 4.6 3.4 1.4 0.3 setosa # 8 5 3.4 1.5 0.2 setosa # 9 4.4 2.9 1.4 0.2 setosa # 10 4.9 3.1 1.5 0.1 setosa # # ... with 140 more rows # Réalisation de l&#39;histogramme par facteur chart(data = iris, ~ sepal_length %fill=% species) + geom_histogram(bins = 25) + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() # palette de couleur harmonieuse Figure 2.3: Distribution des longueurs de sépales de trois espèces d’iris. Ici, nous avons tracé un histogramme unique, mais en prenant soin de colorier les barres en fonction de l’espèce. la formule fait toujours intervenir la variable numérique à découper en classes à la droite du tilde ~, ici sepal_length, mais nous y avons ajouté une directive supplémentaire pour indiquer que le remplissage des barres (%fill=%) doit se faire en fonction du contenu de la variable species. Nous avons ici un bon exemple d’histogramme multimodal lié à la présence de trois sous-groupes (les trois espèces différentes) au sein d’un jeu de données unique. Le rendu du graphique n’est pas optimal. Voici deux astuces pour l’améliorer. La premières consiste à représenter trois histogrammes séparés, mais rassemblés dans une même figure. Pour cela, nous utilisons des facettes (facets) au lieu de l’argument %fill=%. Dans chart(), les facettes peuvent être spécifiées an utilisant l’opérateur | dans la formule. chart(data = iris, ~ sepal_length | species) + geom_histogram(bins = 25) + ylab(&quot;Effectifs&quot;) Figure 2.4: Distribution de la longueur des sépales de trois espèces d’iris (en employant les facettes pour séparer les espèces). L’histogramme est maintenant séparé en trois en fonction des niveaux de la variable facteur species. Cela rend la lecture plus aisée. Une seconde solution combine les facettes avec | et l’argument %fill=%11. Il faut ensuite ajouter par derrière un histogramme grisé de l’ensemble des données. nbins &lt;- 25 chart(data = iris, ~ sepal_length %fill=% species | species) + # histogramme d&#39;arrière plan en gris ne tenant pas compte de la variable species geom_histogram(data = select(iris, -species), fill = &quot;grey&quot;, bins = nbins) + geom_histogram(show.legend = FALSE, bins = nbins) + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() Figure 3.4: Distribution des longueurs de sépales de trois espèces d’iris (avec facettes et histogrammes complets grisés en arrière plans). Vous découvrez sans doute que les graphiques réalisables avec R sont modulables à souhait en ajoutant une série d’instructions successives qui créent autant de couches superposées dans le graphique. Cette approche permet de réaliser quasiment une infinité de graphiques différents en combinant seulement quelques dizaines d’instructions. Pour s’y retrouver, les fonctions qui ajoutent des couches commencent toutes par geom_, et celles qui manipulent les couleurs par scale_, par exemple. Vous découvrirez encore d’autres fonctions graphiques plus loin. La distribution des données en statistique se réfère à la fréquence avec laquelle les différentes valeurs d’une variable s’observent.↩ Une variable numérique est découpée en classes en spécifiant différents intervalles, et ensuite en dénombrant le nombre de fois que les observations rentrent dans ces classes.↩ Les modes d’un histogramme correspondent à des classes plus abondantes localement, c’est-à-dire que les classes à gauche et à droite du mode comptent moins d’occurrences que lui.↩ Un histogramme est dit symétrique lorsque son profil à gauche est identique ou très similaire à son profil à droite autour d’un mode.↩ Attention : le jeu de donnée iris est un grand classique dans R, mais lorsqu’il est chargé à l’aide de la fonction read() du package data.io, le nom de ses variables est modifié pour suivre la convention “snake-case” qui veut que seules des lettres minuscules soient utilisées et que les mots soient séparés par un trait souligné _. Ainsi, dans le jeu de données d’origine, les variables sont nommées Petal_Length ou Species. Ici, ces même variables se nomment petal_length et species.↩ Astuce proposée ici.↩ "],
["graphique-de-densite.html", "3.2 Graphique de densité", " 3.2 Graphique de densité L’histogramme n’est pas le seul outil à votre disposition. Vous pouvez également employer le graphique de densité qui se présente un peu comme un histogramme lissé. Le passage d’un histogramme vers un graphe de densité se base sur une estimation par noyaux gaussien12 Figure 3.5: A. Histogramme et B. graphique de densité montrant la distribution de la taille de zooplancton étudié par analyse d’image. Comme pour les autres graphiques, veuillez à soigner les indications qui permettent d’interpréter le graphique. Outre la courbe de densité, il faut : Les axes avec les graduations (en rouge) les labels des axes, et l’unité pour l’axe des abscisses (en bleu) Les instructions en R pour produire un graphique de densité avec la fonction chart() sont : # Importation du jeu de données (zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;)) # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # ... with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; # Réalisation du graphique chart(data = zooplankton, ~ size) + geom_density() + ylab(&quot;Densité&quot;) Figure 3.6: Distribution des tailles au sein de l’échantillon de zooplancton. Ici, nous utilisons donc la fonction geom_density(). L’opération effectuée pour passer d’un histogramme à une courbe de densité consiste effectivement à lisser les pics plus ou moins fort dans l’histogramme de départ.↩ "],
["diagramme-en-violon.html", "3.3 Diagramme en violon", " 3.3 Diagramme en violon Le graphique en violon est constitué de deux graphiques de densité en miroir. Le résultat fait penser un peu à un violon pour une distribution bimodale. Cette représentation est visuellement très convainquante lorsque la variable étudiée contient suffisamment d’observations pour permettre de déterminer précisément sa distribution (plusieurs dizaines ou centaines d’individus mesurés). Figure 3.7: Graphe en violon de la distribution de la taille en fonction des groupes taxonomiques dans un échantillon de zooplancton. Les instructions en R pour produire un diagramme en violon à l’aide de la fonction chart() sont : # Importation du jeu de données zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) # Réduction du jeu de données zooplankton_sub &lt;- filter(zooplankton, class %in% c(&quot;Annélide&quot;, &quot;Calanoïde&quot;, &quot;Cyclopoïde&quot;, &quot;Décapode&quot;)) # Réalisation du graphique chart(data = zooplankton_sub, size ~ class) + geom_violin() Figure 3.8: Distribution des tailles pour 4 groupes taxonomiques de zooplancton. Ici, la formule fournie à chart() indique la variable numérique à représenter par une graphe de densité dans le terme de gauche, et la variable facteur qui découpe l’échantillon en classes à droite : YNUM (size) ~ XFACT (class). Pour réaliser un graphique de densité vous devez ensuite ajouter la fonction geom_violin(). Vous pouvez aussi utiliser %fill=% pour colorer vos différents graphes en fonction de la variable facteur également, comme dans la Fig. 3.7. Pièges et astuces Parfois, les labels sur l’axe des abscisses d’un diagramme en violon apparaissent trop rapprochés et se chevauchent, comme ci-dessous. chart(data = zooplankton, size ~ class) + geom_violin() Figure 3.9: Distribution de tailles des 17 classes d’organismes planctoniques (diagramme en violon). La fonction coord_flip() permute les axes. Ainsi les labels ne se chevauchent plus sur l’axe des ordonnées. chart(data = zooplankton, size ~ class) + geom_violin() + coord_flip() Figure 3.10: Distribution de tailles des 17 classes d’organismes planctoniques (diagramme en violon avec l’ajout de la fonction coord_flip()). Le package ggridges propose une seconde solution basée sur le principe de graphique de densité avec la fonction geom_density_ridges() qui crée un graphique en lignes de crêtes. Attention : remarquez que la notation est ici inverse du diagramme en violon, soit XFACT (class) ~ YNUM (size) ! chart(data = zooplankton, class ~ size) + ggridges::geom_density_ridges() Figure 3.11: Distribution des tailles des 17 classes d’organismes planctoniques (sous forme de graphique en lignes de crêtes). "],
["visualiser-des-distributions.html", "3.4 Visualiser des distributions", " 3.4 Visualiser des distributions En pratique, vous ne représenterez pas systématiquement tous ces types de graphiques pour toutes les variables. Il faudra choisir le graphique le plus adapté à la situation. La plupart du temps, cela se fait de manière itérative : vous essayez diverses variantes, vous les comparez, et vous gardez celle(s) qui visualisent le mieux les données dans le cas particulier de votre étude. A vous de jouer Reprenez vos différents projets et étudiez la distribution de variables numériques de différentes manières. Commentez vos différents graphiques par des paragraphes rédigés en Markdown. Précisez ceux qui vous semblent les plus appropriés et justifiez vos choix. Terminez ce module en vérifiant que vous avez bien compris les notions apprises jusqu’ici. Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;03a_test&quot;) Pour en savoir plus Si vous avez encore du mal avec la compréhension de l’histogramme, voyez cette vidéo qui vous montre comment le construire à la main. Les histogrammes à classes de largeurs variables. "],
["visu3.html", "Module 4 Visualisation III", " Module 4 Visualisation III Objectifs Savoir réaliser différents graphiques pour représenter des variables facteurs comme le graphique en barres, ou le graphique en camembert dans R avec la fonction chart() Comprendre et utiliser la boîte de dispersion pour synthétiser la distribution de données numériques Arranger différents graphiques dans une figure unique Découvrir différents systèmes graphiques (graphiques de base, lattice, ggplot2) et les comparer entre eux Prérequis Assurez-vous de bien maîtriser les bases relatives à la représentation graphiques vues dans le module 2 et que vous êtes à l’aise dans l’utilisation de vos outils logiciels (SciViews Box, RStudio, R Markdown). "],
["graphique-en-barres.html", "4.1 Graphique en barres", " 4.1 Graphique en barres Le graphique en barres (on dit aussi graphique en bâtons) compare les effectifs pour différents niveaux (ou modalités) d’une variable qualitative ou facteur. La différence avec l’histogramme est donc subtile et tient au fait que, pour l’histogramme, nous partons d’une variable quantitative qui est découpée en classes. 4.1.1 Effectifs par facteur La question du nombre et/ou de l’intervalle des classes ne se pose pas dans le cas du graphique en barres. Par défaut, les barres seront séparées les unes des autres par un petit espace vide pour bien indiquer visuellement qu’il n’y a pas continuité entre les classes (dans l’histogramme, les barres sont accolées les unes aux autres pour matérialiser justement cette continuité). La formule que vous utiliserez, ici encore, ne fait appel qu’à une seule variable et s’écrira donc : \\[\\sim variable \\ facteur\\] Figure 1.1: Exemple d’un graphique en barres montrant le dénombrement des niveaux d’une variable facteur, avec les éléments importants du graphique mis en évidence en couleurs. Outre les barres elles-mêmes, prêtez toujours attention aux éléments suivants du graphique (ici mis en évidence en couleurs) : les axes avec les graduations (en rouge) les niveaux de la variable facteur (en rouge également) le label des axes (en bleu) Les instructions dans R pour produire un graphique en barres à l’aide de la fonction chart() sont : # Importation du jeu de données (zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;)) # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # ... with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; # Réduction du jeu de données (copepoda &lt;- filter(zooplankton, class %in% c(&quot;Calanoïde&quot;, &quot;Cyclopoïde&quot;, &quot;Harpacticoïde&quot;, &quot;Poecilostomatoïde&quot;))) # # A tibble: 535 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 3 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 4 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 5 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 6 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 7 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 8 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 9 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # 10 0.725 0.412 7.14 1.90 0.802 0.655 0.209 0.008 0.004 0.732 0.202 # # ... with 525 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; # Réalisation du graphique chart(data = copepoda, ~ class) + geom_bar() + ylab(&quot;Effectifs&quot;) Figure 2.2: Abondances de quatres types de copépodes dans un échantillon de zooplancton. La fonction geom_bar() se charge d’ajouter les barres verticales dans le graphique. La hauteur de ces barres correspond au nombre d’observations rencontrées dans le jeu de données pour chaque niveau (ou classe, ou groupe) de la variable facteur représentée. 4.1.2 Effectifs par 2 facteurs # Importation des données biometry (biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;FR&quot;)) # # A tibble: 395 x 7 # gender day_birth weight height wrist year_measure age # &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 H 1995-03-11 69 182 15 2013 18 # 2 H 1998-04-03 74 190 16 2013 15 # 3 H 1967-04-04 83 185 17.5 2013 46 # 4 H 1994-02-10 60 175 15 2013 19 # 5 F 1990-12-02 48 167 14 2013 23 # 6 F 1994-07-15 52 179 14 2013 19 # 7 F 1971-03-03 72 167 15.5 2013 42 # 8 F 1997-06-24 74 180 16 2013 16 # 9 H 1972-10-26 110 189 19 2013 41 # 10 H 1945-03-15 82 160 18 2013 68 # # ... with 385 more rows # Conversion de la variable year_measure de numérique à facteur biometry$year_measure &lt;- as.factor(biometry$year_measure) label(biometry$year_measure) &lt;- &quot;Année de la mesure&quot; Différentes représentations sont possibles pour observer des dénombrements tenant compte de plusieurs variables facteurs. Par défaut, l’argument position = a pour valeur par défaut stack (donc, lorsque cet argument n’est pas précisé dans geom_bar()). a &lt;- chart(data = biometry, ~ gender) + geom_bar() + ylab(&quot;Effectifs&quot;) b &lt;- chart(data = biometry, ~ gender %fill=% year_measure) + geom_bar() + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() combine_charts(list(a, b), common.legend = TRUE) Figure 3.2: Dénombrement des hommes (H) et des femmes (F) dans l’étude sur l’obésité en Hainaut en tenant compte des années de mesure pour (B). Il existe d’autres options en utilisant la valeur dodge ou fill pour l’argument position =. a &lt;- chart(data = biometry, ~ gender %fill=% year_measure) + geom_bar(position = &quot;stack&quot;) + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() b &lt;- chart(data = biometry, ~ gender %fill=% year_measure) + geom_bar(position = &quot;dodge&quot;) + ylab(&quot;Effectifs&quot;) + scale_fill_viridis_d() c &lt;- chart(data = biometry, ~ gender %fill=% year_measure) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Fractions&quot;) + scale_fill_viridis_d() combine_charts(list(a, b, c), common.legend = TRUE) Figure 2.3: Dénombrement des hommes (H) et des femmes (F) dans l’étude sur l’obésité en Hainaut en tenant compte des années de mesure (différentes présentations). Soyez vigilant à la différence entre l’argument position = stack et position = fill qui malgré un rendu semblable ont l’axe des ordonnées qui diffère (dans le cas de fill, il s’agit de la fraction par rapport au total qui est représentée, et non pas des effectifs absolus dénombrés). Pièges et Astuces Réordonner la variable facteur par fréquence Vous pouvez avoir le souhait d’ordonner votre variable facteur afin d’améliorer le rendu visuel de votre graphique. Pour cela, vous pouvez employer la fonction fct_infreq(). chart(data = copepoda, ~ fct_infreq(class)) + geom_bar() + labs(x = &quot;Classe&quot;, y = &quot;Effectifs&quot;) Figure 2.4: Dénombrement des classes de copépodes du jeu de données zooplankton. Rotation des axes du graphique en barre Lorsque les niveaux dans la variable étudiée sont trop nombreux, les légendes en abscisse risquent de se chevaucher, comme dans la Fig. 4.1 chart(data = zooplankton, ~ class) + geom_bar() + ylab(&quot;Effectifs&quot;) Figure 4.1: Dénombrement des classes du jeu de données zooplankton. Avec la fonction coord_flip() ajoutée à votre graphique, vous pouvez effectuer une rotation des axes pour obtenir un graphique en barres horizontales. De plus, l’œil humain perçoit plus distinctement les différences de longueurs horizontales que verticales. Donc, de ce point de vue, le graphe en barres horizontal est considéré comme meilleur que le graphe en barres verticales. chart(data = zooplankton, ~ class) + geom_bar() + ylab(&quot;Effectifs&quot;) + coord_flip() Figure 3.4: Dénombrement des classes du jeu de données zooplankton (version avec barres horizontales). Pour en savoir plus Graphes en barres à l’aide de ggplot2. Un tutoriel en français utilisant la fonction ggplot(). L’annotation des barres est également présentée. Page d’aide de la fonction geom_bar() en anglais. Autres exemples de graphes en barres à l’aide de `ggplot(). 4.1.3 Valeurs moyennes Le graphique en barres peut être aussi employé pour résumer des données numériques via la moyenne. Il ne s’agit plus de dénombrer les effectifs d’une variable facteur mais de résumer des données numériques en fonction d’une variable facteur. On peut exprimer cette relation dans R sous la forme de \\[y \\sim x\\] que l’on peut lire : \\[y \\ en \\ fonction \\ de \\ x\\] Avec y une variable numérique et x une variable facteur. Considérez l’échantillon suivant : 1, 71, 55, 68, 78, 60, 83, 120, 82 ,53, 26 Calculez la moyenne sur base de la formule de la moyenne \\[\\overline{y} = \\sum_{i = 1}^n \\frac{y_i}{n}\\] # Création du vecteur x &lt;- c(1, 71, 55, 68, 78, 60, 83, 120, 82, 53, 26) # Calcul de la moyenne mean(x) # [1] 63.36364 Les instructions pour produire ce graphe en barres à l’aide de chart() sont : chart(data = copepoda, size ~ class) + stat_summary(geom = &quot;col&quot;, fun.y = &quot;mean&quot;) Figure 3.6: Exemple de graphique en barres représentant les moyennes de tailles par groupe zooplanctonique. Ici, nous faisons appel à une autre famille de fonctions : celles qui effectuent des calculs sur les données avant de les représenter graphiquement. Le graphe en barres pour représenter les moyennes est très répandu dans le domaine scientifique malgré le grand nombre d’arguments en sa défaveur et que vous pouvez lire dans la section pour en savoir plus ci-dessous. L’un des arguments le plus important est la faible information qu’il véhicule puisque l’ensemble des données n’est plus représenté que par une valeur (la moyenne) pour chaque niveau de la variable facteur. Pour un petit nombre d’observations, il vaut mieux toutes les représenter à l’aide d’un nuage de points. Si le nombre d’observations devient très grand (dizaines ou plus), le graphique en boites de dispersion est plus indiqué (voir plus loin dans ce module). Pour en savoir plus Beware of dynamite. Démonstration de l’impact d’un graphe en barres pour représenter la moyenne (et l’écart type) = graphique en “dynamite”. Dynamite plots : unmitigated evil? Une autre comparaison du graphe en dynamite avec des représentations alternatives qui montre que le premier peut avoir quand même quelques avantages dans des situations particulières. "],
["graphique-en-camembert.html", "4.2 Graphique en camembert", " 4.2 Graphique en camembert Le graphique en camembert (ou en parts de tarte, ou encore appelé diagramme circulaire, pie chart en anglais) va vous permettre de visualiser un dénombrement d’observations par facteur, tout comme le graphique en barres. chart(data = copepoda, ~ factor(0) %fill=% class) + geom_bar(width = 1) + coord_polar(&quot;y&quot;, start = 0) + theme_void() + scale_fill_viridis_d() Figure 3.9: Exemple de graphique en camembert montrant les effectifs des niveaux d’une variable facteur. Ce graphique est plus difficile à réaliser à l’aide de chart() ou ggplot(). En fait, il faut ruser ici, et l’auteur du package ggplot2 n’avait tout simplement pas l’intention d’ajouter ce type de graphique dans la panoplie proposée. En effet, il faut savoir que l’œil humain est nettement moins bon pour repérer des angles que pour comparer des longueurs. Donc, le diagramme en barres est souvent meilleur pour comparer des effectifs par classes. Mais d’une part, le graphique en camembert est (malheureusement) un graphique très répandu et il faut savoir l’interpréter, et d’autre part, il peut s’avérer quand même utile dans certaines situations. Notez l’utilisation de la fonction theme_void() qui crée un graphique sans axes. Pièges et astuces Partons d’un exemple fictif pour vous convaincre qu’un graphique en barres est souvent plus lisible qu’un graphique en camembert. Combien d’observations comptez-vous pour la lettre H ? Figure 3.11: Arrivez-vous à lire facilement des valeurs sur un graphique en camenbert (une échelle y est ajoutée de manière exceptionnelle pour vous y aider). Maintenant, effectuez le même exercice sur base d’un graphique en barres, combien d’observations pour la lettre H ? Figure 4.2: Dénombrement des niveaux d’une variable facteur sur un graphique en barres. Dans ce dernier cas, c’est bien plus facile : il y a effectivement 24 observations relatives à la lettre H. Pour en savoir plus Graphique en camembert à l’aide de la fonction ggplot(). Explications en français des différentes étapes pour passer d’un graphique en barres à un graphique en camembert avec ggplot2. Autre explication en français, également accompagnée d’informations sur les bonnes pratiques en matière de graphique en camembert. Save the pies for dessert est une démonstration détaillée des méfaits du graphique en camembert (le graphique en camembert, un graphique puant ? Pourrait-on peut-être titrer en français). Les côtés positifs du graphe en camembert sont mis en évidence dans ce document (en anglais). "],
["boite-de-dispersion-boxplot.html", "4.3 Boite de dispersion {boxplot}", " 4.3 Boite de dispersion {boxplot} Vous souhaitez représenter graphiquement cette fois un résumé d’une variable numérique mesurée sur un nombre (relativement) important d’individus, soit depuis une dizaine jusqu’à plusieurs millions. Vous souhaitez également conserver de l’information sur la distribution des données, et voulez éventuellement comparer plusieurs distributions entre elles : soit différentes variables, soit différents niveaux d’une variable facteur. Nous avons déjà vu au module 3 les diagrammes en violon et en lignes de crêtes pour cet usage. Nous allons étudier ici les boites de dispersion (encore appelée boite à moustaches) comme option alternative intéressante. La boite de dispersion va représenter graphiquement cinq descripteurs appelés les cinq nombres. Considérez l’échantillon suivant : 1, 71, 55, 68, 78, 60, 83, 120, 82 ,53, 26 Ordonnez-le de la plus petite à la plus grande valeur : # Créer du vecteur x &lt;- c(1, 71, 55, 68, 78, 60, 83, 120, 82, 53, 26) # Ordonner le vecteur par ordre croissant sort(x) # [1] 1 26 53 55 60 68 71 78 82 83 120 Le premier descripteur des cinq nombres est la médiane qui est la valeur se situant à la moitié des observations, donc, avec autant d’observations plus petites et d’observations plus grande qu’elle. La médiane sépare l’échantillon en deux. median(x) # [1] 68 Les quartiles séparent l’échantillon en quatre. Le premier quartile (Q1) sera la valeur pour laquelle 25% des observations seront plus petites. Elle se situe donc entre la valeur minimale et la médiane. Cette médiane est égale au second quartile (50% des observations plus petites). Le troisième quartile (Q3) est la valeur pour laquelle 75% des observations de l’échantillon sont plus petites13. Enfin, la valeur minimale et la valeur maximale observées dans l’échantillon complètent ces cinq nombres qui décrivent de manière synthétique la position et l’étendue des observations. Les cinq nombres sont : la valeur minimale, le premier quartile, la médiane (ou deuxième quartile), le troisième quartile et la valeur maximale. Voici comment on les calcules facilement dans R : fivenum(x) # [1] 1 54 68 80 120 La boite de dispersion est une représentation graphique codifiée de ces cinq nombres. La représentation de x sous forme de nuage de points n’est ni très esthétique, ni très lisible, surtout si nous avons affaire à des milliers ou des millions d’observations qui se chevauchent sur le graphique14. Figure 4.3: Nuage de points univarié. La boite de dispersion va remplacer cette représentation peu lisible par un objet géométrique qui représente les cinq nombres. Figure 4.4: A) Nuage de points annoté avec les cinq nombres représentés par des traits horizontaux. B) Boite de dispersion obtenue pour les même données que A. Vous observez à la Fig. 4.4 que certaines valeurs minimales et maximales ne sont pas reliées à la boite de dispersion, il s’agit de valeurs extrêmes. Règle pour déterminer s’il y a des valeurs extrêmes avec une boite de dispersion : une valeur est considérée comme extrême si son écart par rapport à la boite est supérieur à une fois et demi la hauteur de la boite (encore appelée espace inter-quartile IQR correspondant à Q3 - Q1). Les tiges (ou “moustaches”) qui prolongent la boite de dispersion s’arrêtent donc aux dernières valeurs les plus petites et plus grandes, mais qui rentrent encore dans une fois et demi l’IQR. Les valeurs extrêmes sont ensuite représentées individuellement par un point au dessus et en dessous. La boite de dispersion finale ainsi que sa description sont représentées à la Fig. 4.5 ci-dessous. Figure 4.5: A) Boite de dispersion pour x et B) description des différents éléments constitutifs. Les instructions dans R pour produire un graphique en boites de dispersion parallèles (comparaison de la distribution d’une variable numérique pour différents niveaux d’une autre variable facteur) sont : chart(data = copepoda, size ~ class) + geom_boxplot() Figure 4.6: Distribution des tailles par groupes taxonomiques pour le zooplancton. La formule à employer est YNUM (size) ~ XFACTOR (class). Ensuite, pour réaliser une boite de dispersion vous devez ajouter la fonction geom_boxplot(). 4.3.1 Taille de l’échantillon Lors de la réalisation de boites de dispersion, vous devez être vigilant au nombre d’observations qui se cachent sous chacune d’elles. En effet, réaliser une boite de dispersion à partir d’échantillons ne comportant que cinq valeurs ou moins n’a aucun sens ! Figure 4.7: Piège des boites de dispersion : trop peu d’observations disponibles pour a. La boite de dispersion A est calculée à partir de seulement quatre observations. C’est trop peu. Comme les points représentant les observations ne sont habituellement pas superposés à la boite, cela peut passer inaperçu et tromper le lecteur ! Une bonne pratique consiste à ajouter n, le nombre d’observations au-dessus de chaque boite. Cela peut se faire facilement avec les fonctions give_n() et stat_summary() ci-dessous. give_n &lt;- function(x) c(y = max(x) * 1.1, label = length(x)) chart(data = copepoda, size ~ class) + geom_boxplot() + stat_summary(fun.data = give_n, geom = &quot;text&quot;, hjust = 0.5) Figure 4.8: Taille de copépodes pour différents groupes taxonomiques (le nombre d’observations est indiqué au dessus de chaque boite). 4.3.2 En fonction de 2 facteurs La Fig. 4.9 présente un graphique en boites de dispersion parallèles qui combine l’usage de deux variables facteurs différentes. # Importation du jeu de données ToothGrowth (tooth_growth &lt;- read(&quot;ToothGrowth&quot;, package = &quot;datasets&quot;)) # # A tibble: 60 x 3 # len supp dose # &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; # 1 4.2 VC 0.5 # 2 11.5 VC 0.5 # 3 7.3 VC 0.5 # 4 5.8 VC 0.5 # 5 6.4 VC 0.5 # 6 10 VC 0.5 # 7 11.2 VC 0.5 # 8 11.2 VC 0.5 # 9 5.2 VC 0.5 # 10 7 VC 0.5 # # ... with 50 more rows # Remaniement et labelisation du jeu de données tooth_growth$dose &lt;- as.ordered(tooth_growth$dose) tooth_growth &lt;- labelise(tooth_growth, self = FALSE, label = list( len = &quot;Longueur des dents&quot;, supp = &quot;Supplémentation&quot;, dose = &quot;Dose&quot; ), units = list( len = &quot;mm&quot;, supp = NA, dose = &quot;mg/J&quot; ) ) # Réalisation graphique chart(data = tooth_growth, len ~ supp %fill=% dose) + geom_boxplot() + stat_summary(fun.data = give_n, geom = &quot;text&quot;, hjust = 0.5, position = position_dodge(0.75)) Figure 4.9: Croissance de dents de cochons d’Inde en fonction de la supplémentation (OJ = jus d’orange, VC = vitamine C) et de la dose administrée (n indiqué au dessus de chaque boite). Pour en savoir plus Un tutoriel boites de dispersion à l’aide de ggplot() présentant encore bien d’autres variantes possibles. Box plots in ggplot2. Autre explication en anglais avec sortie utilisant plotly. Grouped box plots. Explication plus détaillée sur les cinq nombres, en anglais. Notez que, lorsque la coupure tombe entre deux observations, une valeur intermédiaire est utilisée. Ici par exemple, le premier quartile est entre 53 et 55, donc, il vaut 54. Le troisième quartile se situe entre 78 et 82. Il vaut donc 80.↩ Il est possible de modifier la transparence des points et/ou de les déplacer légèrement vers la gauche ou vers la droite de manière aléatoire pour résoudre le problème de chevauchement des points sur un graphique en nuage de points univarié.↩ "],
["figures-composees.html", "4.4 Figures composées", " 4.4 Figures composées Il arrive fréquemment de vouloir combiner plusieurs graphiques dans une même figure. Plusieurs fonctions sont à votre disposition pour cela. Il faut tout d’abord distinguer deux types de figures multi-graphiques : Soit il s’agit d’un seul graphique que vous souhaitez subdiviser par rapport à une ou des variables facteurs. Soit il s’agit de graphiques indépendants que vous souhaitez assembler dans une même figure parce que les données ont un lien entre elles, ou parce que ces graphiques sont complémentaires pour comprendre les données. Dans le premier cas, les fonctions facet_XXX() comme facet_grid() peuvent être employées. Dans le second cas, la fonction combine_charts() est l’une des alternatives possibles. 4.4.1 Facettes L’une des règles les plus importantes que vous devez impérativement garder à l’esprit lors de la réalisation de vos graphiques est la simplicité. Plus votre graphique va contenir d’information au plus il sera compliqué à décoder par vos lecteurs. # Importation de données relative à la croissance de poulets (chick_weight &lt;- read(&quot;ChickWeight&quot;, package = &quot;datasets&quot;)) # # A tibble: 578 x 4 # weight Time Chick Diet # &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;fct&gt; # 1 42 0 1 1 # 2 51 2 1 1 # 3 59 4 1 1 # 4 64 6 1 1 # 5 76 8 1 1 # 6 93 10 1 1 # 7 106 12 1 1 # 8 125 14 1 1 # 9 149 16 1 1 # 10 171 18 1 1 # # ... with 568 more rows # Réalisation du graphique (points semi-transparents) chart(data = chick_weight, weight ~ Time %col=% Diet) + geom_point(alpha = 0.5) + labs(x = &quot;Age [j]&quot;, y = &quot;Masse [g]&quot;) Figure 4.10: Croissance de poulets en utilisant quatre aliments différents. Le graphique à la Fig. 4.10 est mal adapté pour montrer les différences entre les quatre aliments : tous les points sont entremêlés. Il peut typiquement être simplifié en utilisant des facettes pour représenter les résultats relatifs aux différents régimes alimentaires sur des graphiques séparés. L’information est la même mais la lecture est beaucoup plus aisée. chart(data = chick_weight, weight ~ Time | Diet) + geom_point(alpha = 0.5) + labs(x = &quot;Age [j]&quot;, y = &quot;Masse [g]&quot;) Figure 4.11: Croissance de poulets en utilisant quatre aliments différents (1-4). Vous observez que les échelles en abscisse et en ordonnée sont similaires sur tous les graphiques. Cela permet une meilleure comparaison. Notez toutefois que, plus le nombre de facettes augmente, plus chaque graphique individuel devient petit. Faites attention à ne pas finir avec des graphiques individuels tellement petits qu’ils en deviennent illisibles ! 4.4.2 Graphiques assemblés La fonction combine_charts() permet de combiner plusieurs graphiques dans une figure unique. Nous l’avons déjà utilisée à plusieurs reprises. Cette fonction attend une liste de graphiques de type chart() à assembler. # Importation des données urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) # Réalisation des graphiques a &lt;- chart(data = urchin, weight ~ height %col=% origin) + geom_point() b &lt;- chart(data = urchin, weight ~ solid_parts %col=% origin) + geom_point() # Combinaison des graphiques dans une même figure combine_charts(list(a, b), common.legend = TRUE) Figure 4.12: A) Masse d’oursins en fonction de leur taille et de leur origine. B) Masse totale en fonction de la masse des parties solides de ces mêmes oursins. Il existe d’autres fonctions permettant de combiner plusieurs graphiques comme plot_grid() du package cowplot, mais avec combine_charts() vous pourrez déjà faire beaucoup. De plus, un libellé sous forme d’une lettre majuscule est automatiquement associé à chaque sous-région de la figure composée. Cela permet d’y faire plus facilement référence dans le texte et/ou dans la légende. Pour en savoir plus Partitionnement des graphiques en facettes. Différentes options sont présentées ici. Figures composées à l’aide de grid.arrange(). Une autre option, mais moins flexible et moins riche que combine_charts(). Figures composées à l’aide de plot_grid() avec les différentes options, aussi disponibles avec combine_charts(). Troisième possibilité pour des figures composées à l’aide de ggarrange(). combine_charts() fait la même chose, mais avec des valeurs par défaut légèrement différentes (labels = &quot;auto&quot; par défaut pour ce dernier, mais labels = NULL pour ggarrange()). "],
["differents-moteurs-graphiques.html", "4.5 Différents moteurs graphiques", " 4.5 Différents moteurs graphiques Prolifération des standards d’après xkcd. Depuis le début, l’ensemble des graphiques que nous vous avons proposés utilise la fonction chart() du package chart. Cependant, il ne s’agit pas de la seule fonction permettant de réaliser des graphiques dans R, loin de là. En fait, chart est tout récent et a été développé pour homogénéiser autant que possible les graphiques issus de trois moteurs graphiques différents : ggplot2, lattice et les graphiques base. La fonction chart() a d’autres avantages également : Un thème par défaut qui est le plus proche possible d’un rendu typique d’une publication scientifique. La possibilité d’utiliser l’interface formule avec ggplot2. La cohérence des objets graphiques obtenus qui peuvent tous êtres combinés en une figure composée, même si ils sont produits avec des moteurs graphiques différents. Un libellé automatique des axes et autres éléments du graphique en fonction des attributs label et units des variables (pour l’instant, seulement les graphiques de type ggplot2). # Importation des données (urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;)) # # A tibble: 421 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Pêche… 9.9 10.2 5 NA 0.522 0.478 # 2 Pêche… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Pêche… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Pêche… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Pêche… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Pêche… 10.5 11.1 5 NA 0.610 0.551 # 7 Pêche… 11 11 5.2 NA 0.672 0.605 # 8 Pêche… 11.1 11.2 5.7 NA 0.703 0.628 # 9 Pêche… 9.4 9.2 4.6 NA 0.413 0.375 # 10 Pêche… 10.1 9.5 4.7 NA 0.449 0.398 # # ... with 411 more rows, and 12 more variables: integuments &lt;dbl&gt;, # # dry_integuments &lt;dbl&gt;, digestive_tract &lt;dbl&gt;, # # dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, dry_gonads &lt;dbl&gt;, # # skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, spines &lt;dbl&gt;, # # maturity &lt;int&gt;, sex &lt;fct&gt; # Réalisation du graphique chart(data = urchin, height ~ weight %col=% origin) + geom_point() Figure 4.13: Graphique typique obtenu avec chart() : rendu par défaut publiable tel quel, et libellé automatique des axes avec les unités. 4.5.1 ggplot2 Le moteur graphique ggplot2 est écrit pas Hadley Wickham, un personnage emblématique de la “révolution tidyverse” qui propose une surcouche moderne au dessus de R. ggplot2 implémente une “grammaire graphique” particulièrement puissante et flexible, proposée et popularisée par le statisticien Leland Wilkinson. Par défaut, chart() crée en réalité un graphique ggplot2 adapté. Voici la version ggplot2 standard du même graphique représenté à la Fig. 4.13 : ggplot(data = urchin, mapping = aes(x = weight, y = height, col = origin)) + geom_point() Figure 4.14: Graphique typique obtenu avec ggplot() (moteur graphique ggplot2). En comparant les Figs. 4.13 et 4.14 (en faisant abstraction des instructions R utilisées pour l’instant), plusieurs points sautent immédiatement aux yeux: Le thème par défaut de ggplot2 est très reconnaissable avec un quadrillage blanc sur fond gris clair. On aime ou on n’aime pas, mais il est clair que (1) ce n’est pas une présentation “standard” d’un graphique scientifique, et (2) le thème tord un peu le cou à une règle importante pour réaliser un graphique de qualité : minimiser la quantité d’“encre” nécessaire pour représenter un graphique, autrement dit, plus le graphique est simple et sobre, mieux c’est. Le thème par défaut de chart() respecte mieux tout ceci15. La taille des caractères est légèrement plus grande dans la Fig. 4.13 réalisée avec chart(). Le manque de lisibilité des parties textuelles dans un graphique est un défaut fréquent, dépendant de la résolution et de la taille de reproduction du graphique dans le document final. Le choix de chart() recule un peu ce risque. chart() est capable d’aller lire les métadonnées (libellés en français et unités des variables) et les utilisent automatiquement pour proposer des libellés corrects et complets des axes par défaut. ggplot() ne peut pas le faire, et il faut utiliser la fonction labs() pour l’indiquer manuellement. De manière générale, par rapport à ggplot(), chart() a été conçu pour produire le graphique le plus proche d’un rendu final impeccable avec tous les paramètres par défaut. Quelques règles simples vous permettent de passer des instructions ggplot() à chart() et vice versa16 : On peut toujours remplacer ggplot() par chart() dans les instructions R (à condition que le package chart soit chargé bien sûr, par exemple via SciViews::R). Dans ce cas, le thème par défaut diffère, et le libellé automatique des axes (non disponible avec ggplot()) est activé. Avec chart() on peut utiliser aes() pour spécifier les “esthétiques” (éléments à visualiser sur le graphique) comme pour ggplot(), mais on peut aussi utiliser une interface formule plus compacte. Cette interface formule rapproche la version chart() des graphiques ggplot2 d’un autre moteur de graphique dans R : lattice. Outre les esthétiques classiques x et y, l’interface formule de chart() permet d’en inclure d’autres directement dans la formule à l’aide d’opérateurs spécifiques %&lt;esth&gt;%=. Par exemple, aes(x = weight, y = height, col = origin) dans la Fig. 4.14 se traduit en la formule plus concise height ~ weight %col=% origin avec chart() (notez la position inversée de x et y dans la formule puisqu’on a y ~ x). Tous les esthétiques de ggplot2 sont supportés de cette manière. Partout où aes() est utilisé pour les instructions ggplot2, on peut utiliser à la place f_aes() et y spécifier plutôt une formule de type chart(). Avec ggplot() les facettes doivent être spécifiées à l’aide de facet_XXX(). A condition d’utiliser chart(), il est possible d’inclure les spécifications des facettes les plus utilisées directement dans la formule en utilisant l’opérateur |. Cette façon de procéder est, encore une fois, identique à ce qui se fait dans lattice (voir plus loin). Le point (5) mérite une petite démonstration pour comparaison : a &lt;- chart(data = urchin, height ~ weight | origin) + geom_point() b &lt;- ggplot(data = urchin, mapping = aes(x = weight, y = height)) + geom_point() + facet_grid( ~ origin) combine_charts(list(a, b)) Figure 4.15: Graphique à facettes. A. version chart(), B. version ggplot(). 4.5.2 lattice Autant ggplot2 est complètement modulable en ajoutant littéralement à l’aide de l’opérateur + des couches successives sur le graphique, autant lattice vise à réaliser les graphiques en une seule instruction. lattice utilise également abondamment l’interface formule pour spécifier les variables à utiliser dans le graphique. La version lattice du graphique d’exemple est présentée à la Fig. 4.16. xyplot(height ~ weight, data = urchin, groups = origin, auto.key = TRUE) Figure 4.16: Graphique exemple réalisé avec lattice. Et voici la version chart() utilisant le moteur lattice. Notez la façon d’appeler la fonction xyplot() de lattice via chart$xyplot() : theme_sciviews_lattice(n = 2) a &lt;- chart$xyplot(height ~ weight, data = urchin, groups = origin, auto.key = list(space = &quot;right&quot;, title = &quot;Origine&quot;, cex.title = 1, columns = 1), ylab = &quot;Hauteur du test [mm]&quot;, xlab = &quot;Masse totale [g]&quot;, par.settings = list(superpose.symbol = list(col = scales::hue_pal()(2)))) b &lt;- chart(data = urchin, height ~ weight %col=% origin) + geom_point() combine_charts(list(a, b)) Figure 4.17: Graphique exemple réalisé avec chart() A. avec le moteur lattice, B. avec le moteur ggplot2. La quantité d’instructions nécessaires pour rendre la version lattice proche de la version ggplot2 devrait disparaître dans les prochaines versions de chart(). Un autre objectif est aussi de gommer le plus possible les différences entre les rendus des différents moteurs de graphiques R, et en particuliers entre ggplot2 et lattice. Comparez la Fig. 4.17A avec la Fig. 4.16 pour apprécier le gain déjà obtenu en matière d’homogénéisation. Par rapport à ggplot2, les graphiques lattice sont moins flexibles du fait qu’ils doivent être spécifiés en une seule instruction. Cependant, ils sont beaucoup plus rapides à générer (appréciable quand il y a beaucoup de points à tracer) ! lattice offre également quelques types de graphiques non supportés par ggplot2 comme les graphiques en 3D à facettes, par exemple. Voici un graphique à facettes réalisé avec chart() et le moteur lattice. Notez que la formule utilisée est identique à cette employée pour la version ggplot2 avec chart(). chart$xyplot(data = urchin, height ~ weight | origin, scales = list(alternating = 1), xlab = &quot;Masse totale [g]&quot;, ylab = &quot;Hauteur du test [mm]&quot;) Figure 4.18: Graphique à facettes, avec chart() version lattice. Mise à part les instructions additionnelles encore nécessaires dans cette version de chart(), l’appel et le rendu sont très similaires par rapport à la version ggplot2 du même graphique avec chart() : chart(data = urchin, height ~ weight | origin) + geom_point() Figure 4.19: Graphique à facettes, avec chart() version ggplot2. 4.5.3 Graphiques de base Comme son nom le suggère, le moteur graphique de base est celui qui est implémenté de manière natif dans R. Il est donc utilisé un peu partout. Il est vieillissant et est plus difficile à manipuler que ggplot2 certainement, et même que lattice. Néanmoins, il est très flexible et rapide, … mais son rendu par défaut n’est plus vraiment au goût du jour. Voici notre graphique d’exemple rendu avec le moteur graphique R de base : plot(urchin$weight, urchin$height, col = c(&quot;red&quot;, &quot;darkgreen&quot;)[urchin$origin], pch = 1) legend(x = 80, y = 10, legend = c(&quot;Culture&quot;, &quot;Pêcherie&quot;), col = c(&quot;red&quot;, &quot;darkgreen&quot;), pch = 1) Figure 4.20: Graphique exemple réalisé avec le moteur graphique R de base. Vous rencontrerez très fréquemment la fonction plot(). C’est une fonction dite générique dont le comportement change en fonction de l’objet fourni en premier argument. Ainsi, elle réalise le graphique le plus pertinent à chaque fois en fonction du contexte. Notez tout de suite les instructions un peu confuses nécessaires pour spécifier la couleur souhaitée en fonction de l’origine des oursins. Le moteur graphique de base ne gère pas automatiquement des aspects plus complexes du graphique, telle que le positionnement d’une légende. Donc, à moins d’avoir prévu la place suffisante avant de tracer le graphique, nous ne pouvons que l’inclure à l’intérieur du cadre du graphique dans un second temps à l’aide de la fonction legend(). Comme cette dernière ne comprend rien à ce qui a été réalisé jusqu’ici, il faut lui respécifier les couleurs, formes et tailles de points utilisés ! C’est un des aspects pénibles du moteur graphique R de base. Voici maintenant une version chart() de ce graphique de base : chart$base({ par(mar = c(5.1, 4.1, 4.1, 6.1)) plot(urchin$weight, urchin$height, col = scales::hue_pal()(2)[urchin$origin], pch = 19, cex = 0.8, xlab = &quot;Masse totale [g]&quot;, ylab = &quot;Hauteur du test [mm]&quot;) legend(x = 105, y = 20, legend = c(&quot;Culture&quot;, &quot;Pêcherie&quot;), title = &quot;Origine&quot;, col = scales::hue_pal()(2), pch = 19, bty = &quot;n&quot;, cex = 0.8, y.intersp = 2) }) Figure 4.21: Graphique exemple réalisé avec le moteur graphique de base et la fonction chart(). Notez que le graphique est généré deux fois : une première fois dans un format propre aux graphiques R de base, et ensuite, il est traduit en une forme compatible avec les autres graphiques ggplot2 et lattice (et au passage, il gagne la grille en traits grisés). Dans le chunck, nous devons spécifier fig.keep = 2 pour éviter d’imprimer la première version dans le rapport lorsqu’on utilise chart$base(). Pour l’instant, le seul avantage de chart() avec les graphiques de base est qu’il les convertit en une forme combinable avec les autres graphiques dans une figure composite (sinon, ce n’est pas possible). A part cela, il faut fournir à chart$base() tout le code nécessaire pour tracer et personnaliser le graphique. Comme on peut le voir sur cet exemple, cela demande une quantité considérable de code. C’est aussi un autre aspect pénible de ce moteur graphique : il est très flexible, mais l’interface n’est pas optimale. Pour finir, les graphiques de base ont plus de mal avec les facettes, mais il peuvent quand même générer les versions les plus simples, par exemple à l’aide de la fonction coplot() qui accepte une formule très similaire à ce qui s’utilise avec lattice : coplot(data = urchin, height ~ weight | origin) Figure 4.22: Graphique à facettes avec le moteur graphique de base. A l’issue de cette comparaison, vous pourrez décider du moteur graphique que vous préférerez utiliser. Dans le cadre de ce cours, nous n’utiliserons en tous cas que quasi-exclusivement des graphiques ggplot2 créés à l’aide la fonction chart(). A vous de jouer Proposez cinq graphiques inédits (qui n’ont pas été vu jusqu’ici) dans vos différents projets. Employez par exemple les liens suivants pour vous inspirer : https://www.r-graph-gallery.com http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html Terminez ce module en vérifiant que vous avez acquis l’ensemble des notions abordées. Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;04a_test&quot;) Pour en savoir plus Chapitre Data visualisation de R for Data Science qui utilise ggplot(). Site rassemblant des extensions pour ggplot2 Introduction rapide à lattice Variantes de graphiques avec lattice Comparaison de lattice et ggplot2. Cette page fait aussi référence à un ensemble de graphiques différents générés en lattice et en ggplot2 pour comparaison (en anglais). Divers exemples de graphiques réalisés avec le moteur de base Autres exemples de graphiques R de base ggplot2 comparé aux graphiques R de base. Un point de vue différent d’un utilisateur habitué aux graphiques R de base (en anglais). Notez que plusieurs thèmes existent dans ggplot2. Il est facile d’en changer et des les personnaliser… mais c’est toujours appréciable d’avoir un rendu impeccable dès le premier essai.↩ Etant donné l’abondante littérature écrite sur ggplot2, il est utile de pouvoir convertir des exemples ggplot2 en graphiques chart(), si vous êtes convaincu par cette nouvelle interface.↩ "],
["import.html", "Module 5 Traitement des données I", " Module 5 Traitement des données I Objectifs Savoir importer des données depuis différents formats et différentes sources via la fonction read(). Appréhender les types de variables et l’importance de les encoder convenablement. Etre capable de convertir des variables d’un type à l’autre, y compris par l’utilisation du découpage en classes pour passer de variable quantitative à qualitative. Savoir remanier des variables, filtrer un tableau et le résumer afin d’en extraire l’information importante. Prérequis Le contenu du module 1 doit être parfaitement maîtrisé. Il est également souhaitable, mais pas indispensable, de comprendre comment réaliser des graphiques dans R pour pouvoir comprendre le contenu de ce module. "],
["importation-des-donnees.html", "5.1 Importation des données", " 5.1 Importation des données Il est possible d’encoder des très petits jeux de données dans R. La fonction tribble() permet de le faire facilement. Notez que les noms des colonnes du tableau sont à rentrer sous forme de formules (~var), que chaque entrée est séparée par une virgule, et que les chaines de caractères sont entourées de guillemets. Les espaces sont optionnels et peuvent être utilisés pour aligner les données afin que le tout soit plus lisible. Des commentaires peuvent être utilisés éventuellement en fin de ligne (un dièse # suivi du commentaire). small_dataset &lt;- tribble( ~treatment, ~dose, ~response, &quot;control&quot;, 0.5, 18.35, &quot;control&quot;, 1.0, 26.43, # This value needs to be double-checked &quot;control&quot;, 2.0, 51.08, &quot;test&quot; , 0.5, 10.29, &quot;test&quot; , 1.0, 19.92, &quot;test&quot; , 2.0, 41.06) # Print the table small_dataset # # A tibble: 6 x 3 # treatment dose response # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 control 0.5 18.4 # 2 control 1 26.4 # 3 control 2 51.1 # 4 test 0.5 10.3 # 5 test 1 19.9 # 6 test 2 41.1 Dans la plupart des cas, vous utiliserez ou collecterez des données stockées dans des formats divers : feuilles Excel, fichiers CSV (“comma-separated-values”, un format standard d’encodage d’un tableau de données sous forme textuelle), formats spécifiques à divers logiciels statistiques comme SAS, Stata ou Systat, … Ces données peuvent être sur un disque local ou disponibles depuis un lien URL sur le net17. De nombreuses fonctions existent dans R pour importer toutes ces données. La fonction read() du package data.io est l’une des plus simples et conviviales d’entre-elles. Vous l’avez déjà utilisée, mais reprenons un exemple pour en discuter les détails. (biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;)) # # A tibble: 395 x 7 # gender day_birth weight height wrist year_measure age # &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 M 1995-03-11 69 182 15 2013 18 # 2 M 1998-04-03 74 190 16 2013 15 # 3 M 1967-04-04 83 185 17.5 2013 46 # 4 M 1994-02-10 60 175 15 2013 19 # 5 W 1990-12-02 48 167 14 2013 23 # 6 W 1994-07-15 52 179 14 2013 19 # 7 W 1971-03-03 72 167 15.5 2013 42 # 8 W 1997-06-24 74 180 16 2013 16 # 9 M 1972-10-26 110 189 19 2013 41 # 10 M 1945-03-15 82 160 18 2013 68 # # ... with 385 more rows Le jeu de données biometry est disponible dans le package R BioDataScience. Dans ce cas, il ne faut pas spécifier de chemin d’accès au fichier : R sait où le trouver tout seul. Il est également spécifié ici que la langue souhaitée est le français avec l’argument lang = &quot;fr&quot;. Le résultat de l’importation est assigné à la variable biometry(mais elle pourrait tout aussi bien porter un autre nom). Pour finir, le tout est entouré, de manière optionnelle, de parenthèses afin de forcer l’impression du résultat. Visualisez toujours votre tableau de données juste après l’importation. Vérifiez que les différentes colonnes ont été importées au bon format. En particulier, Les données numériques sont-elle bien comprises par R comme des nombres (&lt;dbl&gt; ou &lt;int&gt;) ? Les variables qualitatives ou semi-quantitatives sont importées comme chaines de caractères (&lt;chr&gt;) et doivent éventuellement être converties en variables de type facteur à l’aide de as.factor() ou facteur ordonné avec as.ordered(), voir plus loin. L’impression du tableau de données est une façon de voir cela, mais il y en a bien d’autres : essayez View(biometry), str(biometry), ou cliquez sur la petite icône bleue avec une flèche devant biometry dans l’onglet Environnement. Avant d’importer vos données dans R, vous devez vous poser les deux questions suivantes : Où ces données sont stockées ? Vous venez d’importer des données depuis un package R. Vous pouvez également les lire depuis un fichier sur le disque ou via une URL depuis le Web. Tous ces cas sont gérés par read() qui unifie donc de manière simple vos accès aux données. Quels est le format de vos données ? Souvent ce format est renseigné par l’extension du fichier. Par exemple .xlsx pour un Microsoft Excel ou .csv pour du “comma-separated-value”. Attention ! L’extension du fichier est cachée sous Windows, et parfois sous MacOS. Visualisez vos fichiers dans l’onglet Files dans RStudio pour voir leurs noms complets, avec les extensions. Pour l’instant, read() supporte 32 formats de fichiers différents, mais cette liste est amenée à s’agrandir à l’avenir. Pour découvrir les formats supportés, et les fonctions d’importation spécifiques appelées à chaque fois, utilisez : getOption(&quot;read_write&quot;) # # A tibble: 32 x 5 # type read_fun read_header write_fun comment # &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; # 1 csv readr::read_… data.io::hrea… readr::write_… comma separated valu… # 2 csv2 readr::read_… data.io::hrea… &lt;NA&gt; semicolon separated … # 3 xlcsv readr::read_… data.io::hrea… readr::write_… write a CSV file mor… # 4 tsv readr::read_… data.io::hrea… readr::write_… tab separated values # 5 fwf readr::read_… data.io::hrea… &lt;NA&gt; fixed width file # 6 log readr::read_… &lt;NA&gt; &lt;NA&gt; standard log file # 7 rds readr::read_… &lt;NA&gt; readr::write_… R data file (no comp… # 8 txt readr::read_… &lt;NA&gt; readr::write_… text file (as length… # 9 raw readr::read_… &lt;NA&gt; &lt;NA&gt; binary file (read as… # 10 ssv readr::read_… data.io::hrea… &lt;NA&gt; space separated valu… # # ... with 22 more rows Par la suite, vous allez apprendre à importer vos données depuis différentes sources. 5.1.1 Données sur le disque Lorsque l’extension du fichier reflète le format des données, il vous suffit juste d’indiquer le chemin d’accès au fichier à read(). La plupart du temps, cela suffira pour importer correctement les données. N’oubliez pas que le chemin d’accès à votre fichier peut s’écrire de manière absolue ou bien de manière relative. Vous devez autant que possible employer des chemins relatifs pour que votre projet soit portable. Si vous avez du mal à déterminer le chemin relatif par rapport à vos données, le snippet filerelchoose vous sera très utile : Assurez-vous que le chemin actif dans la fenêtre Console est le même que le répertoire contenant le fichier édité. Pour cela, utilisez l’entrée de menu RStudio Session -&gt; Set Working Directory -&gt; To Source File Location. Utilisez le snippet filerelchoose que vous activez dans une zone de code R (dans un script R, ou à l’intérieur d’un chunk dans un document R Markdown/R Notebook). Entrez file, attendez que le menu contextuel de complétion apparaisse, sélectionnez filerelchoose dans la liste et tapez Entrée. Une boite de dialogue de sélection de fichier apparaît. Sélectionnez le fichier qui vous intéresse et … file est remplacé par le chemin relatif vers votre fichier dans l’éditeur. Les explications détaillées concernant l’organisation de vos projets dans RStudio pour qu’ils soient portables, la gestion des chemins d’accès aux fichiers et les chemins relatifs sont détaillés dans l’annexe B, à la section B.1.1. C’est le moment de vérifier que vous avez bien compris et assimilé son contenu. Pièges et astuces Si l’extension est incorrecte, vous pouvez forcer un format de fichier particulier à l’importation en l’indiquant dans l’appel à read() comme read$&lt;ext&gt;(). Par exemple, pour forcer l’importation d’un fichier de type “comma-separated-values” pour un fichier qui se nommerait my_data.txt, vous écrirez read$csv(my_data.txt). Si les données ne sont pas importées correctement, cela signifie que les arguments d’importation par défaut ne sont pas adaptés. Les arguments à spécifier sont différents d’un format à l’autre. Voyez d’abord la fonction appelée en interne par read()dans le tableau obtenu via getOption(&quot;read_write&quot;). Par exemple, pour un fichier xlsx, il s’agit de la fonction readxl::read_excel() qui est utilisée. Ensuite, voyez l’aide de cette dernière fonction pour en découvrir les différents arguments (?readxl::read_excel). Là, vous pourrez découvrir les arguments sheet =qui indiquent la feuille à importer depuis le fichier (première feuille par défaut), ou range = qui indique la plage de données dans le feuille à utiliser (par défaut, depuis la cellule A1 en haut à gauche jusqu’à la fin du tableau). Donc, si votre fichier my_data.xlsx contient les feuilles sheet1, sheet2 et sheet3, et que les données qui vous intéressent sont dans la plage C5:E34 de sheet2, vous pourrez écrire: read(&quot;my_data.xlsx&quot;, sheet = &quot;sheet2&quot;, range = &quot;C5:E34&quot;). 5.1.2 Données depuis Internet Il existe différents logiciels qui permettent d’éditer des tableaux de données en ligne et de les partager sur le Net. Google Sheets est l’un d’entre eux, tout comme Excel Online. Des stockages spécifiques pour les données scientifiques existent aussi comme Figshare ou Zenodo. Ces sites permettent de partager facilement des jeux de données sur le Net. La science est de plus en plus ouverte, et les pratiques d’Open Data de plus en plus fréquentes et même imposées par des programmes de recherche comme les programmes européens ou le FNRS en Belgique. Vous serez donc certainement amenés à accéder à des données depuis des dépôts spécialisés sur Internet. Concentrez-vous sur les outils spécifiques à la gestion de ce type de données. il s’agit, en effet, d’une compétence clé qu’un bon scientifique des données se doit de maîtriser parfaitement. En recherchant à chaque fois la meilleure façon d’accéder à des données sur le Net, vous développerez cette compétence progressivement par la pratique… et vous pourrez faire valoir un atout encore rare mais apprécié lors d’un entretien d’embaûche plus tard. Voici un exemple de feuille de données Google Sheets : https://docs.google.com/spreadsheets/d/1iEuGrMk4IcCkq7gMNzy04DkSaPeWH35Psb0E56KEQMw. Il est possible d’importer ce genre de données directement depuis R, mais il faut d’abord déterminer l’URL à utiliser pour obtenir les données dans un format reconnu. Dans le cas de Google Sheets, il suffit d’indiquer à la fin de cette URL que l’on souhaite exporter les données au format CSV en rajoutant /export?format=csv à la fin de l’URL. Cette URL est très longue. Elle est peu pratique et par ailleurs, elle a toujours la même structure : &quot;https://docs.google.com/spreadsheets/d/{id}/export?format=csv&quot; avec {id} qui est l’identifiant unique de la feuille Google Sheets (ici 1iEuGrMk4IcCkq7gMNzy04DkSaPeWH35Psb0E56KEQMw). Vous pouvez indiquer explicitement ceci dans votre code et profiter des capacités de remplacement de texte dans des chaînes de caractères de la fonction glue::glue() pour effectuer un travail impeccable. googlesheets_as_csv &lt;- &quot;https://docs.google.com/spreadsheets/d/{id}/export?format=csv&quot; coral_id &lt;- &quot;1iEuGrMk4IcCkq7gMNzy04DkSaPeWH35Psb0E56KEQMw&quot; (coral_url &lt;- glue::glue(googlesheets_as_csv, id = coral_id)) # https://docs.google.com/spreadsheets/d/1iEuGrMk4IcCkq7gMNzy04DkSaPeWH35Psb0E56KEQMw/export?format=csv Vous n’aurez alors plus qu’à lire les données depuis cette URL. N’oubliez pas non plus de spécifier à read() que les données sont à lire au format CSV en utilisant read$csv() : (coral &lt;- read$csv(coral_url)) # Parsed with column specification: # cols( # localisation = col_character(), # species = col_character(), # id = col_integer(), # salinity = col_double(), # temperature = col_double(), # date = col_datetime(format = &quot;&quot;), # time = col_integer(), # gain = col_double(), # gain_std = col_double() # ) # # A tibble: 98 x 9 # localisation species id salinity temperature date # &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; # 1 A0 s.hyst… 1 34.7 24.5 2018-04-24 09:10:00 # 2 A0 s.hyst… 2 34.7 24.5 2018-04-24 09:10:00 # 3 A0 s.hyst… 3 34.7 24.5 2018-04-24 09:10:00 # 4 A0 s.hyst… 4 34.7 24.5 2018-04-24 09:10:00 # 5 A0 s.hyst… 5 34.7 24.5 2018-04-24 09:10:00 # 6 A0 s.hyst… 6 34.7 24.5 2018-04-24 09:10:00 # 7 A0 s.hyst… 7 34.7 24.5 2018-04-24 09:10:00 # 8 A0 s.hyst… 8 34.7 24.5 2018-04-24 09:10:00 # 9 A0 s.hyst… 9 34.7 24.5 2018-04-24 09:10:00 # 10 A0 s.hyst… 10 34.7 24.5 2018-04-24 09:10:00 # # ... with 88 more rows, and 3 more variables: time &lt;int&gt;, gain &lt;dbl&gt;, # # gain_std &lt;dbl&gt; Lorsque vous travaillez sur des données issues d’une source externe, et donc susceptibles d’être modifiées ou même pire, de disparaître. Il est préférable d’enregistrer une copie locale de ces données dans votre projet (dans le sous-dossier data de préférence). Si vous travaillez exclusivement avec R, l’un des meilleurs formats est RDS, un format natif qui conservera toutes les caractéristiques de votre objet, y compris sa classe, et d’éventuels attributs18. Par défaut, les données seront stockées non compressées, mais vous pourrez aussi décider de compresser avec les algorithmes &quot;gz&quot; (plus rapide et répandu), &quot;bz2&quot; (intermédiaire), ou &quot;xz&quot; (le plus efficace en taux de compression mais aussi le plus lent et gourmand en ressources CPU). Par exemple, pour enregistrer les données avec compression &quot;gz&quot;, vous écrirez : write$rds(coral, file = &quot;../data/coral.rds&quot;, compress = &quot;gz&quot;) Ensuite, vous pourrez simplement charger ces données plus loin depuis la version locale dans votre R Markdown comme ceci : coral &lt;- read(&quot;../data/coral.rds&quot;) Attention, ne supprimez jamais l’instruction permettant de retrouver vos données sur Internet sous prétexte que vous avez maintenant une copie locale à disposition. C’est le lien, le fil conducteur vers les données originales. Vous pouvez soit mettre l’instruction en commentaire en ajoutant un dièse devant, soit soustraire le chunk de l’évaluation en indiquant eval=FALSE dans son entête. Faites-en de même avec l’instruction write(). Ainsi, le traitement de vos données commencera à l’instruction read() et vous partirez de la copie locale. Si jamais vous voulez effectuer une mise à jour depuis la source initiale, il sera toujours possible de dé-commenter les instructions, ou de passer le chunk à eval=TRUE temporairement (ou encore plus simplement, forcez l’exécution du chunk dans l’éditeur en cliquant sur la petite flèche verte en haut à gauche du chunk). Pièges et astuces Comme il s’agit seulement d’une copie des données originelles, vous pouvez choisir de ne pas inclure le fichier .rds dans le système de gestion de version de Git. C’est très simple : il suffit d’ajouter une entrée .rds dans le fichier .gitignore à la racine de votre dépôt, et tous les fichiers avec cette extension seront ignorés. Notez toutefois que, si vous partagez votre projet sur GitHub, les données locales n’y apparaitront pas non plus. D’une part, cela décharge le système de gestion de version, et d’autre part, les gros fichiers de données n’ont pas vraiment leur place sur GitHub. Cependant, soyez conscient de ce que quelqu’un qui réalise un clone ou un fork de votre dépôt devra d’abord réimporter lui aussi localement les données avant de pouvoir travailler, ce qui implique de bien comprendre le mécanisme que vous avez mis en place. Documentez-le correctement, avec une note explicite dans le fichier README.md, par exemple. Les données originales ne sont peut-être pas présentées de la façon qui vous convient. Cela peut nécessiter un travail important de préparation du tableau de données. Au fur et à mesure que le ou les chunks d’importation/préparation des données augmentent en taille, ils deviennent de plus en plus gênants dans un document consacré à l’analyse de ces données. Si c’est le cas, vous avez deux options possibles : Séparer votre R Markdown en deux. Un premier document dédié à l’importation/préparation des données et un second qui se concentre sur l’analyse. Une bonne pratique consiste à numéroter les fichiers en tête pour qu’ils apparaissent par ordre logique lorsqu’ils sont listés par ordre alphabétique (01_import.Rmd, 02_analysis.Rmd). Effectuer le travail d’importation/préparation du tableau de données dans un script R. Dans le R Markdown, vous pouvez ajouter l’instruction (commentée ou placée dans un chunk eval=FALSE) pour “sourcer” ce script R afin de réimporter/retraiter vos données : #source(&quot;../R/data-import.R&quot;) Si le travail de préparation des données est lourd (et donc, prend beaucoup de temps) il peut être avantageux d’enregistrer localement la version nettoyée de vos données plutôt que la version originale. Mais alors indiquez-le explicitement. Faites toujours la distinction entre données brutes et données nettoyées. Ne les mélangez jamais et documentez toujours de manière reproductible le processus qui mène des unes aux autres ! C’est tout aussi important que de garder un lien vers la source originale des données dans votre code et d’utiliser toujours des chemins relatifs vers vos fichiers pour une analyse portable et reproductible. 5.1.3 Données depuis un package Les packages R comme data.io, chart ou encore flow, fournissent une série de fonctions supplémentaires. Certains d’entre eux proposent également des jeux de données. Ici aussi, read() permet de les récupérer, même si c’est la fonction data() qui est souvent utilisée à cet effet dans R. Comparons read() et data() dans le cas des données issues de packages R. Avec data(), vous n’assignez pas le jeu de données à un nom. Ce nom vous est imposé comme le nom initial du jeu de données : data(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) # package = optionnel si déjà chargé Le jeu de données urchin_bio n’est pas véritablement chargé dans l’environnement utilisateur avec data(). Seulement une “promesse” de chargement (Promise) est enregistrée. Voyez dans l’onglet Environnement ce qui apparaît. Ce n’est qu’à la première utilisation du jeu de données que le tableau est véritablement chargé. Par exemple : head(urchin_bio) # origin diameter1 diameter2 height buoyant_weight weight solid_parts # 1 Fishery 9.9 10.2 5.0 NA 0.5215 0.4777 # 2 Fishery 10.5 10.6 5.7 NA 0.6418 0.5891 # 3 Fishery 10.8 10.8 5.2 NA 0.7336 0.6770 # 4 Fishery 9.6 9.3 4.6 NA 0.3697 0.3438 # 5 Fishery 10.4 10.7 4.8 NA 0.6097 0.5587 # 6 Fishery 10.5 11.1 5.0 NA 0.6096 0.5509 # integuments dry_integuments digestive_tract dry_digestive_tract gonads # 1 0.3658 NA 0.0525 0.0079 0 # 2 0.4447 NA 0.0482 0.0090 0 # 3 0.5326 NA 0.0758 0.0134 0 # 4 0.2661 NA 0.0442 0.0064 0 # 5 0.4058 NA 0.0743 0.0117 0 # 6 0.4269 NA 0.0492 0.0097 0 # dry_gonads skeleton lantern test spines maturity sex # 1 0 0.1793 0.0211 0.0587 0.0995 0 &lt;NA&gt; # 2 0 0.1880 0.0205 0.0622 0.1053 0 &lt;NA&gt; # 3 0 0.2354 0.0254 0.0836 0.1263 0 &lt;NA&gt; # 4 0 0.0630 0.0167 0.0180 0.0283 0 &lt;NA&gt; # 5 0 NA NA NA NA 0 &lt;NA&gt; # 6 0 NA NA NA NA 0 &lt;NA&gt; Regardez à nouveau dans l’onglet Environnement. Ce coup-ci urchin_bio apparaît bien dans la section Data et l’icône en forme de petit tableau à la droite qui permet de le visualiser est enfin accessible. La fonction read() permet de choisir librement le nom que nous souhaitons donner à notre jeu de données. Si nous voulons l’appeler urchin au lieu de urchin_bio, pas de problèmes. De plus, il est directement chargé et accessible dans l’onglet Environnement (en effet, si on utilise une instruction qui charge un jeu de données, c’est très vraissemblablement parce que l’on souhaite ensuite le manipuler depuis R, non ?). urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) Nous avons déjà vu que read() donne accès également dans certains cas à des métadonnées (par exemple le label et les unités des jeux de données) dans différentes langues, ce que ne permet pas data(). Enfin, la syntaxe et la fonction utilisée sont pratiquement identiques pour charger des données depuis un fichier, depuis Internet ou depuis un package avec read(). C’est logique et facile à retenir. data() ne permet que de récupérer des données liées à un package R, et c’est tout ! Pour toutes ces raisons, nous préférons utiliser ici read() à data(). 5.1.3.1 Langue du jeu de données La fonction read() est également capable de lire un fichier annexe permettant de rajouter des métadonnées (données complémentaires) à notre tableau, comme les labels et les unités des variables en différentes langues. Lorsque l’on importe le jeu de données avec la fonction data(), ces métadonnées ne sont pas employées. data(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) # Visualisation des données chart(urchin_bio, height ~ weight %col=% origin) + geom_point() Comparez ceci avec le même graphique, mais obtenu à partir de différentes versions du jeu de données urchin_bio importé à l’aide de read() avec des valeurs différentes pour l’argument lang =. urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) urchin_en &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;en&quot;) urchin_fr &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;) urchin_FR &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) Les différences dans les labels sont observables sur le graphique ci-dessous. a &lt;- chart(urchin, height ~ weight %col=% origin) + geom_point() b &lt;- chart(urchin_en, height ~ weight %col=% origin) + geom_point() c &lt;- chart(urchin_fr, height ~ weight %col=% origin) + geom_point() d &lt;- chart(urchin_FR, height ~ weight %col=% origin) + geom_point() combine_charts(list(a, b, c, d)) A &amp; B : l’argument lang = par défaut est lang = &quot;en&quot;. Il utilise les labels et unités en anglais avec les unités dans le système international. C : l’argument lang = &quot;fr&quot; utilise les labels et unités en français. Il laisse cependant les niveaux des variables facteurs en anglais (Farm et Fishery) afin d’éviter de devoir changer les instructions de manipulation des données qui feraient référence à ces niveaux. D : l’argument lang = &quot;FR&quot; ajoute les labels et unités en français. De plus, il traduit également les niveaux des variables facteurs (Culture et Pêcherie). Il vous est conseillé d’employé l’argument lang = &quot;fr&quot; lors de vos différents travaux. La langue internationale en science est l’anglais et vous serez très certainement amené dans votre carrière scientifique à produire des documents en français et en anglais. L’utilisation de lang = &quot;fr&quot;rend le même code réutilisable sur la version française ou anglaise, contrairement à lang = &quot;FR&quot;. Observez les exemples ci-dessous. urchin_en %&gt;.% filter(., origin == &quot;Farm&quot;) %&gt;.% head(.) # # A tibble: 6 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Farm 53.1 54.5 26.3 9.57 60.2 41.7 # 2 Farm 52.7 52.7 25.9 10.8 63.2 46.6 # 3 Farm 54 54.2 24.5 10.7 64.4 44.3 # 4 Farm 51.1 51.3 28.8 11.2 62.4 45.0 # 5 Farm 52.1 53.6 31.2 11.1 63.7 44.0 # 6 Farm 52.3 51.4 28.6 12.4 68.6 53.9 # # ... with 12 more variables: integuments &lt;dbl&gt;, dry_integuments &lt;dbl&gt;, # # digestive_tract &lt;dbl&gt;, dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, # # dry_gonads &lt;dbl&gt;, skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, # # spines &lt;dbl&gt;, maturity &lt;int&gt;, sex &lt;fct&gt; urchin_fr %&gt;.% filter(., origin == &quot;Farm&quot;) %&gt;.% head(.) # # A tibble: 6 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Farm 53.1 54.5 26.3 9.57 60.2 41.7 # 2 Farm 52.7 52.7 25.9 10.8 63.2 46.6 # 3 Farm 54 54.2 24.5 10.7 64.4 44.3 # 4 Farm 51.1 51.3 28.8 11.2 62.4 45.0 # 5 Farm 52.1 53.6 31.2 11.1 63.7 44.0 # 6 Farm 52.3 51.4 28.6 12.4 68.6 53.9 # # ... with 12 more variables: integuments &lt;dbl&gt;, dry_integuments &lt;dbl&gt;, # # digestive_tract &lt;dbl&gt;, dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, # # dry_gonads &lt;dbl&gt;, skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, # # spines &lt;dbl&gt;, maturity &lt;int&gt;, sex &lt;fct&gt; Pas d’adaptation nécessaire du code pour passer de urchin_en à urchin_fr. urchin_FR %&gt;.% filter(., origin == &quot;Pêcherie&quot;) %&gt;.% head(.) # # A tibble: 6 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Pêche… 9.9 10.2 5 NA 0.522 0.478 # 2 Pêche… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Pêche… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Pêche… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Pêche… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Pêche… 10.5 11.1 5 NA 0.610 0.551 # # ... with 12 more variables: integuments &lt;dbl&gt;, dry_integuments &lt;dbl&gt;, # # digestive_tract &lt;dbl&gt;, dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, # # dry_gonads &lt;dbl&gt;, skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, # # spines &lt;dbl&gt;, maturity &lt;int&gt;, sex &lt;fct&gt; Le code a dû être modifier dans l’instruction filter() lors du passage à urchin_FR (Farm -&gt; Pêcherie). Bien évidemment, pour un rapport plus formel en français, tout doit être traduit en français et l’option lang = &quot;FR&quot; accompagnée d’une vérification et une adaptation éventuelle du code est à préférer dans ce cas précis. R permet également d’interroger des bases de données spécialisées, mais nous n’aborderons ce sujet spécifique qu’au cours de Science des Données Biologique 5 en Master 2.↩ Si vous devez aussi accéder à vos données à partir d’autres langages comme Python, Java ou C++, utilisez un format commun reconnu par les différents logiciels. Le CSV fonctionne généralement bien, mais des formats binaires plus performants sont également disponibles. Parmi ces formats “inter-langages”, gardez un œil sur Apache Arrow très prometteur et avec une version pour R qui sera disponible prochainement.↩ "],
["types-de-variables.html", "5.2 Types de variables", " 5.2 Types de variables Lors de la réalisation de graphiques dans les modules précédents vous avez compris que toutes les variables ne sont pas équivalentes. Certains graphiques sont plutôt destinés à des variables qualitatives (par exemple, graphique en barres), alors que d’autres représentent des données quantitatives comme le nuage de points. (biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;)) # # A tibble: 395 x 7 # gender day_birth weight height wrist year_measure age # &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 M 1995-03-11 69 182 15 2013 18 # 2 M 1998-04-03 74 190 16 2013 15 # 3 M 1967-04-04 83 185 17.5 2013 46 # 4 M 1994-02-10 60 175 15 2013 19 # 5 W 1990-12-02 48 167 14 2013 23 # 6 W 1994-07-15 52 179 14 2013 19 # 7 W 1971-03-03 72 167 15.5 2013 42 # 8 W 1997-06-24 74 180 16 2013 16 # 9 M 1972-10-26 110 189 19 2013 41 # 10 M 1945-03-15 82 160 18 2013 68 # # ... with 385 more rows La Figure 5.1 montre deux boites de dispersion parallèles différentes. Laquelle de ces deux représentations est incorrecte et pourquoi ? a &lt;- chart(biometry, height ~ gender %fill=% gender) + geom_boxplot() b &lt;- chart(biometry, height ~ weight %fill=% gender) + geom_boxplot() combine_charts(list(a, b), common.legend = TRUE) # Warning: position_dodge requires non-overlapping x intervals Figure 5.1: Boites de dispersion parallèles de la taille (height) en fonction de A. une variable qualitative (gender) et B. une variable quantitative (weight) et couleur en fonction de gender` C’est la figure 5.1B qui tente de représenter une variable quantitative numérique heightsous forme de boites de dispersion parallèles (correct), mais en fonction d’une variable de découpage en sous-ensemble (weight) qui est elle-même une variable quantitative, … alors qu’une variable qualitative telle que gender aurait dû être utilisée (comme dans la Fig. 5.1A). Dans le cas présent, R a bien voulu réaliser le graphique (avec juste un petit message d’avertissement), mais comment l’interpréter ? Dans d’autres situations, il vous renverra purement et simplement un message d’erreur. Les jeux de données, lorsqu’ils sont bien encodés (tableaux “cas par variables”, en anglais on parlera de tidy data) sont en fait un ensemble de variables en colonnes mesurées sur un ensemble d’individus en lignes. Vous avez à votre disposition plusieurs types de variables pour personnaliser le jeu de données. Deux catégories principales de variables existent, chacune avec deux sous-catégories : Les variables quantitatives sont issues de mesures quantitatives ou de dénombrements Les variables quantitatives continues sont représentées par des valeurs réelles (double dans R) Les variables quantitatives discrètes sont typiquement représentées par des entiers (integer dans R) Les variables qualitatives sont constituées d’un petit nombre de valeurs possibles (on parle des niveaux de la variables ou de leurs modalités) Les variables qualitatives ordonnées ont des niveaux qui peuvent être classés dans un ordre du plus petit au plus grand. elles sont typiquement représentées dans R par des objets ordered. Les variables qualitatives non ordonnées ont des niveaux qui ne peuvent être rangés et sont typiquement représentées par des objets factor en R Il existe naturellement encore d’autres types de variables. Les dates sont représentées, par exemple, par des objets Date, les nombres complexes par complex, les données binaires par raw, etc. La fonction skim() du package skimr permet de visualiser la classe de la variable et bien plus encore. Elle fournit un résumé différent en fonction du type de la variable et propose, par exemple, un histogramme stylisé pour les variables numériques comme le montre le tableau ci-dessous. skimr::skim(biometry) # Skim summary statistics # n obs: 395 # n variables: 7 # # ── Variable type:Date ─────────────────────────────────────────────────────────────── # variable missing complete n min max median n_unique # day_birth 0 395 395 1927-08-29 2000-08-11 1988-10-05 210 # # ── Variable type:factor ───────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # gender 0 395 395 2 M: 198, W: 197, NA: 0 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 # age 0 395 395 35.34 17.32 15 19 27 50 # height 0 395 395 170.71 9.07 146 164 171 177 # weight 0 395 395 71.2 15.45 41.5 59 69.3 80 # wrist 2 393 395 16.65 1.67 10 15.5 16.5 18 # year_measure 0 395 395 2015.32 1.61 2013 2014 2016 2017 # p100 hist # 89 ▇▂▁▅▂▁▁▁ # 193 ▁▂▆▆▇▅▃▁ # 131 ▂▇▇▆▂▁▁▁ # 23 ▁▁▂▇▇▂▁▁ # 2017 ▅▅▁▁▁▅▁▇ Avec une seule instruction, on obtient une quantité d’information sur notre jeu de données comme le nombre d’observations, le nombre de variables et un traitement spécifique pour chaque type de variable. Cette instruction permet de visualiser et d’appréhender le jeu de données mais ne doit généralement pas figurer tel quel dans un rapport d’analyse. "],
["conversion-de-variables.html", "5.3 Conversion de variables", " 5.3 Conversion de variables Il est possible de convertir les variables seulement dans un sens : du plus détaillé au moins détaillé, c’est-à-dire, quantitatif continu -&gt; quantitatif discret -&gt; qualitatif ordonné -&gt; qualitatif non ordonné. 5.3.1 Quantitatif continu à discret R essaye de gommer autant que possible la distinction entre nombres integer et double tous deux rassemblés en numeric. Si besoin, la conversion se fait automatiquement. En pratique, concentrez-vous essentiellement sur les objets numeric pour tout ce qui est quantitatif. Un nombre tel que 1 est considéré par R comme un double par défaut. Si vous vouliez expressément spécifier que c’est un entier, vous pouvez le faire en ajoutant un L majuscule derrière le nombre. Ainsi, 1L est compris par R comme l’entier 1. Encore une fois, cette distinction explicite est rarement nécessaire dans R. Si vous voulez arrondir des nombres, vous pouvez utiliser la fonction round() avec son argument digits = qui indique le chiffre derrière la virgule qui doit être arrondi (0 par défaut). Pour arrondir vers l’entier le plus proche vers le haut, utilisez floor() et pour le plus proche vers le bas, employez ceiling(). (x &lt;- seq(-1, 1, by = 0.1) + 0.01) # [1] -0.99 -0.89 -0.79 -0.69 -0.59 -0.49 -0.39 -0.29 -0.19 -0.09 0.01 # [12] 0.11 0.21 0.31 0.41 0.51 0.61 0.71 0.81 0.91 1.01 round(x) # [1] -1 -1 -1 -1 -1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 round(x, digits = 1) # [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 # [15] 0.4 0.5 0.6 0.7 0.8 0.9 1.0 ceiling(x) # [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 floor(x) # [1] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 0 0 0 0 0 0 0 0 1 5.3.2 Quantitatif à qualitatif Le traitement diffère selon le nombre de valeurs différentes rencontrées dans le jeu de données. Si une variable numérique contient en réalité un petit nombre de valeurs différentes, il suffit de convertir la classe de l’objet de numeric vers factor ou ordered pour que R comprenne que la variable doit être traitée comme une variable qualitative. Un exemple concret l’illustre ci-dessous. Si, par contre, le nombre de valeurs différentes est important (dizaines ou plus) alors il va falloir créer des regroupements. C’est le découpage en classes abordé plus loin. Voici un jeu de données qui étudie l’allongement des dents chez le cobaye en fonction de la supplémentation alimentaire en acide ascorbique. tooth &lt;- read(&quot;ToothGrowth&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) Le jeu de données comprend 60 observations effectuées sur des cochons d’Inde. Ces derniers reçoivent deux types de suppléments alimentaires : soit du jus d’orange (OJ), soit de la vitamine C (VC). Des lots différents reçoivent des doses différentes d’acide ascorbique via ces suppléments, soit 0.5, 1, ou 2 mg/j. Vous pouvez inspecter ces données rapidement avec la fonction skim(). skimr::skim(tooth) # Skim summary statistics # n obs: 60 # n variables: 3 # # ── Variable type:factor ───────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # supp 0 60 60 2 OJ: 30, VC: 30, NA: 0 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # dose 0 60 60 1.17 0.63 0.5 0.5 1 2 2 # len 0 60 60 18.81 7.65 4.2 13.07 19.25 25.27 33.9 # hist # ▇▁▇▁▁▁▁▇ # ▃▅▃▅▃▇▂▂ La variable dose est encodée sous forme numérique alors que cette dernière ne contient que trois niveaux différents et devra être le plus souvent traitée comme une variable qualitative ordonnée à trois niveaux . Vous devrez donc probablement recoder cette variable en variable facteur. Ce n’est pas le caractère quantitatif ou qualitatif du mécanisme sous-jacent qui est mesuré qui détermine si la variable est quantitative ou qualitative, mais d’autres critères comme la précision avec laquelle la mesure a été effectuée. Par exemple, un anémomètre mesure la vitesse du vent sous forme de variable quantitative alors qu’une échelle approximative de type vent nul, vent faible, vent moyen, vent fort ou tempête basée sur l’observation des rides ou des vagues à la surface de la mer pourrait éventuellement convenir pour mesurer le même phénomène si une grande précision n’est pas nécessaire. Mais dans ce cas, la variable devra être traitée comme une variable qualitative. De même, un plan expérimental qui réduit volontairement les valeurs fixées dans une expérience, comme ici les doses journalières d’acide ascorbique, fera aussi basculer la variable en qualitative, et ce, quelle que soit la précision avec laquelle les valeurs sont mesurées par ailleurs. Un découpage en classes aura aussi le même effet de transformer une variable quantitative en variable qualitative ordonnée. Indiquons à présent explicitement à R que la variable dose doit être considérée comme qualitative : tooth$dose &lt;- as.factor(tooth$dose) # Visualisation des données skimr::skim(tooth) # Skim summary statistics # n obs: 60 # n variables: 3 # # ── Variable type:factor ───────────────────────────────────────────────────────────── # variable missing complete n n_unique # dose 0 60 60 3 # supp 0 60 60 2 # top_counts ordered # 0.5: 20, 1: 20, 2: 20, NA: 0 FALSE # OJ: 30, VC: 30, NA: 0 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # len 0 60 60 18.81 7.65 4.2 13.07 19.25 25.27 33.9 # hist # ▃▅▃▅▃▇▂▂ Vous pouvez (et devez !) cependant aller encore plus loin car la variable est en réalité qualitative ordonnée, et doit être représentée par un objet “facteur ordonné” (ordered) plutôt que factor. Il y a en effet, une progression dans les doses administrées. Lors de la conversion, R considère les différents niveaux par ordre alphabéthique par défaut. Ici cela convient, mais ce n’est pas toujours le cas. Il vaut donc mieux spécifier explicitement l’ordre des niveaux dans l’argument optionnel levels =. Cela donne : tooth$dose &lt;- ordered(tooth$dose, levels = c(0.5, 1, 2)) # Visualisation des données skimr::skim(tooth) # Skim summary statistics # n obs: 60 # n variables: 3 # # ── Variable type:factor ───────────────────────────────────────────────────────────── # variable missing complete n n_unique # dose 0 60 60 3 # supp 0 60 60 2 # top_counts ordered # 0.5: 20, 1: 20, 2: 20, NA: 0 TRUE # OJ: 30, VC: 30, NA: 0 FALSE # # ── Variable type:numeric ──────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # len 0 60 60 18.81 7.65 4.2 13.07 19.25 25.27 33.9 # hist # ▃▅▃▅▃▇▂▂ Les fonctions as.factor() ou factor() et as.ordered() ou ordered() effectuent cette conversion de character ou numeric vers des objets factor ou ordered. Une variable facteur ordonnée sera alors reconnue comme telle par un ensemble de fonction dans R. Elle ne sera, de ce fait, pas traitée de la même manière qu’une variable facteur non ordonnée, ni même qu’une variable numérique. Soyez bien attentif à l’encodage correct des données dans R avant d’effectuer vos graphiques et vos analyses. 5.3.3 Découpage en classes La conversion d’une variable quantitative à qualitative doit souvent passer par une réduction des niveaux en rassemblant les valeurs proches dans des classes. Vous avez déjà utilisé de manière implicite le découpage en classes lorsque vous avez réalisé des histogrammes. Si les histogrammes sont bi- ou multimodaux, un découpage se justifie. Par exemple, le jeu de données portant sur la biométrie humaine est typique d’un cas de distribution bimodale. En fait, ce sont des étudiants (ayant tous une vingtaine d’années) qui ont réalisé ces mesures. La plupart ont choisi de s’inclure dans l’échantillon, d’où un premier mode vers une vingtaine d’années. Ensuite, ils ont pu mesurer d’autres personnes, éventuellement dans leur entourage. Beaucoup ont demandé à leurs parents, ce qui résulte en un second mode vers la cinquantaine19. Donc, la distribution bimodale résulte plus de l’échantillonnage en lui-même que d’une réalité démographique ! Cela ne change cependant rien pour l’exercice. biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;) chart(data = biometry, ~ age) + geom_histogram(bins = 20) + ylab(&quot;Effectifs&quot;) Les addins de RStudio vont vous permettre de réaliser facilement un découpage du jeu de données en fonction de classes d’âges (bouton Addins -&gt; QUESTIONR -&gt; Numeric range dividing). Vous spécifiez le découpage voulu dans une boite de dialogue sur base de l’histogramme et lorsque vous cliquez sur le bouton Done, le code R qui effectue ce découpage est inséré dans l’éditeur RStudio à l’endroit du curseur. la nouvelle variable facteur age_rec basée sur le découpage en classes va être utile pour faire ressortir ensuite de l’information supplémentaire en contrastant les individus plus jeunes et ceux plus âgés. # Instructions obtenues à partir de l&#39;addins biometry$age_rec &lt;- cut(biometry$age, include.lowest = FALSE, right = TRUE, breaks = c(14, 27, 90)) # Visualisation de la variable facteur obtenue chart(biometry, formula = ~ age %fill=% age_rec) + geom_histogram(bins = 20) + ylab(&quot;Effectifs&quot;) 5.3.4 Qualitatif ordonné ou non Les données qualitatives sont souvent représentées par du texte (nom d’une couleur par exemple) et importées sous forme de chaines de caractère (character) par défaut dans R à partir de la fonction read(). Vous devez les convertir de manière explicite à l’aide de as.factor(), factor(), as.ordered() ou ordered() par la suite. Voici un exemple : df &lt;- tibble( color = c(&quot;blue&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), intensity = c(&quot;low&quot;, &quot;low&quot;, &quot;high&quot;, &quot;mid&quot;, &quot;high&quot;)) df # # A tibble: 5 x 2 # color intensity # &lt;chr&gt; &lt;chr&gt; # 1 blue low # 2 green low # 3 blue high # 4 red mid # 5 green high # Conversion en factor (color) et ordered (intensity) df$color &lt;- factor(df$color, levels = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) df$intensity &lt;- ordered(df$intensity, levels = c(&quot;low&quot;, &quot;mid&quot;, &quot;high&quot;)) df # # A tibble: 5 x 2 # color intensity # &lt;fct&gt; &lt;ord&gt; # 1 blue low # 2 green low # 3 blue high # 4 red mid # 5 green high # Information plus détaillée str(df) # Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 2 variables: # $ color : Factor w/ 3 levels &quot;red&quot;,&quot;green&quot;,..: 3 2 3 1 2 # $ intensity: Ord.factor w/ 3 levels &quot;low&quot;&lt;&quot;mid&quot;&lt;&quot;high&quot;: 1 1 3 2 3 skimr::skim(df) # Skim summary statistics # n obs: 5 # n variables: 2 # # ── Variable type:factor ───────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # color 0 5 5 3 gre: 2, blu: 2, red: 1, NA: 0 # intensity 0 5 5 3 low: 2, hig: 2, mid: 1, NA: 0 # ordered # FALSE # TRUE Les différents niveaux des variables factor ou ordered sont et doivent rester entièrement de votre responsabilité. Certains aspects anciens de R essayent de gérer cela pour vous, mais ces fonctions ou options (StringsAsFactor = par exemple) tendent heureusement à être remplacées par des versions moins assertives. De même, les niveaux ne sont pas réduits lorsque vous filtrez un tableau pour ne retenir que certains niveaux. Vous devez indiquer explicitement ensuite que vous voulez éliminer les niveaux vides du tableau avec la fonction droplevels(). Le jeu de données iris contient des données relatives à trois espèces différentes (table() permet de compter le nombre d’observations pour chaque niveau d’une variable qualitative factor ou ordered) : iris &lt;- read(&quot;iris&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) table(iris$species) # # setosa versicolor virginica # 50 50 50 Si nous restreignons le tableau aux 20 premiers individus, cela donne : iris20 &lt;- iris[1:20, ] table(iris20$species) # # setosa versicolor virginica # 20 0 0 Nous voyons que le tableau réduit iris20 ne contient des données que d’une seule espèce. Pourtant table() continue de lister les autres niveaux de la variable. Les niveaux connus sont aussi imprimés avec levels() : levels(iris20$species) # [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; Dans le cas ici, nous souhaitons peut-être nous focaliser uniquement sur l’espèce I. setosa. Dans ce cas, droplevels() permettra de faire disparaître les autres niveaux de la variable species. iris20$species &lt;- droplevels(iris20$species) levels(iris20$species) # [1] &quot;setosa&quot; table(iris20$species) # # setosa # 20 Notez que ceci ne constitue pas un échantillonnage correct par rapport à la population générale du Hainaut pour plusieurs raisons. (1) toutes les tranches d’âges ne sont échantillonnées de manière équivalente pour les raisons évoquées, (2) des liens génétiques existent au sein des familles, ce qui résulte en une non indépendance des observations entre elles, et (3) seule une sous-population constituée de personnes fréquentant l’université et de leur entourage a été échantillonnée. Cependant, dans le cadre de l’exercice, nous accepterons ces biais, tout en étant conscients qu’ils existent.↩ "],
["remaniement-des-donnees.html", "5.4 Remaniement des données", " 5.4 Remaniement des données Dans le module 4, vous avez réalisé vos premiers remaniements de données dans le cadre des graphiques en barres. Nous ne nous sommes pas étendu sur les fonctions utilisées à cette occasion. Le remaniement des données est une étape cruciale en analyse des données et il faut en maîtriser au moins les principaux outils. Heureusement, il est déjà possible d’aller loin en combinant une petite dizaine d’outils simples. Les cinq principaux (les plus utilisés) en tout cas dans l’approche Tidyverse utilisée ici sont : sélectionner des colonnes au sein d’un jeu de données avec select() filtrer des lignes dans un jeu de données avec filter() calculer de nouvelles variables dans un jeu de données avec mutate() regrouper les données au sein d’un tableau avec group_by() résumer les variables d’un jeu de données avec summarise() Ces outils provenant du package dplyr sont décrits en détails dans le chapitre 5 de “R for Data Science”. Nous allons nous familiariser avec eux via une approche pratique sur base d’exemples concrets. urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;) rmarkdown::paged_table(urchin) 5.4.1 select() Lors de l’utilisation de vos jeux de données, vous serez amené à réduire vos données en sous-tableau ne reprenant qu’un sous-ensemble des variables initiales. select() effectue cette opération20 : urchin1 &lt;- select(urchin, origin, solid_parts, test) rmarkdown::paged_table(urchin1) urchin2 &lt;- select(urchin, c(1, 4, 14)) rmarkdown::paged_table(urchin2) urchin3 &lt;- select(urchin, origin, contains(&quot;weight&quot;)) rmarkdown::paged_table(urchin3) urchin4 &lt;- select(urchin, ends_with(&quot;ht&quot;)) rmarkdown::paged_table(urchin4) 5.4.2 filter() De même que toutes les colonnes d’un tableau ne sont pas forcément utiles, il est souvent nécessaire de sélectionner les lignes en fonction de critères particuliers pour restreindre l’analyse à une sous-population données, ou pour éliminer les cas qui ne correspondent pas à ce que vous voulez. La fonction filter() effectue ce travail. Repartons du jeu de données urchin_bio simplifié à trois variables (urchin2). rmarkdown::paged_table(urchin2) Si vous voulez sélectionner uniquement un niveau lvl d’une variable facteur fact, vous pouvez utiliser un test de condition “égal à” (==) : fact == &quot;lvl&quot;. Notez bien le double signe égal ici, et n’oubliez pas d’indiquer le niveau entre guillemets. De même, vous pouvez sélectionner tout sauf ce niveau avec l’opérateur “différent de” (!=). Les opérateur “plus petit que” (&lt;) ou “plus grand que” (&gt;) fonctionnent sur les chaines de caractère selon une logique d’ordre alphabétique, donc, &quot;a&quot; &lt; &quot;b&quot;21. Comparaison Opérateur Exemple Égal à == fact == &quot;lvl&quot; Différent de != fact != &quot;lvl&quot; Plus grand que &gt; fact &gt; &quot;lvl&quot; Plus grand ou égal à &gt;= fact &gt;= &quot;lvl&quot; Plus petit que &lt; fact &lt; &quot;lvl&quot; Plus petit ou égale à &lt;= fact &lt;= &quot;lvl&quot; # Tous les oursins sauf ceux issus de la pêche urchin_sub1 &lt;- filter(urchin2, origin != &quot;Fishery&quot;) rmarkdown::paged_table(urchin_sub1) Vous pouvez aussi utiliser une variable numérique pour filtrer les données. Les comparaisons précédentes sont toujours applicables, sauf que cette fois vous faites porter la comparaison par rapport à une constante (ou par rapport à une autre variable numérique). # Oursins plus hauts que 20mm urchin_sub2 &lt;- filter(urchin2, height &gt; 20) rmarkdown::paged_table(urchin_sub2) Vous pouvez combiner différentes comparaisons avec les opérateurs “et” (&amp;) et “ou” (|) : # Oursins plus hauts que 20 mm ET issus d&#39;élevage (&quot;Farm&quot;) urchin_sub3 &lt;- filter(urchin2, height &gt; 20 &amp; origin == &quot;Farm&quot;) rmarkdown::paged_table(urchin_sub3) Avec des variables facteurs composées des nombreux niveaux comme on peut en retrouver dans le jeu de données zooplankton du package BioDataScience, vous pouvez être amené à sélectionner plusieurs niveaux au sein de cette variable. L’opérateur %in% permet d’indiquer que nous souhaitons garder tous les niveaux qui sont dans une liste. Il n’existe pas d’opérateur %not_in%, mais il suffit d’inverser le résultat en précédent l’instruction de ! pour obtenir cet effet. Par exemple, !letters %in% c(&quot;a&quot;, &quot;d&quot;, &quot;f&quot;) conserve toutes les lettres sauf a, d et f. L’opérateur ! est d’ailleurs utilisable avec toutes les comparaisons pour en inverser les effets. Ainsi, !x == 1 est équivalent à x != 1. zooplankton &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) # Garde uniquement les copépodes (correspondant à 4 groupes distincts) copepoda &lt;- filter(zooplankton, class %in% c(&quot;Calanoïde&quot;, &quot;Cyclopoïde&quot;, &quot;Harpacticoïde&quot;, &quot;Poecilostomatoïde&quot;)) rmarkdown::paged_table(select(copepoda, ecd:perimeter, class)) Enfin, la détection et l’élimination de lignes contenant des valeurs manquantes (encodées comme NA) est spéciale. En effet, vous ne pouvez pas écrire quelque chose comme x == NA car ceci se lit comme “x est égale à … je ne sais pas quoi”, ce qui renvoie à son tour NA pour toutes les comparaisons quelles qu’elles soient. Vous pouvez utiliser la fonction spécialement prévue pour ce test is.na(). Ainsi, is.na(x) effectue en réalité ce que vous voulez et peut être utilisée à l’intérieur de filter(). Cependant, il existe une fonction spécialement prévue pour débarrasser les tableaux des lignes contenant des valeurs manquantes : drop_na() du package tidyr. Si vous spécifier des noms de colonnes (facultatifs), la fonction ira rechercher les valeurs manquantes uniquement dans ces colonnes-là, sinon, elle scrutera tout le tableau. urchin_sub4 &lt;- drop_na(urchin) rmarkdown::paged_table(urchin_sub4) 5.4.3 mutate() La fonction mutate() permet de calculer de nouvelles variables (si le nom fourni n’existe pas encore dans le jeu de donnée) ou écrase les variables existantes de même nom. Repartons du jeu de données urchin. Pour calculer de nouvelles variables, vous pouvez employer : les opérateurs arithmétiques : addition : + soustraction : - multiplication : * division : / exposant : ^ modulo (reste lors d’une division entière) : %% division entière : %/% urchin &lt;- mutate(urchin, sum_skel = lantern + spines + test, ratio = sum_skel / skeleton, skeleton2 = skeleton^2) rmarkdown::paged_table(select(urchin, skeleton:spines, sum_skel:skeleton2)) les fonctions mathématiques : ln() ou log() (logarithme népérien), lg() ou log10() (logarithme en base 10) ln1p() ou log1p() (logarithme népérien de x + 1), ou lg1p() (logarithme en base 10 de x + 1) exp() (exponentielle, ex) et expm1() (ex - 1) sqrt() (racine carrée) sin(), cos(), tan() … urchin &lt;- mutate(urchin, skeleton_log = log(skeleton), skeleton_sqrt = sqrt(skeleton)) rmarkdown::paged_table(select(urchin, skeleton, skeleton_log, skeleton_sqrt)) La fonction transmute() effectue la même opération, mais en plus, elle laisse tomber les variables d’origine pour ne garder que les nouvelles variables calculées. 5.4.4 group_by() La fonction group_by() ne change rien dans le tableau lui-même, mais ajoute une annotation qui indique que les calculs ultérieurs devront être effectués sur des sous-ensembles du tableau en parallèle. Ceci est surtout utile avec summarise() (voir ci-dessous). Pour annuler le regroupement, il suffit d’utiliser ungroup(). urchin_by_orig &lt;- group_by(urchin, origin) head(urchin_by_orig) # # A tibble: 6 x 24 # # Groups: origin [1] # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Fishe… 9.9 10.2 5 NA 0.522 0.478 # 2 Fishe… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Fishe… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Fishe… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Fishe… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Fishe… 10.5 11.1 5 NA 0.610 0.551 # # ... with 17 more variables: integuments &lt;dbl&gt;, dry_integuments &lt;dbl&gt;, # # digestive_tract &lt;dbl&gt;, dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, # # dry_gonads &lt;dbl&gt;, skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, # # spines &lt;dbl&gt;, maturity &lt;int&gt;, sex &lt;fct&gt;, sum_skel &lt;dbl&gt;, ratio &lt;dbl&gt;, # # skeleton2 &lt;dbl&gt;, skeleton_log &lt;dbl&gt;, skeleton_sqrt &lt;dbl&gt; identical(ungroup(urchin_by_orig), urchin) # [1] TRUE 5.4.5 summarise() Si vous voulez résumer vos données (calcul de la moyenne, médiane, etc.), vous pouvez réaliser ceci sur une variable en particulier avec les fonctions dédiées. Par exemple mean(urchin$skeleton) renvoie la masse moyenne de squelette pour tous les oursins (ce calcul donne NA dès qu’il y a des valeurs manquantes, mais l’argument na.rm = TRUE permet d’obtenir un résultat en ne tenant pas compte de ces données manquantes : mean(urchin$skeleton, na.rm = TRUE)). Cela devient vite laborieux s’il faut réitérer ce genre de calcul sur plusieurs variables du jeu de données, et assembler ensuite les résultats dans un petit tableau synthétique. D’autant plus, s’il faut séparer d’abord le jeu de données en sous-groupes pour faire ces calculs. La fonction summarise() reporte automatiquement ces calculs, en tenant compte automatiquement des regroupements proposés via group_by(). tooth &lt;- read(&quot;ToothGrowth&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) tooth_summary &lt;- summarise(tooth, &quot;moyenne&quot; = mean(len), &quot;minimum&quot; = min(len), &quot;médiane&quot; = median(len), &quot;maximum&quot; = max(len)) knitr::kable(tooth_summary, digits = 2, caption = &quot;Allongement des dents chez des cochons d&#39;Inde recevant de l&#39;acide ascorbique.&quot;) Tableau 5.1: Allongement des dents chez des cochons d’Inde recevant de l’acide ascorbique. moyenne minimum médiane maximum 18.81 4.2 19.25 33.9 Voici les mêmes calculs, mais effectués séparément pour les deux types de supplémentations alimentaires : tooth_by_supp &lt;- group_by(tooth, supp) tooth_summary2 &lt;- summarise(tooth_by_supp, &quot;moyenne&quot; = mean(len), &quot;minimum&quot; = min(len), &quot;médiane&quot; = median(len), &quot;maximum&quot; = max(len)) knitr::kable(tooth_summary2, digits = 2, caption = &quot;Allongement des dents chez des cochons d&#39;Inde en fonction du supplément jus d&#39;orange (OJ) ou vitamine C (VC).&quot;) Tableau 5.2: Allongement des dents chez des cochons d’Inde en fonction du supplément jus d’orange (OJ) ou vitamine C (VC). supp moyenne minimum médiane maximum OJ 20.66 8.2 22.7 30.9 VC 16.96 4.2 16.5 33.9 Pièges et astuces Tout comme lors de réalisation d’une boite de dispersion, vous devez être particulièrement vigilant au nombre d’observation par sous-groupe. Pensez toujours à ajoutez à chaque tableau de résumé des données, le nombre d’observation par sous-groupe grâce à la fonction n(). tooth_summary2 &lt;- summarise(tooth_by_supp, &quot;moyenne&quot; = mean(len), &quot;minimum&quot; = min(len), &quot;médiane&quot; = median(len), &quot;maximum&quot; = max(len), &quot;n&quot; = n()) knitr::kable(tooth_summary2, digits = 2, caption = &quot;Allongement des dents chez des cochons d&#39;Inde en fonction du supplément jus d&#39;orange (OJ) ou vitamine C (VC).&quot;) Tableau 5.3: Allongement des dents chez des cochons d’Inde en fonction du supplément jus d’orange (OJ) ou vitamine C (VC). supp moyenne minimum médiane maximum n OJ 20.66 8.2 22.7 30.9 30 VC 16.96 4.2 16.5 33.9 30 Voyez ?select_helpers pour une panoplie de fonctions supplémentaires qui permettent une sélection “intelligente” des variables.↩ L’ordre alphabétique qui fait également intervenir les caractères accentués diffère en fonction de la configuration du système (langue). L’état du système tel que vu par R pour le tri alphabétique est obtenu par Sys.getlocale(&quot;LC_COLLATE&quot;). Dans la SciViews Box, ceci est toujours &quot;en_US.UTF-8&quot;, ceci afin de rendre le traitement reproductible d’un PC à l’autre, qu’il soit en anglais, français, espagnol, chinois, ou n’importe quelle autre langue.↩ "],
["chainage-des-instructions.html", "5.5 Chaînage des instructions", " 5.5 Chaînage des instructions Le chaînage (ou “pipe” en anglais) permet de combiner une suite d’instructions R. Il permet une représentation facilement lisible et compréhensible d’un traitement décomposé en plusieurs étapes simples de remaniement des données. Différents opérateurs de chaînage existent dans R. Le Tidyverse et RStudio sont en faveur de l’adoption d’un opérateur de chaînage %&gt;% issu du package magrittr. Si nous sommes sensibles au clin d’œil fait ici à un artiste belge bien connu (“ceci n’est pas un pipe”), nous n’adhérons pas à ce choix pour des raisons multiples et plutôt techniques qui n’ont pas leur place dans ce document22. Nous vous présentons ici l’un des opérateurs de chaînage du package flow : %&gt;.%. Le jeu de données sur la biométrie humaine est employé pour cette démonstration qui va comparer le remaniement d’un tableau de données avec et sans l’utilisation du chaînage. biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;) Vous vous intéressez à l’indice de masse corporelle ou IMC (BMI en anglais) des individus de moins de 25 ans. Vous souhaitez représenter la moyenne, la médiane et le nombre d’observations de manière séparée pour les hommes et les femmes. Pour obtenir ces résultats vous devez : calculer le BMI, filtrer le tableau pour ne retenir que les individus de moins de 25 ans, résumer les données afin d’obtenir la moyenne et la médiane par genre, afficher un tableau de données avec ces résultats. Il est très clair ici que le traitement peut être décomposé en étapes plus simples. Cela apparaît naturellement rien que dans la description de ce qui doit être fait. Sans l’utilisation de l’opérateur de chaînage, deux approches sont possibles : Imbriquer les instructions les unes dans les autres (très difficile à lire et à déboguer) : knitr::kable( summarise( group_by( filter( mutate(biometry, bmi = weight / (height/100)^2), age &lt;= 25), gender), mean = mean(bmi), median = median(bmi), number = n()), rows = NULL, digits = 1, col = c(&quot;Genre&quot;, &quot;Moyenne&quot;, &quot;Médiane&quot;, &quot;Observations&quot;), caption = &quot;IMC d&#39;hommes (M) et femmes (W) de 25 ans maximum.&quot; ) Tableau 5.4: IMC d’hommes (M) et femmes (W) de 25 ans maximum. Genre Moyenne Médiane Observations M 22.3 22.1 97 W 21.8 21.0 94 Passer par des variables intermédiaires (biometry_25 et biometry_tab). Les instructions sont plus lisibles, mais les variables intermédiaires “polluent” inutilement l’environnement de travail (en tout cas, si elles ne servent plus par après) : biometry &lt;- mutate(biometry, bmi = weight / (height/100)^2) biometry_25 &lt;- filter(biometry, age &lt;= 25) biometry_25 &lt;- group_by(biometry_25, gender) biometry_tab &lt;- summarise(biometry_25, mean = mean(bmi), median = median(bmi), number = n()) knitr::kable(biometry_tab, rows = NULL, digits = 1, col = c(&quot;Genre&quot;, &quot;Moyenne&quot;, &quot;Médiane&quot;, &quot;Observations&quot;), caption = &quot;IMC d&#39;hommes (M) et femmes (W) de 25 ans maximum.&quot;) Tableau 5.5: IMC d’hommes (M) et femmes (W) de 25 ans maximum. Genre Moyenne Médiane Observations M 22.3 22.1 97 W 21.8 21.0 94 Des trois approches, la version ci-dessous avec chaînage des opérations est la plus lisible et la plus pratique23. biometry %&gt;.% mutate(., bmi = weight / (height/100)^2) %&gt;.% filter(., age &lt;= 25) %&gt;.% group_by(., gender) %&gt;.% summarise(., mean = mean(bmi), median = median(bmi), number = n()) %&gt;.% knitr::kable(., rows = NULL, digits = 1, col = c(&quot;Genre&quot;, &quot;Moyenne&quot;, &quot;Médiane&quot;, &quot;Observations&quot;), caption = &quot;IMC d&#39;hommes (M) et femmes (W) de 25 ans maximum.&quot;) Tableau 5.6: IMC d’hommes (M) et femmes (W) de 25 ans maximum. Genre Moyenne Médiane Observations M 22.3 22.1 97 W 21.8 21.0 94 Le pipe %&gt;.% injecte le résultat précédent dans l’instruction suivante à travers l’objet . Ainsi, en seconde ligne mutate(.), . se réfère à biometry. A la ligne suivante, filter(.), le . se réfère au résultat issu de l’opération mutate(), et ainsi de suite. La logique d’enchaînement des opérations sur le résultat, à chaque fois, du calcul précédent est donc le fondement de cet opérateur “pipe”. Le pipe permet d’éviter de répéter le nom des objets (version avec variables intermédiaires), ce qui alourdit inutilement le code et le rend moins agréable à la lecture. L’imbrication des fonctions dans la première version est catastrophique pour la compréhension du code car les arguments des fonctions de plus haut niveau sont repoussés loin. Par exemple, l’argument de l’appel à group_by() (gender) se retrouve quatre lignes plus loin. Et encore, nous avons pris soin d’indenter le code pour repérer sur un plan vertical qui appartient à qui, mais imaginez ce que cela donne si l’instruction est mise à plat sur une seule ligne ! Le code le plus clair à la lecture est définitivement celui avec chaînage des opérations. Or, un code plus lisible est plus compréhensible… et donc, moins bogué. A vous de jouer Maintenant que vous venez d’apprendre à importer correctement vos données, à les remanier avec quelques-uns des opérateurs les plus fréquents, et que vous savez chaîner vos instructions, il est temps de vous exercer sur un cas concret. Une tâche individuelle vous est assignée via l’URL suivante : https://classroom.github.com/a/WfxTmH4b Créez un rapport et effectuez les différents exercices en suivant les instructions qui sont dans le fichier README.md de ce dépôt GitHub Classroom. Terminez ce module en vérifiant que vous en avez acquis les notions principales. Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;05a_test&quot;) Des challenges vous sont proposés afin d’améliorer vos compétences en remaniement de données. Ces derniers sont disponibles ici. Pour en savoir plus Présentation en détail du “dot-pipe” assez proche fonctionnellement de %&gt;.% du package flow. Section sur le pipe dans “R for Data Science” expliquant l’utilisation du pipe de magrittr (et aussi quand ne pas l’utiliser !) Le lecteur intéressé pourra lire les différents articles suivants : more pipes in R, y compris les liens qui s’y trouvent, permet de se faire une idée de la diversité des opérateurs de chaînage dans R et de leur historique. Dot pipe présente l’opérateur %.&gt;% du package wrapr très proche du nôtre et in praise of syntactic sugar explique ses avantages. Nous partageons l’idée que le “pipe de base” ne devrait pas modifier l’instruction de droite contrairement à ce que fait %&gt;% de magrittr, et notre opérateur %&gt;.% va en outre plus loin encore que %.&gt;% dans la facilité de débogage du code chaîne.↩ Le chaînage n’est cependant pas forcément plus facile à déboguer que la version avec variables intermédiaires. Le package flow propose la fonction debug_flow() à appeler directement après un plantage pour inspecter la dernière instruction qui a causé l’erreur, voir ?debug_flow.↩ "],
["qualit.html", "Module 6 Traitement des données II", " Module 6 Traitement des données II Objectifs Comprendre les principaux tableaux de données utilisés en science des données Savoir réaliser des tableaux de contingences Acquérir des données et les encoder correctement et de manière à ce que les analyses soient reproductibles Etre capable de remanier des tableaux de données et de fusionner plusieurs tableaux Prérequis Ce module est la continuation du module 5 dont le contenu doit être bien compris et maîtrisé avant de poursuivre ici. "],
["tableaux-de-donnees.html", "6.1 Tableaux de données", " 6.1 Tableaux de données Les tableaux de données sont principalement représentés sous deux formes : les tableaux cas par variables et les tableaux de contingence. 6.1.1 Tableaux cas par variables Chaque individus est représenté en ligne et chaque variable en colonne par convention. En anglais, on parlera de tidy data. Nous nous efforcerons de toujours créer un tableau de ce type pour les données brutes. La question à se poser est la suivante : est-ce que j’ai un seul et même individu représenté sur chaque ligne du tableau ? Si la réponse est non, le tableau de données n’est pas correctement encodé. TODO : exemple et solution Les tableaux de données que vous avez traités jusqu’à présent étaient tous des tableaux cas par variables. Chaque ligne représentait un individu sur qui une ou plusieurs variables (en colonnes) étaient mesurées. biometry &lt;- read(&quot;biometry&quot;, package = &quot;BioDataScience&quot;, lang = &quot;fr&quot;) head(biometry) # # A tibble: 6 x 7 # gender day_birth weight height wrist year_measure age # &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 M 1995-03-11 69 182 15 2013 18 # 2 M 1998-04-03 74 190 16 2013 15 # 3 M 1967-04-04 83 185 17.5 2013 46 # 4 M 1994-02-10 60 175 15 2013 19 # 5 W 1990-12-02 48 167 14 2013 23 # 6 W 1994-07-15 52 179 14 2013 19 L’encodage d’un petit tableau cas par variables directement dans R est facile. Cela peut se faire de plusieurs façons différentes. En voici deux utilisant les fonctions tibble() (spécification colonne par colonne, utilisez le snippet .dmtibble pour vous aider) et tribble() (spécification ligne par ligne, utilisez le snippet .dmtribble) : # Spécification colonne par colonne avec tibble() (df &lt;- as_dataframe(tibble( x = c(1, 2), y = c(3, 4) ))) # # A tibble: 2 x 2 # x y # &lt;dbl&gt; &lt;dbl&gt; # 1 1 3 # 2 2 4 # Spécification ligne par ligne avec tribble() (df1 &lt;- as_dataframe(tribble( ~x, ~y, 1, 3, 2, 4 ))) # # A tibble: 2 x 2 # x y # &lt;dbl&gt; &lt;dbl&gt; # 1 1 3 # 2 2 4 La seconde approche est plus naturelle, mais la première permet d’utiliser diverses fonctions de R pour faciliter l’encodage, par exemple : Séquence d’entiers successifs : 1:10 # [1] 1 2 3 4 5 6 7 8 9 10 Répétition d’un vecteur 5 fois : rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), 5) # [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; Répétition de chaque item d’un vecteur 5 fois : rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 5) # [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;b&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; Pour de plus gros tableaux, il vaut mieux utiliser un tableur tel que Excel ou LibreOffice Calc pour l’encodage. Les tableurs en ligne comme Google Sheets ou Excel Online conviennent très bien également et facilitent un travail collaboratif ainsi que la mise à disposition sut le Net, comme nous avons vu au module 5. 6.1.2 Tableaux de contingence C’est le dénombrement de l’occurrence de chaque niveau d’une (tableau à une entrée) ou de deux variables qualitatives (tableau à double entrée). La fonction table() crée ces deux types de tableaux de contingence à partir de données encodées en tableau cas par variables : biometry$age_rec &lt;- cut(biometry$age, include.lowest = FALSE, right = TRUE, breaks = c(14, 27, 90)) (bio_tab &lt;- table(biometry$gender, biometry$age_rec)) # # (14,27] (27,90] # M 106 92 # W 97 100 Le tableau de contingence peut toujours être calculé à partir d’un tableau cas par variable, mais il peut également être encodé directement si nécessaire. Voici un petit tableau de contingence à simple entrée encodé directement comme tel (vecteur nommé transformé en objet table à l’aide de la fonction as.table()) : anthirrhinum &lt;- as.table(c( &quot;fleur rouge&quot; = 54, &quot;fleur rose&quot; = 122, &quot;fleur blanche&quot; = 58) ) anthirrhinum # fleur rouge fleur rose fleur blanche # 54 122 58 Une troisième possibilité est d’utiliser un tableau indiquant les fréquences d’occurence dans une colonne (freq ci-dessus). Ce n’est pas un tableau cas par variable, mais une forme bien plus concise et pratique pour pré-encoder les données qui devront être ensuite transformées en tableau de contingence à l’aide de la fonction xtabs(). Voici un exemple pour un tableau de contingence à double entrée. Notez que le tableau cas par variable correspondant devrait contenir 44 + 116 + 19 + 128 = 307 lignes et serait plus fastidieux à construire et à manipuler (même en utilisant la fonction rep()). timolol &lt;- tibble( traitement = c(&quot;timolol&quot;, &quot;timolol&quot;, &quot;placebo&quot;, &quot;placebo&quot;), patient = c(&quot;sain&quot;, &quot;malade&quot;, &quot;sain&quot;, &quot;malade&quot;), freq = c(44, 116, 19, 128) ) # Creation du tableau de contingence timolol_table &lt;- xtabs(data = timolol, freq ~ patient + traitement) timolol_table # traitement # patient placebo timolol # malade 128 116 # sain 19 44 La sortie par défaut d’un tableau de contingence n’est pas très esthétique, mais plusieurs options existent pour le formater d’une façon agréable. En voici deux exemples : pander::pander(timolol_table, caption = &quot;Exemple de table de contingence à double entrée.&quot;) Exemple de table de contingence à double entrée. placebo timolol malade 128 116 sain 19 44 knitr::kable(timolol_table, caption = &quot;Exemple de table de contingence à double entrée.&quot;) Tableau 6.1: Exemple de table de contingence à double entrée. placebo timolol malade 128 116 sain 19 44 Il est même possible de représenter graphiquement un tableau de contingence pour l’inclure dans une figure composée, éventuellement en le mélangeant avec des graphiques24. tab1 &lt;- ggpubr::ggtexttable(head(biometry), rows = NULL) tab2 &lt;- ggpubr::ggtexttable(table(biometry$gender, biometry$age_rec)) combine_charts(list(tab1, tab2), nrow = 2) Différentes fonctions dans R existent également pour convertir un tableau de contingence en tableau cas par variables (ou en tous cas, en un tableau similaire). Par exemple, as_dataframe() renvoie un tableau indiquant les fréquences d’occurrences : (timolol2 &lt;- as_dataframe(timolol_table, n = &quot;freq&quot;)) # # A tibble: 4 x 3 # patient traitement freq # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; # 1 malade placebo 128 # 2 sain placebo 19 # 3 malade timolol 116 # 4 sain timolol 44 Si vous insistez, vous pouvez aussi obtenir un tableau cas par variables (mais celui-ci est très long et peu pratique à manipuler) à l’aide de la fonction uncount()25 : uncount(timolol2, freq) # # A tibble: 307 x 2 # patient traitement # &lt;chr&gt; &lt;chr&gt; # 1 malade placebo # 2 malade placebo # 3 malade placebo # 4 malade placebo # 5 malade placebo # 6 malade placebo # 7 malade placebo # 8 malade placebo # 9 malade placebo # 10 malade placebo # # ... with 297 more rows 6.1.3 Métadonnées Les données dans un tableau de données doivent impérativement être associées à un ensemble de métadonnées. Les métadonnées (“metadata” en anglais) apportent des informations complémentaires nécessaires pour une interprétation correcte des données. Elles permettent donc de replacer les données dans leur contexte et de spécifier des caractéristiques liées aux mesures réalisées comme les unités de mesure par exemple. \\[Donn\\acute{e}es \\ de \\ qualit\\acute{e} \\ = \\ tableau \\ de \\ donn\\acute{e}es + \\ m\\acute{e}tadonn\\acute{e}es\\] Les données correctement qualifiées et documentée sont les seules qui peuvent être utilisées par un collaborateur externe. C’est à dire qu’une personne externe à l’expérience ne peut interpréter le tableau de données que si les métadonnées sont complètes et explicites. Exemple de métadonnées : Unités de mesure (exemple : 3,5 mL, 21,2 °C) Précision de la mesure (21,2 +/- 0,2 dans le cas d’un thermomètre gradué tous les 0,2 °C) Méthode de mesure utilisée (thermomètre à mercure, ou électronique, ou …) Type d’instrument employé (marque et modèle du thermomètre par exemple) Date de la mesure Nom du projet lié à la prise de mesure Nom de l’opérateur en charge de la mesure … Vous avez pu vous apercevoir que la fonction read() permet d’ajouter certaines métadonnées comme les unités aux variables d’un jeu de données. Cependant, il n’est pas toujours possible de rajouter les métadonnées dans un tableau sous forme électronique, mais il faut toujours les consigner dans un cahier de laboratoire, et ensuite les retranscrire dans le rapport. La fonction labelise() vous permet de rajouter le label et les unités de mesure pour vos différentes variables directement dans le tableau. Par exemple, voici l’encodage direct d’un petit jeu de données qui mesure la distance du saut (jump) en cm de grenouilles taureaux en fonction de leur masse (weight) en g pour 5 individus différents (ind). Vous pouvez annoter ce data frame de la façon suivante : frog &lt;- tribble( ~ind, ~jump, ~weight, 1, 71, 204, 2, 70, 240, 3, 100, 296, 4, 120, 303, 5, 103, 422 ) # Ajout des labels et des unités frog &lt;- labelise(frog, self = FALSE, label = list( ind = &quot;Individu&quot;, jump = &quot;Distance du saut&quot;, weight = &quot;Masse&quot;), units = list( jump = &quot;cm&quot;, weight = &quot;g&quot;) ) # Affichage synthétique des données et métadonnées associées str(frog) # Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 3 variables: # $ ind : atomic 1 2 3 4 5 # ..- attr(*, &quot;label&quot;)= chr &quot;Individu&quot; # $ jump : atomic 71 70 100 120 103 # ..- attr(*, &quot;label&quot;)= chr &quot;Distance du saut&quot; # ..- attr(*, &quot;units&quot;)= chr &quot;cm&quot; # $ weight: atomic 204 240 296 303 422 # ..- attr(*, &quot;label&quot;)= chr &quot;Masse&quot; # ..- attr(*, &quot;units&quot;)= chr &quot;g&quot; # Affichage des labels label(frog) # ind jump weight # &quot;Individu&quot; &quot;Distance du saut&quot; &quot;Masse&quot; Les métadonnées sont enregistrées dans des attributs en R (attr). De même, comment() permet d’associer ou de récupérer un attribut commentaire : # Ajout d&#39;un commentaire concernant le jeu de données lui-même comment(frog) &lt;- &quot;Saut de grenouilles taureaux&quot; # Ajout d&#39;un commentaire sur une variable comment(frog$jump) &lt;- &quot;Premier saut mesuré après stimulation de l&#39;animal&quot; # Affichage synthétique str(frog) # Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 3 variables: # $ ind : atomic 1 2 3 4 5 # ..- attr(*, &quot;label&quot;)= chr &quot;Individu&quot; # $ jump : atomic 71 70 100 120 103 # ..- attr(*, &quot;label&quot;)= chr &quot;Distance du saut&quot; # ..- attr(*, &quot;units&quot;)= chr &quot;cm&quot; # ..- attr(*, &quot;comment&quot;)= chr &quot;Premier saut mesuré après stimulation de l&#39;animal&quot; # $ weight: atomic 204 240 296 303 422 # ..- attr(*, &quot;label&quot;)= chr &quot;Masse&quot; # ..- attr(*, &quot;units&quot;)= chr &quot;g&quot; # - attr(*, &quot;comment&quot;)= chr &quot;Saut de grenouilles taureaux&quot; # Récupération des commentaires comment(frog) # [1] &quot;Saut de grenouilles taureaux&quot; comment(frog$jump) # [1] &quot;Premier saut mesuré après stimulation de l&#39;animal&quot; comment(frog$weight) # Rien! # NULL 6.1.4 Dictionnaire des données Le dictionnaire des données est un élément important de la constitution d’une base de données. Il s’agit d’un tableau annexe qui reprend le nom de chaque variable, son label (nom plus long et explicite), son type (numérique, facteur, facteur ordonné, date, …), la taille (de moindre importance pour nous), et un commentaire éventuel. Dans notre contexte, il est également utile de renseigner les unités de mesure, et la façon dont les données manquantes sont encodées. Cela donne donc un tableau du genre : Variable Label Unités Type Val. manquantes Commentaire date Date - Date NA Date de mesure age Âge années numeric -1 diameter Diamètre du test mm numeric NA Moyenne de deux diamètres perpendiculaires origin Origine - factor unknown “Fishery” = oursins sauvages, “Farm” = oursins d’élevage Ce tableau peut-être encodé sous forme textuelle et placé dans le même dossier que le jeu de données lui-même. Il peut aussi être encodé comme feuille supplémentaire dans une fichier Excel. Le dictionnaire des données est un outil important pour comprendre ce que contient le tableau de données, et donc, son interprétation. Ne le négligez pas ! Utilisez cette option avec parcimonie : il vaut toujours mieux représenter un tableau comme … un tableau plutôt que comme une figure !↩ Notez également que passer d’un tableau cas par variables à un tableau des fréquences d’occurrences se fait à l’aide de count().↩ "],
["population-et-echantillonnage.html", "6.2 Population et échantillonnage", " 6.2 Population et échantillonnage TODO: partie encore à écrire DT::datatable(iris) "],
["acquisition-de-donnees.html", "6.3 Acquisition de données", " 6.3 Acquisition de données Dans le module 5, vous avez pris connaissance des types de variable et venez d’apprendre comment encoder différents types de tableaux de données et de leurs associer les indispensables métadonnées. Cependant, la première étape avant d’acquérir des données est de planifier correctement son expérience. La Science des Données est intimement liée à la démarche scientifique et intervient dans toutes les étapes depuis la caractérisation de la question et le planning de l’expérience jusqu’à la diffusion des résultats. Plus en détails, cela correspond à : Définir une question (objectif) Réaliser une recherche bibliographique sur la thématique Définir le protocole de l’expérience à partir de l’objectif Définir la population étudiée et l’échantillonnage Définir les variables à mesurer Définir les unité des mesures Définir la précision des mesures Définir les instruments de mesure nécessaires Définir les conventions d’encodage Codifier l’identification des individus Définir les niveaux des variables facteurs et leurs labels Acquérir et encoder les données Traiter les données Importer des données Remanier des données Visualiser et décrire des données Analyser les données (traitements statistiques, modélisation,…). Produire des supports de présentation pour répondant à la question de départ et diffuser l’information dans la communauté scientifique Nous traitons ici des premières étapes qui visent à acquérir les données. 6.3.1 Précision et exactitude Les erreurs de mesures sont inévitables lors de l’acquisition de nos données. Cependant, il est possible de les minimiser en choisissant un instrument plus précis (“precise” en anglais) et plus exact (“accurate” en anglais). La figure ci-dessous illustre de manière visuelle la différence qu’il y a entre précision et exactitude. 6.3.2 Codification des données Afin d’éviter que divers collaborateurs encodent différemment la même information, vous allez devoir préciser très clairement comment encoder les différentes variables de votre jeu de données. Par exemple pour une variable genre, est-ce que vous indiquez homme ou femme, ou h / f, ou encore H / F ? De même, vous allez devoir attribuer un code unique à chaque individu mesuré. Enfin, vous devez vous assurer que toutes les mesures sont réalisées de la même manière et avec des instruments qui, s’ils sont différents, seront cependant intercalibrés. Comment faire ? Réfléchissez à cette question sur base d’une mesure de la masse des individus à l’aide de pèse-personnes différents ! 6.3.2.1 Respect de la vie privée Lors d’expérience sur des personnes, le respect de la vie privée doit être pris en compte26. Le nom et le prénom, ou toute autre information permettant de retrouver les individus étudiés (adresse mail, numéro de sécurité sociale, etc.) ne peut pas apparaître dans la base de données consolidée. En outre, il vous faudra un accord explicite des personnes que vous voulez mesurer, et il faudra leur expliquer ce que vous faites, et comment les données seront ensuite utilisées. Une question se pose : comment pouvoir revenir vers les enregistrements liés à un individu en particulier (en cas d’erreur d’encodage, par exemple) si les informations relatives directement à ces individus ne sont pas consignées dans le tableau final ? Réfléchissez à la façon dont vous vous y prendriez avant de lire la suite… Voici un petit tableau qui correspond à ce que vous ne pourrez pas faire (nom et prénom explicitement mentionnés dans le tableau) : (biometry_marvel &lt;- as_dataframe(tribble( ~id, ~sex ,~weight, ~height, &quot;Banner Bruce&quot;, &quot;M&quot;, 95, 1.91, &quot;Stark Tonny&quot;, &quot;M&quot;, 80, 1.79, &quot;Fury Nicholas&quot;, &quot;M&quot;, 82, 1.93, &quot;Romanoff Natasha&quot;, &quot;F&quot;, 53, 1.70 ))) # # A tibble: 4 x 4 # id sex weight height # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Banner Bruce M 95 1.91 # 2 Stark Tonny M 80 1.79 # 3 Fury Nicholas M 82 1.93 # 4 Romanoff Natasha F 53 1.7 Vous devez fournir une code permettant de garder l’anonymat des sondés à l’ensemble des personnes étudiées vis à vis des analystes qui vont utiliser ces données. Cependant, le code doit permettre au chercheur ayant pris ces mesures de les retrouver dans son cahier de laboratoire, si besoin. Une façon de procéder consiste à attribuer un numéro au hasard par tirage dans une urne à chacune des personnes chargées des mesures. Ensuite, chaque expérimentateur attribue lui-même un second numéro aux différentes personnes qu’il mesure. Prenons par exemple le scientifique n°24 (seul lui sait qu’il porte ce numéro). Il attribue un code de 1 à n à chaque personne étudiée. En combinant le code secret de l’expérimentateur et le code individu, cela donne un identifiant unique de la forme 24_1, 24_2, etc. Il pourra alors encoder sa partie comme suit : (biometry_marvel1 &lt;- as_dataframe(tribble( ~id, ~sex , ~weight, ~height, &quot;24_1&quot;, &quot;M&quot;, 95, 1.91, &quot;24_2&quot;, &quot;M&quot;, 80, 1.79, &quot;24_3&quot;, &quot;M&quot;, 82, 1.93, &quot;24_4&quot;, &quot;F&quot;, 53, 1.70 ))) # # A tibble: 4 x 4 # id sex weight height # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 24_1 M 95 1.91 # 2 24_2 M 80 1.79 # 3 24_3 M 82 1.93 # 4 24_4 F 53 1.7 Il garde néanmoins les correspondances dans son carnet de laboratoire, au cas où il faudrait faire des vérifications ou revenir à la donnée originale. (biometrie_correspondance &lt;- data_frame( name = biometry_marvel$id, id = biometry_marvel1$id )) # # A tibble: 4 x 2 # name id # &lt;chr&gt; &lt;chr&gt; # 1 Banner Bruce 24_1 # 2 Stark Tonny 24_2 # 3 Fury Nicholas 24_3 # 4 Romanoff Natasha 24_4 A partir des données du tableau général consolidé, personne à part lui ne peut revenir sur ces données d’origine et mettre un nom sur les individus mesurés. Et lui-même n’a pas la possibilité de déterminer qui se cache derrière les autres identifiants tels 3_1, 12_4, 21_2, etc. A vous de jouer Votre objectif est d’acquérir des données pour étudier la prévalence de l’obésité dans la population. En classe, vous allez réfléchir par équipes aux données qu’il vous faudra mesurer : quoi ? pourquoi ? comment ? Les résultats de votre réflexion seront ensuite consolidées pour arriver à un consensus général. Ensuite, le fruit de cette réflexion ainsi que l’analyse que vous réaliserez seront à ajouter dans le projet sdd1_biometry. Une feuille Google Sheets sera mise à disposition pour encoder vos données de manière collaborative sur base des spécifications que vous aurez formulées. La tableau de données que vous devez completer est disponible via le lien suivant : https://docs.google.com/spreadsheets/d/1UfpZvx1_nd7d10vIMAfGVZ1vWyIuzeiKxPL0jfkNSQM/edit?usp=sharing Le dictionnaire des données est disponible via le lien suivant : https://docs.google.com/document/d/1lgYD39W7vmVYyS5ea0wEl9ArE1dhuDRkIIBzZ4K6d_o/edit?usp=sharing Le tableau de données est téléchargeable via le lien suivant : https://docs.google.com/spreadsheets/d/e/2PACX-1vQoVtSWbENWzxbALxD0qyNDqxV4uSYqzLCtJgcNGE7ciT6nkWOjA9b6dMBHaSUY8Nw5f-mSpUEeN-3S/pub?output=csv Attention, veuillez à respectez les conventions que vous aurez édifiées ensemble lors de l’encodage… et n’oubliez pas de préciser également les métadonnées ! En Europe, les données numériques concernant les personnes sont soumises à des règles strictes édictées dans le Règlement Général pour la Protection des Données ou RGPD en abrégé, en vigueur depuis le 25 mai 2018. Vous devez vous assurer de respecter ce règlement lors de la collecte et de l’utilisation de données relatives à des personnes. Pour les autres type de données, le droit d’auteur ou des copyrights peuvent aussi limiter votre champ d’action. Renseignez-vous !↩ "],
["recombinaison-de-tableaux.html", "6.4 Recombinaison de tableaux", " 6.4 Recombinaison de tableaux 6.4.1 Formats long et large gather() versus spread() par gadenbuie. Le format long d’un tableau de données correspond à un encodage en un minimum de colonnes, les données étant réparties sur un plus grand nombre de lignes en comparaison du format large qui regroupe les données dans plusieurs colonnes successives. Voici un exemple fictif d’un jeu de données au fomat long : # # A tibble: 6 x 3 # sex traitment value # &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; # 1 m control 1.2 # 2 f control 3.4 # 3 m test1 4.8 # 4 f test1 3.1 # 5 m test2 0.9 # 6 f test2 1.2 Voici maintenant le même jeu de données présenté dans le format large : # # A tibble: 2 x 4 # sex control test1 test2 # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 m 1.2 4.8 0.9 # 2 f 3.4 3.1 1.2 Dans le format large, les différents niveaux de la variable facteur treatment deviennent autant de colonnes (donc de variables) séparées, et la variable d’origine n’existe plus de manière explicite. Ces deux tableaux contiennent la même information. Bien évidemment, un seul de ces formats est un tableau cas par variables correct. Le format long sera le bon si tous les mesures sont réalisées sur des individus différents. Le format large sera correct, par contre, si les différentes mesures ont été faites à chaque fois sur les mêmes individus (dans le cas présent, un seul mâle et une seule femelle auraient alors été mesurés dans les trois situations). C’est la règle qui veut qu’une ligne corresponde à un et un seul individu dans un tableau cas par variables qui permet de décider si le format long ou le format large est celui qui est correctement encodé. Encoder correctement un tableau de données n’est pas une chose simple. Il peut y avoir plusieurs manières de le représenter. De plus, beaucoup de scientifiques ignorent ou oublient l’importance de bien encoder un tableau sous forme cas par variables. Lorsque vous souhaitez effectuer une représentation graphique, un format peut convenir mieux qu’un autre également, en fonction de ce que vous souhaitez visualiser sur le graphique. Il est donc important de connaitre les fonctions permettant de recombiner simplement un tableau de données d’une forme vers l’autre : gather() et spread(). L’aide-mémoire Data Import est un outil pratique pour vous aider à retrouver les fonctions. Les explications relatives à cette partie s’y trouvent dans la section Reshape Data. L’utilisation des fonction gather() et spread() provenant du package tidyr est également décrite en détails dans R for Data Science. Prenons l’exemple d’un jeu de données provenant de l’article scientifique suivant : Paleomicrobiology to investigate copper resistance in bacteria : isolation and description of Cupriavidus necator B9 in the soil of a medieval foundry. L’article est basé sur l’analyse métagénomique de type “shotgun” pour quatre communautés microbiennes (notées c1, c4, c7et c10, respectivement)27. Il en résulte une longue liste de séquences que l’on peut attribuer à des règnes. shotgun_wide &lt;- tibble( kingdom = c(&quot;Archaea&quot;, &quot;Bacteria&quot;, &quot;Eukaryota&quot;, &quot;Viruses&quot;, &quot;other sequences&quot;, &quot;unassigned&quot;, &quot;unclassified sequences&quot;), c1 = c( 98379, 6665903, 81593, 1245, 757, 1320419, 15508), c4 = c( 217985, 9739134, 101834, 4867, 1406, 2311326, 21572), c7 = c( 143314, 7103244, 71111, 5181, 907, 1600886, 14423), c10 = c(272541, 15966053, 150918, 15303, 2688, 3268646, 35024)) rmarkdown::paged_table(shotgun_wide) Ce tableau est clair et lisible… seulement, est-il correctement encodé en cas par variables d’après vous ? Quelle que soit la réponse à cette question, il est toujours possible de passer de ce format large à un format long dans R de la façon suivante : shotgun_long &lt;- gather(shotgun_wide, c1, c4, c7, c10, key = &quot;batch&quot;, value = &quot;sequences&quot;) rmarkdown::paged_table(shotgun_long) Voici la logique derrière gather(), présentée sous forme d’une animation : gather() par apreshill. Vous conviendrez que le tableau nommé shotgun_long est moins compact et moins aisé à lire comparé à shotgun_wide. C’est une raison qui fait que beaucoup de scientifiques sont tentés d’utiliser le format large alors qu’ici il ne correspond pas à un tableau cas par variables correct, puisqu’il est impossible que les mêmes individus soient présents dans les différents lots (il s’agit de communautés microbiennes indépendantes les unes des autres). De plus, seul le format shotgun_long permet de produire des graphiques pertinents28. chart(data = shotgun_long, sequences ~ batch %fill=% kingdom) + geom_col(position = &quot;fill&quot;) Essayez de réaliser ce type de graphique en partant de shotgun_wide… Bonne chance ! Très souvent, lorsqu’il est impossible de réaliser un graphique avec chart() ou ggplot() parce que les données se présentent mal, c’est parce que le jeu de données est encodé de manière incorrecte ! Si les données sont, par contre, correctement encodées, demandez-vous alors si le graphique que vous voulez faire est pertinent. Pour passer du format long au format large (traitement inverse à gather()), il faut utiliser la fonction spread(). Ainsi pour retrouver le tableau d’origine (ou quelque chose de très semblable) à partir de shotgun_long nous utiliserons : shotgun_wide2 &lt;- spread(shotgun_long, key = batch, value = sequences) rmarkdown::paged_table(shotgun_wide2) La logique de spread() est illustrée via l’animation suivante : spread() par apreshill. Une tâche en binome vous est assignée via l’URL suivante : https://classroom.github.com/g/shtUkGbz Créez un rapport et effectuez les différents exercices en suivant les instructions qui sont dans le fichier README.md de ce dépôt GitHub Classroom. 6.4.2 Recombinaison de variables Parfois, ce sont les variables qui sont encodées de manière inappropriée par rapport aux analyses que vous souhaitez faire. Les fonctions separate() et unite() permettent de séparer une colonne en plusieurs, ou inversément. L’aide-mémoire Data Import vous rappelle ces fonctions dans sa section Split Cells. Elles sont également décrites en détails dans R for Data Science. Partons, par exemple, du jeu de données sur la biométrie des crabes du package MASS : crabs &lt;- read(&quot;crabs&quot;, package = &quot;MASS&quot;, lang = &quot;fr&quot;) rmarkdown::paged_table(crabs) La fonction unite() permet de combiner facilement les colonnes sex et species comme montré dans l’exemple ci-dessous. N’hésitez pas à faire appel à la page d’aide de la fonction via ?unite pour vous guider. crabs &lt;- unite(crabs, col = &quot;sp_sex&quot;, sex, species, sep = &quot;_&quot;) rmarkdown::paged_table(crabs) La fonction complémentaire à unite() est separate(). Elle permet de séparer une variable en deux ou plusieurs colonnes séparées. Donc, pour retrouver un tableau similaire à celui d’origine, nous pourrons faire : crabs &lt;- separate(crabs, col = &quot;sp_sex&quot;, into = c(&quot;sex&quot;, &quot;species&quot;), sep = &quot;_&quot;) rmarkdown::paged_table(crabs) Les analyses métagénomiques coûtent très cher. Il est souvent impossible de faire des réplicats. Un seul échantillon d’ADN a donc été séquencé ici pour chaque communauté.↩ Notez malgré tout que, à condition de bien en comprendre les implications, le format complémentaire peut se justifier dans une publication pour y présenter un tableau le plus lisible possible, ce qui est le cas ici. Mais pour les analyses, c’est le format qui correspond à un tableau cas par variables qui doit être utilisé.↩ "],
["traitements-multi-tableaux.html", "6.5 Traitements multi-tableaux", " 6.5 Traitements multi-tableaux Durant vos analyses, vous serez confronté à devoir gérer plusieurs tableaux que vous allez vouloir rassembler en un seul. Selon le travail à réaliser, il s’agit de coller les tableaux l’un au dessus de l’autre, l’un à côté de l’autre, ou d’effectuer un travail de fusion plus complexe. Nous allons maintenant voir ces différents cas successivement. L’aide-mémoire Data Transformation vous rappelle les différentes fonctions à utiliser dans sa section Combine Tables. Leur utilisation est également décrite en détails dans R for Data Science. 6.5.1 Empilement vers le bas Pour empiler des tableaux l’un au dessus de l’autre, la fonction la plus simple est bind_rows(). Partons de données mesurée dans les mésoscosmes de notre laboratoire lors des travaux pratiques du cours d’océanographie générale. Les différentes variables mesurées sont les suivantes : les données physico-chimiques : la température, le pH, la salinité, l’oxygène dissous à l’aide, respectivement, d’un pHmètre, d’un conductimètre et d’un oxymètre la concentration en nutriments : orthophosphates (PO43-) et nitrates (NO3-) dissous dans l’eau par analyse colorimétrique Pour la première série de mesures, ils ont encodé deux fichiers qu’ils ont du par la suite rassembler. Le groupe 1 a encodé le tableau suivant : physico1 &lt;- as_dataframe(tibble( sample = c(&quot;A0&quot;, &quot;B0&quot;, &quot;A0&quot;, &quot;B0&quot;, &quot;A0&quot;, &quot;B0&quot;, &quot;A0&quot;, &quot;B0&quot;), student = c(&quot;st1&quot;, &quot;st1&quot;, &quot;st2&quot;, &quot;st2&quot;, &quot;st3&quot;, &quot;st3&quot;, &quot;st4&quot;, &quot;st4&quot;), ph = c(7.94, 7.94, 7.94, 7.99, 7.94, 7.99, 7.94, 7.99), salinity = c(34.0, 35.3, 33.9, 35.1, 34.0, 35.2, 33.9, 35.1), oxygen = c(7.98, 8.00, 7.98, 7.98, 7.99, 7.86, 7.89, 7.98), temperature = c(24.6, 24.4, 25.1, 24.7, 24.9, 24.7, 25.0, 24.6) )) rmarkdown::paged_table(physico1) Le groupe 2 a encodé ce tableau : physico2 &lt;- as_dataframe(tibble( sample = c(&quot;A0&quot;, &quot;B0&quot;, &quot;A0&quot;, &quot;B0&quot;), student = c( &quot;st5&quot;, &quot;st5&quot;, &quot;st6&quot;, &quot;st6&quot;), ph = c(7.94, 7.99, 7.93, 7.99), salinity = c(33.8, 35.0, 33.9, 35.1), oxygen = c(7.96, 8.01, 7.90, 8.00), temperature = c(25.0, 24.6, 24.0, 24.0) )) rmarkdown::paged_table(physico2) L’empilement des deux tableaux de données en un seul se fait via la fonction bind_rows() lorsque les tableaux contiennent les mêmes variables présentées exactement dans le même ordre comme ici : physico &lt;- bind_rows(physico1, physico2) rmarkdown::paged_table(physico) 6.5.2 Empilement à droite Pour combiner des tableaux de données par les colonnes, de gauche à droite, la fonction la plus simple à utiliser est bind_cols(). Les étudiants ont également réalisé des prélèvements d’eaux qui ont été dosés par colorimétrie avec un autoanalyseur. Les échantillons des deux groupes ont été analysés dans la même série par l’appareil, ce qui donne le tableau suivant pour les nutriments : nutrients &lt;- as_dataframe(tibble( sample = rep(c(&quot;A0&quot;, &quot;B0&quot;), times = 6), student = rep(c(&quot;st4&quot;, &quot;st6&quot;, &quot;st5&quot;, &quot;st2&quot;, &quot;st1&quot;, &quot;st3&quot;), each = 2), po4 = c(2.445, 0.374, 2.446, 0.394, 2.433, 0.361, 2.441, 0.372, 2.438, 0.388, 2.445, 0.390), no3 = c(1.145, 0.104, 0.447, 0.066, 0.439, 0.093, 0.477, 0.167, 0.443, 0.593, 0.450, 0.125) )) rmarkdown::paged_table(nutrients) Vous devez être très vigilant lors de l’utilisation de bind_cols() car cette dernière combine vos tableaux sans s’assurer que vos lignes soient alignées convenablement ! oceano &lt;- bind_cols(nutrients, physico) rmarkdown::paged_table(oceano) Qu’observez vous ? Effectivement nos deux tableaux de données n’ont pas les lignes dans le même ordre. Il faut être vigilant lors de ce genre de combinaison de tableaux. IL est préférable d’employer des fonctions de fusion de tableaux plus complexes comme full_joint() (ci-dessous). Pour utiliser correctement bind_cols(), il faut vous assurer que les lignes des deux tableaux correspondent exactement, par exemple, en utilisant arrange() : nutrients2 &lt;- arrange(nutrients, student, sample) rmarkdown::paged_table(nutrients2) Le tableau nutrients2 a maintenant les données présentées dans le ma^me ordre (en lignes) que le tableau physico. Nous pouvons donc rassembler ces deux tableaux à l’aide de bind_cols() : oceano &lt;- bind_cols(nutrients2, physico) rmarkdown::paged_table(oceano) Après vérification de l’adéquation des lignes, nous n’aurons plus besoin des colonnes sample1 et student1. La vérification automatique à l’aide de code R et l’élimination de ces variables du tableau oceano vous sont laissées comme exercices… 6.5.3 Fusion de tableaux La fusion fera intervenir une ou plusieurs colonnes communes des deux tableaux pour déterminer quelles lignes du premier correspondent aux lignes du second. Ainsi, la fusion des tableaux est assurée d’être réalisée correctement quel que soit l’ordre des lignes dans les deux tableaux d’origine. Utilisons full_join() en joignant les lignes en fonction des valeurs de student et sample : oceano &lt;- full_join(nutrients, physico, by = c(&quot;student&quot;, &quot;sample&quot;)) rmarkdown::paged_table(oceano) Observez bien ce dernier tableau. L’ordre retenu est celui de nutrients (le premier), mais les données issues de physico ont été retriées avant d’être fusionnées pour que les données correspondent. Comparez au tableau physico d’origine réimprimé ci-dessous : rmarkdown::paged_table(physico) Il existe, en fait, plusieurs versions pour la fusion de tableaux, représentées par une série de fonctions xxx_join(). Lorsque les lignes entre les deux tableaux fusionnés correspondent parfaitement comme dans l’exemple traité ici, les différentes variantes ont le même effet. Mais lorsque des lignes diffèrent, les variantes ont leur importance : full_join() garde toutes les lignes, full_join() par gadenbuie. left_join() ne garde que les lignes uniques du tableau de gauche en plus des lignes communes, left_join() par gadenbuie. right_join() ne garde que les lignes uniques du tableau de droite en plus des lignes communes, right_join() par gadenbuie. inner_join() garde uniquement les lignes communes aux deux tableaux, inner_join() par gadenbuie. A vous de jouer TODO Pour en savoir plus Un aide-mémoire général en science des données avec lien vers les sites webs importants et les autres aide-mémoires. "],
["proba.html", "Module 7 Probabilités &amp; distributions", " Module 7 Probabilités &amp; distributions Objectifs Appréhender le calculs de probabilités Appréhender les principales lois de distributions et leurs utilisations pratiques Prérequis Vous devez être à l’aise avec l’utilisation de R, RStudio et R Markdown. Vous avez appris à maîtriser ces outils dans les modules 1 &amp; 2. "],
["probabilites.html", "7.1 Probabilités", " 7.1 Probabilités La vidéo suivante vous introduit la notion de probabilité et le calcul de probabilités d’une manière plaisante à partir d’un jeu de hasard proposé par un petit chat à ses amis… Sachant qu’un événement en statistique est un fait qui se produit, la probabilité que cet événement se produise effectivement peut être quantifiée sur base de l’observation des réalisations passées. Ainsi si l’événement en question s’est produit, disons, 9 fois sur un total de 12 réalisations, on dira que la probabilité que cet événement se produise est de 9/12, soit 0,75. Notez qu’une probabilité est un nombre compris entre zéro (lorsqu’il ne se produit jamais) et un (lorsqu’il se produit toujours). On écrira, pour la probabilité de l’événement E : \\[0 \\leq \\mathrm{P}(E) \\leq 1\\] 7.1.1 Dépistage Voyons tout de suite une application plus proche de la biologie : le dépistage d’une maladie qui touche 8% de la population. Le test de dépistage mis en place détecte 95% des malades. De plus, le test se trompe dans 10% des cas pour les personnes saines. Comment connaitre le risque d’être malade si on est diagnostiqué positif par ce test ? Pour résoudre ce problème, nous devons d’abord apprendre à combiner des probabilités. Ce n’est pas bien compliqué. Si on a affaire à des événements successifs indépendants (c’est-à-dire que l’occurrence de l’un ne dépend pas de l’occurrence de l’autre), la probabilité que les deux événements successifs indépendants se produisent tous les deux est la multiplication des deux probabiltés. On pourra écrire : \\[\\mathrm{P}(E_1 \\,\\mathrm{et}\\, E_2) = \\mathrm{P}(E_1) * \\mathrm{P}(E_2)\\] Vous pouvez dès lors calculer la probabilité que l’on teste un patient malade (probabilité = 0,08) et que le test soit positif (0,95) dans ce cas : # Personne malade et détectée positive (p_sick_positive &lt;- 0.08 * 0.95) # [1] 0.076 Ceci n’indique pas la probabilité que le test soit positif car il est également parfois (erronément) positif pour des patients sains. Mais au fait, quelle est la probabilité d’avoir un patient sain ? La probabilité que l’un parmi tous les événements possibles se produise vaut toujours un. Les seuls événements possibles ici sont que le patient soit sain ou malade. Donc, \\[\\mathrm{P}(sain) + \\mathrm{P}(malade) = 1 \\rightarrow \\mathrm{P}(sain) = 1 - \\mathrm{P}(malade) = 0.92\\] Nous pouvons maintenant déterminer la probabilité que le test soit positif dans le cas d’une personne saine : # Personne saine et détectée positive (p_healthy_positive &lt;- 0.92 * 0.10) # [1] 0.092 Bon, il nous reste à combiner les probabilités que le test soit positif si la personne est malade et si la personne est saine. Mais comment faire ? Ici, on n’a pas affaire à des événements successifs, mais à des évènements mutuellement exclusifs. On les appellent des événements disjoints. Pour déterminer si l’un parmi deux événements disjoints se produit, il suffit d’additionner leurs probabilités respectives. Nous pouvons maintenant déterminer la probabilité que le test soit positif quelle que soit la personne testée : # La probabilité que le test soit positif (p_positive &lt;- p_sick_positive + p_healthy_positive) # [1] 0.168 Nous nous trouvons ici face à un résultat pour le moins surprenant ! En effet, nous constatons que le test est positif dans 16,8% des cas, mais seulement 7,6% du temps, il sera correct (probabilité d’une personne malade détectée). Parmi tous les cas positifs au test, il y en a… p_sick_positive / p_positive # [1] 0.452381 … seulement 45,2% qui sont effectivement malades (on parle de vrais positifs) ! Ceci ne correspond pas du tout aux indications de départ sur les performances du test. Dans le cas de deux faits successifs qui ne peuvent chacun que résulter en deux événements, nous avons seulement quatre situations possibles. Si l’un des cas est qualifié de positif et l’autre de négatif, nous aurons : les vrais positifs (test positif alors que la personne est malade), ici 0.08 * 0.95 les faux positifs (test positif alors que la personne est saine), ici 0.92 * 0.10 les vrais négatifs (test négatif alors que la personne est saine), ici 0.92 * 0.90 les faux négatifs (test négatif alors que la personne est malade), ici 0.08 * 0.05 En fait, les performances finales du test de dépistage dépendent aussi de la prévalence de la maladie. Ainsi pour une maladie très commune qui affecterait 80% de la population, nous obtenons : # Faux positifs 0.20 * 0.10 # [1] 0.02 # Vrais positifs 0.80 * 0.95 # [1] 0.76 # Total des positifs 0.20 * 0.10 + 0.80 * 0.95 # [1] 0.78 # Fractions de tests positifs qui sont corrects (0.80 * 0.95) / (0.20 * 0.10 + 0.80 * 0.95) # [1] 0.974359 Ouf ! Dans ce cas-ci le test positif est correct dans 97,4% des cas. Mais qu’en serait-il si la maladie est très rare (probabilité de 0,008) ? # Faux positifs 0.992 * 0.10 # [1] 0.0992 # Vrais positifs 0.008 * 0.95 # [1] 0.0076 # Total des positifs 0.992 * 0.10 + 0.008 * 0.95 # [1] 0.1068 # Fractions de tests positifs qui sont corrects (0.008 * 0.95) / (0.992 * 0.10 + 0.008 * 0.95) # [1] 0.07116105 Dans ce cas, un test positif n’aura effectivement détecté un malade que dans … 7,1% des cas ! Les 92,9% autres cas positifs seront en fait des personnes saines. Comme nous pouvons le constater ici, le calcul des probabilités est relativement simple. Mais en même temps, les résultats obtenus peuvent être complètement contre-intuitifs. D’où l’intérêt de faire ce genre de calcul justement. 7.1.2 Arbre des probabilités Il se peut que tout cela vous paraisse très (trop) abstrait. Vous êtes peut-être quelqu’un de visuel qui comprend mieux les concepts en image. Dans ce cas, la méthode alternative de résolution des calculs de probabilités via les arbres de probabilités devrait vous éclairer. Le principe consiste à représenter un arbre constitué de nœuds (des faits qui se produisent). De ces nœuds, vous représentez autant de branches (des segments de droites) que d’événements possibles. La figure suivante est l’arbre des probabilités correspondant au cas du dépistage de la maladie qui touche 8% de la population. Arbre de probabilités permettant de déterminer la probabilité d’avoir un résultat positif au test. Du premier nœud (le fait qu’une personne est atteinte ou non de la maladie), nous avons deux branches menant aux deux événements “malade” et “sain”. Chacune de ces deux situations est un nouveau nœud d’où deux événements sont possibles à chaque fois (2 fois 2 nouvelles branches) : un test “positif”, ou un test “négatif”. Les nœuds terminaux (les “négatifs” et “positifs” ici) sont aussi appelés les feuilles de l’arbre. L’arbre reprend donc tous les cas possibles depuis le nœud de départ (sa racine), jusqu’aux feuilles. L’étape suivante consiste à aller indiquer le long des branches les probabilités associées à chaque événement : 0.08 pour “malade”, 0.95 pour un dépistage “positif” si la personne est malade, etc. A ce stade, une petite vérification peut être faite. La somme des probabilités aux feuilles doit toujours valoir un, et il en est de même de la somme de toutes les branches issues d’un même nœud. Le calcul se fait ensuite comme suit. On repère tous les cas qui nous intéressent. Ici, il s’agit de toutes les trajectoires qui mènent à un test “positif”. Le calcul des probabilités se fait en multipliant les probabilités lorsqu’on passe d’un noeud à l’autre et en additionnant les probabilités ainsi calculées le long des feuilles terminales de l’arbre considéré. Donc, le chemin “malade” -&gt; “positif” correspond à 0.08 * 0.95 = 0.076. Le chemin “sain” -&gt; “positif” correspond à 0.92 * 0.10 = 0.092. Enfin, nous sommons les probabilités ainsi calculées pour toutes les feuilles de l’arbre qui nous intéressent. Ici, ce sont toutes les feuilles qui correspondent à un test “positif”, soit 0.076 + 0.092 = 0.168. Et voilà ! Nous avons répondu au problème : la probabilité d’avoir un résultat positif avec le test de dépistage dans un population dont 8% est atteint de la maladie est de 16.8%. 7.1.3 Théorème de Bayes Nous devons introduire ici le concept de probabilité conditionnelle. Une probabilité conditionnelle est la probabilité qu’un événement E2 se produise si et seulement si un premier événement E1 s’est produit (E1 et E2 sont deux événements successifs). La probabilité conditionnelle s’écrit \\(\\mathrm{P}(E2|E1)\\). Vous noterez que l’arbre des probabilités représente, en réalité, des probabilités conditionnelles à partir du second niveau (l’arbre peut être évidemment plus complexe). Arbre de probabilités avec probabilités conditionnelles en rouge. On pourra considérer, donc, la probabilité conditionnelle d’avoir un résultat positif au test, si la personne est malade \\(\\mathrm{P}(positif|malade)\\). Vous l’avez indiquée plus haut dans l’arbre des probabilités, c’est 0.95. Maintenant, la probabilité qu’une personne soit malade si elle est positive au test \\(\\mathrm{P}(malade|positif)\\) est une information capitale ici. Le lien entre les deux n’est pas facile à faire. C’est grâce aux travaux du révérend Thomas Bayes au 18ème siècle que ce problème a été résolu. Les implications du théorème de Bayes sont énormes car cela permet de déterminer des probabilités dites a posteriori en fonction de connaissances a priori. Si nous réanalysons le raisonnement qui est fait dans l’arbre de probabilités, on peut remarquer que le premier calcul (“malade” -&gt; “positif”) correspond en fait à la probabilité que le test soit positif si le patient est malade \\(\\mathrm{P}(positif|malade)\\) multipliée par la probabilité que le patient soit malade \\(\\mathrm{P}(malade)\\), et ceci est aussi égal à \\(\\mathrm{P}(positif\\, et\\, malade)\\). Donc, \\[\\mathrm{P}(positif|malade) * \\mathrm{P}(malade) = \\mathrm{P}(positif\\, et\\, malade)\\] Par un raisonnement symétrique, on peut aussi dire que : \\[\\mathrm{P}(malade|positif) * \\mathrm{P}(positif) = \\mathrm{P}(positif\\, et\\, malade)\\] Donc, nous avons aussi : \\[\\mathrm{P}(malade|positif) * \\mathrm{P}(positif) = \\mathrm{P}(positif|malade) * \\mathrm{P}(malade)\\] … et en divisant les deux termes par \\(\\mathrm{P}(positif)\\), on obtient : \\[\\mathrm{P}(malade|positif) = \\frac{\\mathrm{P}(positif|malade) * \\mathrm{P}(malade)}{\\mathrm{P}(positif)}\\] De manière générale, le théorème de Bayes s’écrit : \\[\\mathrm{P}(A|B) = \\frac{\\mathrm{P}(B|A) * \\mathrm{P}(A)}{\\mathrm{P}(B)}\\] Nous avons maintenant une façon simple de déterminer \\(\\mathrm{P}(malade|positif)\\) à partir de \\(\\mathrm{P}(positif|malade)\\), \\(\\mathrm{P}(malade)\\), et \\(\\mathrm{P}(positif)\\), c’est-à-dire des probabilités auxquelles nous avons facilement accès expérimentalement en pratique. Calculez comme exercice la probabilité qu’un patient soit malade s’il est positif au test via le théorème de Bayes, et comparez le résultat de votre calcul à ce que nous avions obtenu plus haut (45.2%). A retenir Probabilité d’un événement : \\[\\mathrm{P}(E) = \\frac{\\mathrm{nbr\\ occurences\\ } E}{\\mathrm{nbr\\ total\\ essais}}\\] Probabilité de deux événements successifs (cas général) : \\[\\mathrm{P(A\\, \\mathrm{et}\\, B)} = \\mathrm{P}(B|A) * \\mathrm{P(A)}\\] Probabilité qu’un parmi deux événements se produise (cas général) : \\[\\mathrm{P(A\\, \\mathrm{ou}\\, B)} = \\mathrm{P}(A) + \\mathrm{P(B)} - \\mathrm{P}(A\\, \\mathrm{et}\\, B)\\] Application vraiment utile du théorème de Bayes par xkcd. 7.1.4 Probabilités et contingence Comme un tableau de contingence indique le nombre de fois que des événements ont pu être observés, il peut servir de base à des calculs de probabilités. Partons du dénombrement de fumeur en fonction du revenu dans une population. tabac &lt;- data.frame( revenu_faible = c(634,1846,2480), revenu_moyen = c(332, 1622,1954), revenu_eleve = c(247,1868,2115), total = c(1213, 5336, 6549)) rownames(tabac) &lt;- c(&quot;fume&quot;, &quot;ne fume pas&quot;, &quot;total&quot;) knitr::kable(tabac) revenu_faible revenu_moyen revenu_eleve total fume 634 332 247 1213 ne fume pas 1846 1622 1868 5336 total 2480 1954 2115 6549 Quelle est la probabilité d’être un fumeur \\(\\mathrm{P}(fumeur)\\) ? Rappelons-nous de la définition de probabilité : nombre de cas où l’événement se produit sur le nombre total de cas. Ici, on a 1213 fumeurs dans un effectif total de l’échantillon de 6549 personnes, soit : 1213 / 6549 # [1] 0.1852191 Quelle est la probabilité d’être fumeur si le revenu élevé \\(\\mathrm{P}(fumeur|revenu\\_eleve)\\) ? Le nombre de fumeurs à revenus élevés se monte à 247. Attention, ici l’échantillon de référence n’est plus la population totale, mais seulement ceux qui ont des revenus élevés, donc 2115 personnes : 247 / 2115 # [1] 0.1167849 Quelle est la probabilités d’avoir un revenu faible ou d’avoir un élevé ? Cette question peut s’écrire : \\(\\mathrm{P}(revenu\\-faible\\, ou\\, revenu\\_eleve)\\). 2480 / 6549 + 2115 / 6549 # [1] 0.7016338 Il s’agit d’une somme de probabilités disjointes. Quelle est la probabilités d’être fumeur ou d’avoir un revenu moyen ? Cette question peut s’écrire : \\(\\mathrm{P}(fumeur\\, ou\\, revenu\\_moyen)\\). 1213 / 6549 + 1954 / 6549 - 332 / 6549 # [1] 0.4328905 Il s’agit d’une somme de probabilités non disjointes29. 7.1.4.1 Populations de taille infinie Dans une population, voici les proportions de différents groupes sanguins : 44% O, 42% A, 10% B, 4% AB Quelles est la probabilité d’obtenir 1 individu du groupe B ? Cette question peut s’écrire : \\(\\mathrm{P}(B)\\). 0.10 # [1] 0.1 Quelle est la probabilité d’obtenir 3 individus du groupe B d’affilée ? Cette question peut s’écrire : \\(\\mathrm{P}(B\\, et\\, B\\, et\\, B)\\). 0.10 * 0.10 * 0.10 # [1] 0.001 Nous avons ici 3 événements successifs indépendants. Donc, on multiplie leurs probabilités respectives. 7.1.4.2 Populations de taille finie Dans une population de 100 personnes dont les proportions des différentes groupes sanguins sont identiques au cas précédent. Quelles est la probabilité d’obtenir un échantillon de trois individus du groupe B issus de cette population30 ? Cette question peut s’écrire : \\(\\mathrm{P}(B\\, et\\, B\\, et\\, B)\\). 10 / 100 * 9 / 99 * 8 / 98 # [1] 0.000742115 Il s’agit d’événements successifs non-indépendants. En effet, le retrait d’un individu de la population de taille finie modifie les proportions relatives des groupes sanguins dans le reste de la population, et donc, les probabilités aux tirages suivants. Ainsi pour le groupe B, nous n’avons plus que 9 individus de ce groupe dans une population de 99 individus après le premier tirage d’un individu du groupe B ! Autrement dit, \\(\\mathrm{P}(B|B) \\neq \\mathrm{P}(B|not\\ B)\\). On a donc, \\(\\mathrm{P}(B|B) = 9/99\\) et ensuite \\(\\mathrm{P}(B|B\\, \\mathrm{et}\\, B) = 8/98\\). A vous de jouer Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;07a_proba&quot;) Si E1 et E2 sont deux événements non disjoints, la probabilité que l’un de ces deux événements se produise est : \\(\\mathrm{P}(E1\\, ou\\, E2) = \\mathrm{P}(E1) + \\mathrm{P}(E2) - \\mathrm{P}(E1\\, et\\, E2)\\).↩ En statistique, on appelle cela un tirage au sort sans remise. Le résultat est très différent si le premier individu tiré au hasard était remis dans la population et pouvait être éventuellement pris à nouveau au second ou troisième tirage (tirage au sort avec remise). Notez aussi que, pour une population de taille infinie ou quasi-infinie, les deux types de tirage au sort sont équivalents à celui avec remise car enlever un individu d’une population infinie ne change pas fondamentalement son effectif, donc les probabilités ultérieures.↩ "],
["lois-de-distributions.html", "7.2 Lois de distributions", " 7.2 Lois de distributions Étant donné que les sciences des données reposent sur un nombre (si possible important) de répétitions d’une mesure -des réplicats-, il est possible de déterminer à quelle fréquence un événement E se produit de manière expérimentale. La probabilité observée est quantifiable sur base d’un échantillon comme nous venons de le voir dans la section précédente. La probabilité théorique est connue si le mécanisme sous-jacent est parfaitement connu. Donc, en situation réelle, seule la probabilité observée est accessible, et ce n’est qu’une approximation de la vraie valeur, ou valeur théorique. Cependant, dans des situations particulières les statisticiens ont calculé les probabilités théoriques. Ce sont des lois de distribution. Elles associent une probabilité théorique à chaque événement possible. La comparaison des probabilités théoriques et observées constitue l’un des piliers des statistiques. Le raisonnement est le suivant : si les probabilités observées sont suffisamment proches des probabilités théoriques, alors, nous pouvons considérer que les événements sont générés selon un mécanisme identique ou proche de celui qui est à la base de la loi de distribution théorique correspondante. Même dans la vie de tous les jours, les calculs de probabilités peuvent être utiles, enfin… d’après xkcd. Avant d’explorer ces lois de distributions statistiques, nous devons d’abord introduire la distinction entre probabilité discrète et probabilité continue. Une probabilité discrète est associée à une variable qualitative ou à la rigueur, à une variable continue discrète qui peut prendre un nombre fini -et généralement relativement petit- de valeurs. A chaque valeur est associé un événement, et chaque événement a une certaine probabilité de se produire dans un contexte donné. Jusqu’à présent, nous n’avons traité que ce cas-là. Par contre, une variable quantitative continue peut prendre un nombre infini de valeurs matérialisées généralement par l’ensemble des nombres réels. Dans ce cas, l’association d’un événement à une valeur de la variable, et d’une probabilité à chaque événement reste vraie en théorie. Mais en pratique, ces probabilités dites continues ne sont pas calculables par les équations étudiées jusqu’ici. Par contre, les lois de distributions continues permettent des calculs, moyennant une petite astuce que nous étudierons plus loin dans ce chapitre. A vous de jouer Tout au long de cette section des questions sous la forme d’un learnr vous sont proposées. Complétez progressivement le learnr avec vos nouvelles connaissances Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(“07b_distri”) "],
["distribution-uniforme.html", "7.3 Distribution uniforme", " 7.3 Distribution uniforme La loi de la distribution uniforme se rapporte à un mécanisme qui génère tous les événements de manière équiprobable. 7.3.1 Distribution discrète Dans le cas d’événements discrets, si \\(n_E\\) est le nombre total d’événements possibles, la probabilité d’un de ces événements vaut donc : \\[\\mathrm{P}(E) = \\frac{1}{n_E}\\] La distribution uniforme est d’application pour les jeux de hasard (dés, boules de loto, …). En biologie, elle est plus rare. Dans le cas d’un sexe ratio de 1:1 (autant de mâles que de femelles), la probabilité qu’un nouveau né soit un mâle ou une femelle suit une distribution uniforme et vaut 1/2. La distribution spatiale des individus dans une population biologique peut être uniforme lorsque les individus interagissent de telle manière que la distance entre eux soit identique (par exemple, dans un groupe de manchots Aptenodytes patagonicus sur la banquise). Imaginons un animal hypothétique pour lequel la portée peut être de 1 à 4 petits de manière équiprobable. Nous avons alors 1/4 des portées qui présentent respectivement, 1, 2, 3 ou 4 petits (Fig. 7.1). Figure 7.1: Probabilité du nombre de petits dans une portée qui suivrait un distribution strictement uniforme entre 1 et 4. 7.3.2 Distribution continue D’emblée, nous pouvons facilement démontrer quel est le problème avec les probabilités dans le cas de la distribution uniforme continue. Nous avons en effet, un nombre infini d’événements équiprobables possibles. Donc, la probabilité de chaque événement est (\\(n_E = \\infty\\)) : \\[\\mathrm{P}(E) = \\frac{1}{\\infty} = 0\\] … et ce calcul est correct ! Dans le cas de probabilités continues, la probabilité d’un événement en particulier est toujours nulle. Nous pouvons seulement calculer que l’un parmi plusieurs événements se produise (compris dans un intervalle). La représentation graphique d’une loi de distribution continue est un outil utile pour la comprendre et vérifier ses calculs. La forme la plus courante consiste à montrer la courbe de densité de probabilité pour une distribution continue. Sur l’axe X, nous avons les quantiles (les valeurs observables), et sur l’axe Y, la densité de probabilité31. Par exemple, si nous constatons qu’un insecte butineur arrive sur une fleur en moyenne toutes les 4 minutes, la probabilité qu’un butineur arrive dans un intervalle de temps compris entre 0 et 4 min depuis le moment initial \\(t_0\\) de nos observations suit une distribution uniforme continue (Fig. 7.2). Figure 7.2: Probabilité qu’un nouvel insecte butineur arrive dans un intervalle de 0 à 4 min si, en moyenne, un insecte arrive toutes les 4 min. Une autre représentation courante est la densité de probabilité cumulée qui représente la probabilité d’observer un quantile ou moins. Dans le cas présent, cela représente la probabilité qu’au moins un insecte butineur soit observé pour des durées d’observation croissantes (Fig. 7.3). Figure 7.3: Probabilité cumulée qu’un nouvel insecte butineur arrive dans un intervalle de 0 à 4 min si, en moyenne, un insecte arrive toutes les 4 min. Notation : nous noterons qu’une variable suit une loi de distribution comme ceci (le tilde ~ se lit “suit une distribution”, et U représente la distribution uniforme avec entre parenthèse, les paramètres de la distribution, ici, les bornes inférieure et supérieure) : \\[X \\sim U(0, 4)\\] Cela signifie : “la variable aléatoire X suit une distribution uniforme 0 à 4”. La distribution \\(U(0, 1)\\) est particulière et est appelée distribution uniforme standard. Elle a la propriété particulière que si \\(X \\sim U(0, 1)\\) alors \\((1-X) \\sim U(0, 1)\\). 7.3.3 Quantiles vers probabilités L’aire sous la courbe représente une probabilité associée à l’intervalle considéré pour les quantiles. Répondez aux questions suivantes pour notre variable \\(X \\sim U(0,4)\\) : Quelle est la probabilité que X = 1 (un insecte butineur arrive après exactement 1,00000… min d’observation) ? Quelle est la probabilité que X soit compris entre 1 et 1,5 ? La réponse à la question (1) est immédiate. Cette probabilité est nulle (voir plus haut)32 ! Pour la question (2), nous pouvons répondre en calculant l’aire sous la courbe entre les quantiles 1 et 1,5 (représentée par l’aire en rouge à la Fig. 7.4). Figure 7.4: Probabilité qu’un insecte butineur arrive entre 1 et 1,5 min après le début d’une observation (aire P en rouge). Ici, le calcul est assez simple à faire à la main. Mais nous verrons d’autres lois de distribution plus complexes. Dans tous les cas, R offre des fonctions qui calculent les aires à gauche ou à droite d’un quantile donné. Le nom de la fonction est toujours p&lt;distri&gt;(), avec pour la distribution uniforme punif(). L’aire à gauche du quantile nécessite de spécifier l’argument lower.tail = TRUE (“queue en bas de la distribution” en anglais). Pour l’aire à droite, on indiquera évidemment lower.tail = FALSE. Donc, pour calculer la probabilité qu’un insecte arrive en moins de 1,5 sec, nous écrirons : punif(1.5, min = 0, max = 4, lower.tail = TRUE) # [1] 0.375 Mais comme nous voulons déterminer la probabilité qu’un insecte arrive entre 1 et 1,5 sec, nous devons soustraire à cette valeur la probabilité qu’un insecte arrive en moins de 1 sec (la zone hachurée en rouge dans la Fig 7.4 est en effet l’aire à gauche depuis le quantile 1,5 moins l’aire à gauche depuis le quantile 1) : punif(1.5, min = 0, max = 4, lower.tail = TRUE) - punif(1.0, min = 0, max = 4, lower.tail = TRUE) # [1] 0.125 La réponse est 0,125, soit une fois sur huit. Le calcul de probabilités sur base de lois de distributions continues se fait via les aires à gauche ou à droite d’un quantile sur le graphique de densité de probabilité. Pour une aire centrale, nous soustrayons les aires à gauche des deux quantiles respectifs. 7.3.4 Probabilités vers quantiles Le calcul inverse est parfois nécessaire. Par exemple pour répondre à la question suivante : Combien de temps devons-nous patienter pour observer l’arrivée d’un insecte butineur sur la fleur une fois sur trois observations en moyenne ? Ici, nous partons d’une probabilité (1/3) et voulons déterminer le quantile qui définit une aire à gauche de 1/3 sur le graphique (Fig. 7.5). Figure 7.5: Temps d’observation nécessaire (quantile Q) pour voir arriver un butineur une fois sur trois (aire P en rouge de 1/3 à gauche de Q). Dans R, la fonction qui effectue ce calcul est q&lt;distri&gt;(). Donc ici, il s’agit de qunif(). Les arguments sont les mêmes que pour punif() sauf le premier qui est une ou plusieurs probabilités. Nous répondons à la question de la façon suivante : qunif(1/3, min = 0, max = 4, lower.tail = TRUE) # [1] 1.333333 Donc il faut observer pendant 1,33 min (1 min et 20 sec) pour avoir 1 chance sur 3 d’observer l’arrivée d’un insecte butineur. 7.3.5 Calcul avec les snippets La SciViews Box propose différents snippets pour nous aider à effectuer nos différents calculs et graphiques relatifs à la distribution uniforme continue. Ils se retrouvent dans le menu (d)istributions accédé depuis .... Donc ..i donne directement accès à ce menu, et puis (d)istributions: uniform accédé depuis .iu directement. Ensuite, il suffit de choisir le snippet dans le menu déroulant (voir ci-dessous). .iuproba : calcul de probabilités depuis des quantiles .iuquant : calcul de quantile depuis des probabilités .iurandom : génération de nombres pseudo-aléatoires .iudens : graphique de la densité de probabilité .iucumul : graphique de la densité de probabilité cumulée .iullabel : ajout d’un label sur le graphique à gauche .iurlabel: ajout d’un label sur le graphique à droite Le snippet .iurandom nécessite quelques explications supplémentaires. R est capable de simuler la génération de nombres aléatoires selon différentes lois de distribution (r&lt;distri&gt;()). runif() est la fonction qui le fait pour une distribution uniforme continue. Comme il ne s’agit pas réellement de nombres aléatoires, on parle de générateur de nombres pseudo-aléatoires. En fait, il s’agit d’une série de nombres qui a les mêmes propriétés que des nombres réellement aléatoires. R se positionne au hasard dans cette série. Donc, à chaque fois que vous appelez la fonction runif(), vous obtenez logiquement des valeurs différentes. A des fins de reproductibilité, il est possible de forcer R à partir en un point précis de la série avec la fonction set.seed() avec un nombre comme argument qui donne la position. Par exemple set.seed(281)33. La génération de nombres aléatoires dans les instructions qui suivent seront alors toujours les mêmes. Voici un exemple de 10 nombres aléatoires générés depuis une distribution uniforme standard (compris entre 0 et 1). Chaque fois que vous exécuterez ces deux instructions exactement l’une après l’autre, vous obtiendrez toujours la même suite. Si vous ré-exécutez la seconde instruction sans la première, vous obtiendrez une suite différente. set.seed(946) runif(10, min = 0, max = 1) # [1] 0.6378020 0.7524999 0.5593599 0.6688387 0.8989262 0.5300384 0.1520689 # [8] 0.9031163 0.2693327 0.6738862 Plus la densité de probabilité est élevée, plus les événements dans cette région du graphique sont probables.↩ La question n’est pas après environ une minute, mais à exactement 1 min 0 sec, 0 millisec, …, ce qui est alors hautement improbable.↩ Si vous utilisez set.seed() prenez soin de spécifier toujours une valeur différente prise au hasard comme argument !↩ "],
["distribution-binomiale.html", "7.4 Distribution binomiale", " 7.4 Distribution binomiale Partons d’un exemple pratique pour découvrir cette distribution. La mucoviscidose est, dans la population européenne, la plus fréquente des maladies génétiques héréditaires. Elle se caractérise par un mucus (voies respiratoires) anormalement épais qui est à l’origine de diverses complications. L’altération d’une protéine CFTR est à l’origine de cette maladie. Comme le gène qui code pour cette protéine est récessif, il faut que le deux allèles soient porteurs simultanément de la mutation pour que la maladie apparaisse. Parmi des familles de six enfants dont le père et la mère normaux sont tous deux porteurs hétérozygotes du gène altéré, quelle est la probabilité d’obtenir 0, 1, 2, …, 6 enfants atteints de mucoviscidose ? 7.4.1 Epreuve de Benouilli La distribution binomiale est un loi de distribution discrète qui répond à ce genre de question. Ses conditions d’applications sont : résultats binaire (deux événements possibles uniquement ; l’un sera nommé “succès” et l’autre “échec” par convention), essais indépendants (les probabilités ne changement pas d’un essai à l’autre), n le nombre d’essais totaux est fixé à l’avance, probabilité du “succès” p constante (probabilité de l’“échec” = 1 - p). Les conditions particulières de cette situation sont appelées épreuve de Bernouilli. Mathématiquement, nous l’écrirons comme suit. Soit une variable aléatoire \\(Y\\) qui comptabilise le nombre de succès, la probabilité d’obtenir \\(j\\) succès parmi \\(n\\) essais est : \\[P(Y=j)= C^j_n \\times p^j \\times (1-p)^{n-j}\\] Le coefficient binomial \\(C^j_n\\) vaut34 : \\[C^j_n = \\frac{n!}{j!(n-j)!}\\] \\(C^j_n\\) représente le nombre de combinaisons possibles pour obtenir \\(j\\) succès parmi \\(n\\) essais réalisés dans un ordre quelconque. On pourra écrire aussi : \\[Y \\sim B(n,p)\\] Notre exemple rentre parfaitement dans le cadre de l’épreuve de Bernouilli avec n = 6 et p, la probabilité du succès, c’est-à-dire, d’avoir un enfant qui ne développe pas la maladie de 3/4 : \\(Y \\sim B(6, 0.75)\\). 7.4.2 Calculs et graphiques Les calculs sur base d’une distribution binomiale sont assez similaires à ceux de la distribution uniforme dans R, en remplaçant unif par binom dans le nom des fonction. Voici la liste des snippets à votre disposition dans la SciViews Box pour vous aider (menu (d)istributions: binomial à partir de .ib) : Puisqu’il s’agit d’une distribution discrète, un petit nombre d’événements possibles existent. Le snippet .ibtable retourne l’ensemble des valeurs possibles pour \\(j\\) allant de 1 à \\(n\\) en une seule étape. Les autres snippets devraient vous être familiers. (.table &lt;- data.frame(success = 0:6, probability = dbinom(0:6, size = 6, prob = 0.75))) # success probability # 1 0 0.0002441406 # 2 1 0.0043945312 # 3 2 0.0329589844 # 4 3 0.1318359375 # 5 4 0.2966308594 # 6 5 0.3559570312 # 7 6 0.1779785156 La représentation graphique donne la Fig. 7.6. Figure 7.6: Probabilité d’avoir j enfants sains parmi 6 dans des familles dont les deux parents sont porteurs hétérozygotes du gène de la mucoviscidose. La situation la plus probable est donc d’avoir 5 enfants sains sur 6. Nous pouvons aussi observer que, lorsque \\(p\\) s’éloigne de 0,5, les probabilités à l’extrême opposée tendent assez rapidement vers zéro (ici, la probabilité de n’avoir qu’un seul, ou aucun enfant sain). La distribution binomiale trouve de très nombreuses applications en biologie, en écologie, en génétique et dans d’autres disciplines. Elle permet même de représenter vos chances de réussite à l’examen de science des données biologiques ! Voici, pour finir, l’allure d’une distribution binomiale pour laquelle la probabilité du succès est égale à la probabilité d’échec (0,5). Cette distribution est symétrique. Figure 7.7: Probabilité d’avoir des garçons parmi une fratrie de 6 enfants (si le sexe ratio de 1:1). Le factoriel d’un nombre \\(n\\), noté \\(n!\\) est \\(1 \\times 2 \\times 3 \\times ... \\times n\\), avec \\(0! = 1\\).↩ "],
["distribution-de-poisson.html", "7.5 Distribution de poisson", " 7.5 Distribution de poisson Maintenant, nous pouvons poser la question autrement. Prenons un couple sain au hasard en Belgique, quelle est la probabilité que ce couple transmette la mucoviscidose à leur descendance ? Ne considérons pas ici les personnes elles-même atteintes de la maladie qui prendront certainement des précautions particulières. Sachant qu’une personne sur 20 est porteuse du gène défectueux (hétérozygote) dans la population saine belge, la probabilité de former un couple doublement hétérozygote qui pourrait transmettre la maladie est de35 : 1/20 * 1/20 # [1] 0.0025 … soit un couple sur 400. Donc, globalement, les probabilités d’avoir des enfants sains est beaucoup plus grande que 0.75 si nous incluons tous les couples belges de parents sains porteurs ou non. Cette probabilité est de36 : (399 * 1 + 1 * 0.75) / 400 # [1] 0.999375 Pour un couple au hasard sans connaissance a priori du fait que les parents soient porteurs ou non, la probabilité d’avoir un enfant atteint de la mucoviscidose est heureusement très, très faible, mais non nulle (de l’ordre de 1/1600 = 0.999375). Si nous considérons maintenant une population suffisamment grande pour pouvoir espérer y trouver “statistiquement” une personne atteinte de mucoviscidose, nous pourrions décider d’étudier un échantillon aléatoire de 1600 enfants belges. La distribution binomiale requiert alors le calcul de \\(C^j_n\\) sur base de \\(n = 1600\\), ce qui revient à devoir calculer le factoriel de 1600 : factorial(1) # [1] 1 factorial(10) # [1] 3628800 factorial(100) # [1] 9.332622e+157 factorial(1600) # Warning in factorial(1600): valeur d&#39;argument hors intervalle dans # &#39;gammafn&#39; # [1] Inf Or, le factoriel est un nombre qui grandit très, très vite. Déjà le factoriel de 100 est un nombre à 157 chiffres. Nous voyons que R est incapable de calculer précisément le factoriel de 1000. Ce nombre est supérieur au plus grand nombre que l’ordinateur peut représenter pour un double en R (1.797693110^{308}). Donc, nous sommes incapables de répondre à la question à l’aide de la loi binomiale. 7.5.1 Evénements rares La distribution de Poisson permet d’obtenir la réponse à la question posée parce qu’elle effectue le calcul différemment. Cette distribution discrète a un seul paramètre \\(\\lambda\\) qui représente le nombre moyen de cas rares que l’on observe dans un échantillon donné, ou sur un laps de temps fixé à l’avance. Cette distribution est asymétrique pour de faible \\(\\lambda\\). Les conditions d’application sont : résultats binaire, essais indépendants (les probabilités ne changement pas d’un essai à l’autre), taille d’échantillon ou laps de temps que le phénomène est observé fixe, probabilité d’observation de l’évènement \\(\\lambda\\) faible. Pour une variable \\(Y \\sim P(\\lambda)\\), nous pouvons calculer la probabilité que \\(Y = 0\\), \\(Y = 1\\), … de la façon suivante : \\[P(Y=0) = e^{-\\lambda}\\] et \\[P(Y=k) = P(Y=k-1) \\times \\frac{\\lambda}{k}\\] Le calcul se réalise de proche en proche en partant de la probabilité de ne jamais observer l’événement. Comme l’événement est rare, la probabilité tend très rapidement vers une valeur extrêmement faible. Seul le calcul des quelques premiers termes est donc nécessaire. A titre d’exercice, faites le calcul pour notre exemple d’un échantillon de la population belge, avec \\(\\lambda = 1\\) comme paramètre. La densité de probabilité pour cette distribution est représentée à la Fig. 7.8. Figure 7.8: Probabilité d’occurence de mucoviscidose dans un échantillon aléatoire de 1600 belges. 7.5.2 Loi de Poisson dans R Les fonctions dans R relatives à la distribution de Poisson portent des noms &lt;x&gt;pois(), tel que ppois() pour calculer des probabilités, qpois() pour calculer des quantiles ou rpois() pour générer des nombres pseudo-aléatoires selon cette distribution. Voici la liste des snippets à votre disposition dans la SciViews Box pour vous aider (menu (d)istributions: poisson à partir de .ip) : Comme il y a la même proportion d’hommes et de femmes porteurs, nous avons 1/20 des hommes et 1/20 des femmes qui sont porteurs. Nous formons des couples au hasard en piochant un homme dans la population masculine et une femme dans la population féminine de manière indépendante. Dans ce cas, nous obtenons un couple double porteur à une fréquence de 1/20 * 1/20 = 1/400.↩ Sur 400 couples, 399 ont une probabilité d’engendrer des enfants sains (porteurs hétérozygotes ou non porteurs confondus) de 100%. A cela il faut ajouter un couple sur 400 qui aura une probabilité de 75% de faire des enfants non atteints car non homozygotes.↩ "],
["distribution-normale.html", "7.6 Distribution normale", " 7.6 Distribution normale La vidéo suivante vous permettra de récapituler certaines notions étudiées jusqu’ici concernant les types de variables et vous introduira la loi de distribution normale ou distribution de Gauss ou encore, gaussienne. 7.6.1 Une “courbe en cloche” La distribution normale est la distribution la plus utilisée en statistique. Elle se rencontre très souvent en biologie comme dans bien d’autres domaines, à chaque fois qu’une variable continue définie sur tout le domaine des réels est issue d’un nombre important de composantes indépendantes dont les effets sont additifs. La forme de sa densité de probabilité est caractéristique et dite “en cloche” (Fig. 7.9). Figure 7.9: Un exemple de distribution normale. Il s’agit d’une densité de probabilité symétrique et asymptotique à ses deux extrémités en + et -infini. La distribution normale a deux paramètres : la moyenne \\(\\mu\\) et l’écart type \\(\\sigma\\). Sa densité de probabilité est représentée par l’équation suivante : \\[\\Phi(Y) = \\frac{1}{ \\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2} \\left( \\frac{Y - \\mu}{\\sigma} \\right)^2}\\] Pour une variable aléatoire \\(Y\\) qui suit une distribution normale avec une moyenne \\(\\mu\\) et un écart type \\(\\sigma\\), nous écrirons : \\[Y \\sim N(μ, σ)\\] 7.6.2 Loi normale réduite Parmi toutes les distributions normales possibles, un est particulière : la distribution normale réduite qui a toujours une moyenne nulle et un écart type unitaire. \\[N(0, 1)\\] Elle représente la distribution des valeurs pour une variable qui a été standardisée, c’est-à-dire, à laquelle on a soustrait la moyenne et que l’on a divisé par son écart type. \\[Z = \\frac{Y - \\mu}{\\sigma}\\] Sa formulation est nettement simplifiée. \\[\\Phi(Z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{Z^2}{2}}\\] La probabilité qu’une observation soit dans un intervalle de \\(\\pm 1 \\sigma\\) autour de la moyenne est de 2/3 environ. De même, un intervalle de \\(\\pm 2 \\sigma\\) définit une aire de 95%, et celle-ci devient supérieure à 99% pour des observations se situent dans l’intervalle \\(\\pm 3 \\sigma\\) (Fig. 7.10). Figure 7.10: La distribution normale réduite avec les aires centrales autour de 1 et 2 écarts types mises en évidence. 7.6.3 Fonctions et snippets Les fonctions relatives à la distribution normale dans R sont &lt;x&gt;norm(). Le calcul de probabilités se fait à l’aide de pnorm(), de quantiles à partir de qnorm(). Un échantillon pseudo-aléatoire s’obtient à partir de rnorm(). Une série de snippets est à votre disposition dans la SciViews Box pour vous aider (menu (d)istributions: normal à partir de .in) : 7.6.4 Théorème central limite Une des raisons pour lesquelles la distribution normale est très répandue est liée au fait que beaucoup d’autres distributions tendent vers elle de manière asymptotique. Par exemple, une distribution binomiale symétrique (avec \\(p = 0.5\\)) et pour un \\(n\\) croissant ressemblera de plus en plus à une distribution normale. Le théorème central limite démontre cela quelle que soit la distribution de départ. En pratique, la distribution normale est souvent une bonne approximation d’autres distributions pour des tailles d’échantillons déjà à partir de quelques dizaines d’individus. "],
["distribution-log-normale.html", "7.7 Distribution log-normale", " 7.7 Distribution log-normale La loi log-normale est utilisée pour représenter la distribution d’une variable aléatoire qui résulte de la multiplication d’un grand nombre de petits effets indépendants entre eux. C’est le cas en biologie et en chimie, par exemple, la taille d’un animal, la concentration d’une molécule en solution, la température d’un matériau, etc. Ces variables sont définies uniquement pour des valeurs nulles ou positives. Une taille et une concentration négatives ne sont pas possibles. De même pour une température en dessous du zéro absolu (attention, dans ce cas, la température doit être mesurée en Kelvin). 7.7.1 Transformée log La distribution log-normale devient normale lorsque l’on transforme la variable en son logarithme. Donc, si \\[X \\sim log{\\text -}N(0, 0.5)\\] alors \\[log(X) \\sim N(0, 0.5)\\] Par facilité, on défini ses deux paramètres de manière relative à la moyenne \\(\\mu\\) et à l’écart type \\(\\sigma\\) qu’a la distribution normale obtenue après transformation logarithmique. Voici à quoi ressemble la densité de probabilité de cette distribution (Fig 7.11). C’est une distribution asymétrique qui démarre du quantile zéro et est asymptotique à droite en +infini. Figure 7.11: Un exemple de distribution log-normale. 7.7.2 Snippets Les fonctions relatives à la distribution log-normale dans R sont &lt;x&gt;lnorm(). Le calcul de probabilités se fait à l’aide de plnorm(), les quantiles se déterminent à partir de qlnorm() et un échantillon pseudo-aléatoire se calcule en utilisant rlnorm(). Les snippets relatifs à la loi log-normale dans la SciViews Box sont accessibles à partir du menu (d)istributions: log-normal à partir de .il) : "],
["graphique-quantile-quantile.html", "7.8 Graphique quantile-quantile", " 7.8 Graphique quantile-quantile Il n’est pas toujours facile de déterminer quelle est la loi de distribution qui correspond le mieux à la population étudiée. Par contre, une comparaison est possible entre une distribution observée (sur base d’un échantillon, donc, d’un jeu de données) et une distribution théorique (sur base d’une loi théorique). Nous pouvons calculer les quantiles d’un échantillon via une méthode similaire à celle que nous avons employée pour calculer les quartiles et tracer la boite de dispersion dans le module 4. Un quantile divise des données quantitatives en deux sous-groupes de telle manière que le groupe contenant les observations plus petites que ce quantile représente un effectif équivalent à la fraction considérée. Donc, un quantile 10% correspondra à la valeur qui sépare le jeu de données en 10% des observations les plus petites et 90% des observations les plus grandes. Ce quantile dit observé est comparable au quantile dit théorique que nous pouvons calculer sur base d’une probabilité équivalente à la fraction considérée. Prenons un exemple simple pour fixer les idées. Dans les données relatives au plancton, nous avons 50 œufs allongés mesurés. Nous nous demandons si leur taille mesurée ici par la surface (area) de la particule à l’image suit une distribution log-normale. Dans ce cas, il est plus facile de transformer les données en log et de comparer les valeurs ainsi recalculées à une distribution normale. eggs &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) %&gt;.% filter(., class == &quot;Egg_elongated&quot;) %&gt;.% mutate(., log_area = log10(area)) %&gt;.% select(., area, log_area) summary(eggs) # area log_area # Min. :0.4121 Min. :-0.3850 # 1st Qu.:0.4714 1st Qu.:-0.3266 # Median :0.4950 Median :-0.3054 # Mean :0.5100 Mean :-0.2950 # 3rd Qu.:0.5347 3rd Qu.:-0.2719 # Max. :0.6718 Max. :-0.1728 chart(data = eggs, ~ area) + geom_histogram(bins = 12) Sur base de l’histogramme, nous voyons bien que la distribution est soit unimodale et asymétrique, soit bimodale. L’histogramme des données transformées log devrait être plus symétrique si les données originelles suivent bien une distribution log-normale unimodale. chart(data = eggs, ~ log_area) + geom_histogram(bins = 12) C’est légèrement mieux, mais la distribution ne parait pas parfaitement symétrique, voire peut-être encore bimodale (pas flagrant toutefois). L’histogramme est un bon outil pour visualiser globalement une distribution, mais le graphique quantile-quantile offre une représentation plus précise pour comparer précisément deux distributions. Comme nous avons 50 observations à disposition, nous pouvons calculer les quantiles tous les 2% à l’aide de la fonction quantile(). De même, nous pouvons utiliser qnorm() pour calculer les quantiles théoriques selon une distribution normale réduite. Cela donne : (probas &lt;- seq(from = 0.02, to = 0.98, by = 0.02)) # [1] 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 # [15] 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 0.56 # [29] 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 # [43] 0.86 0.88 0.90 0.92 0.94 0.96 0.98 # Quantiles observés dans l&#39;échantillon q_obs &lt;- quantile(eggs$log_area, probs = probas) # quantiles theoriques selon la distribution normale réduite q_theo &lt;- qnorm(probas, mean = 0, sd = 1) qq &lt;- tibble(q_obs = q_obs, q_theo = q_theo) qq # # A tibble: 49 x 2 # q_obs q_theo # &lt;dbl&gt; &lt;dbl&gt; # 1 -0.373 -2.05 # 2 -0.367 -1.75 # 3 -0.351 -1.55 # 4 -0.348 -1.41 # 5 -0.346 -1.28 # 6 -0.343 -1.17 # 7 -0.343 -1.08 # 8 -0.336 -0.994 # 9 -0.333 -0.915 # 10 -0.332 -0.842 # # ... with 39 more rows Si les deux distributions sont compatibles, nous devrions avoir proportionnalité entre les quantiles théoriques et les quantiles observés. Cela devrait donc se marquer par un alignement des points sur un graphique des quantiles observés en fonction des quantiles théoriques (Fig. 7.12). chart(data = qq, q_obs ~ q_theo) + geom_point() Figure 7.12: Graphique quantile-quantile construit à la main. Cet alignement n’est pas flagrant. Le graphique proposé par la fonction car::qqPlot() (Fig. 7.13) et accessible depuis le snippet .cuqqnorm est le même, mais il ajoute différents éléments qui aident à l’interprétation : Une droite (ligne continue bleue) selon laquelle les points devraient s’aligner en cas de concordance parfaite entre les deux distributions Une enveloppe de confiance (lignes pointillées bleues) qui tient compte de la variabilité aléatoire d’un échantillon à l’autre pour inclure une enveloppe de tolérance avec une fiabilité de 95%. Cela signifie que 95% des points doivent, en principe, se trouver à l’intérieur de l’enveloppe. Une individualisation des points les plus suspects éventuels, en indiquant le numéro de la ligne dans le jeu de donnée de chaque point suspect. car::qqPlot(eggs[[&quot;log_area&quot;]], distribution = &quot;norm&quot;, envelope = 0.95, col = &quot;Black&quot;, ylab = &quot;log(area [mm^2])&quot;) Figure 7.13: Graphique quantile-quantile comparant le log(area) en fonction d’une distribution normale obtenu à l’aide de car::qqPlot(). Interprétation Si quasiment tous les points sont compris dans l’enveloppe de confiance à 95%, le graphique indique que les deux distributions ne sont pas fondamentalement différentes. Ici les points correspondant aux valeurs les plus élevées sortent de l’enveloppe pour un certain nombre d’entre eux d’affilée, et les points 11 et 30 sont considérés comme suspects. Ceci indique que les effectifs observés dans l’échantillon sont plus nombreux en queue droite de distribution que ce que la distribution normale prédit en théorie. Ceci confirme l’impression de distribution asymétrique et/ou bimodale. Il est probable qu’on ait au moins deux types d’œufs allongés différents dans l’échantillon, avec le second type moins nombreux, mais représenté par des œufs plus gros, ce qui enfle la partie droite de la distribution. "],
["chi2.html", "Module 8 Test Chi carré", " Module 8 Test Chi carré Dans ce module, nous entrons dans le monde de l’inférence statistique et des tests d’hypothèses qui nous permettront de répondre à des questions biologiques sur base de données empiriques malgré une incertitude inévitables (hasard de l’échantillonnage, variabilité individuelle, erreurs de mesure, …). Objectifs Appréhender l’inférence statistique Être capable d’effectuer un échantillonnage correctement Comprendre ce qu’est un test d’hypothèse Connaitre la distribution Chi2 et les tests d’hypothèses basés sur cette distribution Développer un regard critique (positif !) sur le travail de ses collègues via l’évaluation d’un rapport par les pairs (initiation au “peer reviewing”) Prérequis Les probabilités et lois de distributions statistiques vues au module 7 doivent être comprises avant d’attaquer cette section. "],
["echantillonnage.html", "8.1 Échantillonnage", " 8.1 Échantillonnage Si nous pouvions mesurer tous les individus d’une population statistique à chaque fois, nous n’aurions pas besoin des statistiques. Mais ce n’est pratiquement jamais possible. Tout d’abord, le nombre d’individus est potentiellement très grand. Le travail nécessaire risque alors d’être démesuré. Afin de limiter les mesures à un nombre raisonnable de cas, nous effectuons un échantillonnage qui consiste à prélever un petit sous-ensemble de taille \\(n\\) donné depuis la population de départ. Il existe différentes stratégies d’échantillonnage, mais la plus fréquente est l’échantillonnage aléatoire pour lequel : chaque individu dans la population a la même probabilité d’être pris dans l’échantillon, les mesures et les individu sont indépendants les uns des autres. Nous n’avons pas forcément accès à tous les individus d’une population. Dans ce cas, nous devons la limiter à un sous-ensemble raisonnable. Par exemple, il est impossible de mesurer toutes les souris. Par contre, nous pouvons décider d’étudier la ou les souches de souris disponibles dans l’animalerie, ou chez nos fournisseurs. Quoi qu’il en soit, l’échantillon n’est qu’un petit sous-ensemble sélectionné par un mécanisme faisant intervenir le hasard. Donc, deux échantillons de la même population ont un très forte probabilité d’être différents l’un de l’autre. Il en va également des statistiques calculées sur ces échantillons, comme les effectifs observés pour chaque niveau de variables qualitatives ou les valeurs moyennes pour les variables quantitatives, par exemple. Cette variabilité d’un échantillon à l’autre ne nous intéresse pas car elle n’apporte pas d’information sur la population elle-même. Ce qui nous intéresse, c’est d’estimer au mieux les valeurs (effectifs, moyennes, etc.) dans la population. L’estimation de paramètres d’une population par le biais de calculs sur un échantillon représentatif issu de cette population s’appelle l’inférence statistique. Travail préliminaire Avant de vous lancer dans l’inférence statistique, assurez-vous d’avoir effectué soigneusement les trois étapes suivantes : Vous comprenez bien la question posée, en termes biologiques. Vous connaissez ou vous êtes documenté sur l’état de l’art en la matière (bibliographie). Que sait-on déjà du phénomène étudié ? Quels sont les aspects encore inconnus ou à l’état de simples hypothèses ? Vous avez vérifié que la façon dont les mesures ont été prises permettra effectivement de répondre à la question posée. En particulier, vous avez vérifié que l’échantillonnage a été réalisé dans les règles pour qu’il soit représentatif de la population étudiée. En outre, vous cernez clairement quelle est la population effectivement étudiée. C’est important pour éviter plus tard de sur-généraliser les résultats obtenus (les attribuer à une population plus large que celle effectivement étudiée). Vous avez effectué une analyse exploratoire des données. Vous avez représenté les données à l’aide de graphiques appropriés et vous avez interprété ces graphiques afin de comprendre ce que le jeu de données contient. Vous avez également résumé les données sous forme de tableaux synthétiques et vous avez, si nécessaire, remaniés et nettoyés vos données. "],
["test-dhypothese.html", "8.2 Test d’hypothèse", " 8.2 Test d’hypothèse Le test d’hypothèse ou test statistique est l’outil le plus simple pour répondre à une question via l’inférence. Il s’agit ici de réduire la question à sa plus simple expression en la réduisant à deux hypothèses contradictoires (en gros, la réponse à la question est soit “oui”, soit “non” et rien d’autre). L’hypothèse nulle, notée \\(H_0\\) est l’affirmation de base ou de référence que l’on cherchera à réfuter, L’hypothèse alternative, notée \\(H_1\\) ou \\(H_a\\) représente une autre affirmation qui doit nécessairement être vraie si \\(H_0\\) est fausse. Les deux hypothèses ne sont pas symétriques. Notre intention est de rejeter \\(H_0\\). Dans ce cas, nous pourrons considérer que \\(H_1\\) est vraie, avec un certain degré de certitude que nous pourrons également quantifier. Si nous n’y arrivons pas, nous dirons que nous ne pouvons pas rejeter \\(H_0\\), mais nous ne pourrons jamais dire que que nous l’acceptons car dans ce cas, deux explications resteront possibles : (1) \\(H_0\\) est effectivement vraie, ou (2) \\(H_0\\) est fausse mais nous n’avons pas assez de données à disposition pour le démontrer avec le niveau de certitude recherché. A vous de jouer ! Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;08a_chi2&quot;) 8.2.1 Becs croisés des sapins Tout cela reste très abstrait. Prenons un exemple concret simple. Le bec-croisé des sapins Loxia curvirostra Linné 1758 est un passereau qui a la particularité d’avoir un bec dont les deux parties se croisent, ce qui donne un outil particulièrement adapté pour extraire les graines de conifères dont il se nourrit. Bec-croisés des sapins mâles montrant les deux variétés (bec croisé à gauche ou à droite). Photo : Elaine R. Wilson (license CC BY-SA 3.0). Comme des individus à bec croisé à gauche et d’autres à bec croisé à droite se rencontrent dans la même population, Groth (1992) a comptabilisé les deux types dans un échantillon aléatoire et représentatif de plus de 3000 oiseaux. Il a obtenu les résultats suivants : (crossbill &lt;- as.table(c(left = 1895, right = 1752))) # left right # 1895 1752 Les scientifiques pensent que les variétés gauches et droites se rencontrent avec un ratio 1:1 dans la population étudiée suite à une sélection présumée basée sur le rapport des deux variétés. La question se traduit sous forme d’un test d’hypothèse comme ceci : \\(H_0: \\mathrm{P}(left) = \\frac{1}{2}\\ \\mathrm{et}\\ \\mathrm{P}(right) = \\frac{1}{2}\\) \\(H_1: \\mathrm{P}(left) \\neq \\frac{1}{2}\\ \\mathrm{ou}\\ \\mathrm{P}(right) \\neq \\frac{1}{2}\\) Pouvons-nous rejeter \\(H_0\\) ici ? 8.2.2 Test Chi2 univarié Le test Chi2 (ou \\(\\chi^2\\)) de Pearson est un test d’hypothèse qui permet de comparer des effectifs observés notés \\(a_i\\) à des effectifs théoriques \\(\\alpha_i\\) sous l’hypothèse nulle pour les différents niveaux \\(i\\) allant de 1 à \\(n\\) d’une variable qualitative (version dite univariée du test). A noter que par rapport à la définition des hypothèses ci-dessus, ce ne sont pas les probabilités qui sont testées, mais les effectifs. Notre tableau de contingence à simple entrée crossbill contient nos \\(a_i\\). Nous devons donc calculer quels sont les effectifs théoriques \\(\\alpha_i\\). Le nombre total d’oiseaux observés est : sum(crossbill) # [1] 3647 … et les effectifs attendus sous \\(H_0\\) sont : (alpha_i &lt;- c(left = sum(crossbill)/2, right = sum(crossbill)/2)) # left right # 1823.5 1823.5 Les hypothèses du test \\(\\chi^2\\) univarié se définissent comme ceci : \\(H_0: a_i = \\alpha_i\\) pour tout \\(i\\) \\(H_1: a_i \\neq \\alpha_i\\) pour au moins un \\(i\\) Le principe de la statistique \\(\\chi^2\\) consiste à sommer les écarts au carré par rapport aux \\(\\alpha_i\\) de référence divisés par ces mêmes \\(\\alpha_i\\) pour quantifier l’écart entre les valeurs observées et les valeurs théoriques. Donc : \\[\\chi^2_\\mathrm{obs} = \\sum_{i=1}^n\\frac{(a_i - \\alpha_i)^2}{\\alpha_i}\\] Notez que cette statistique prend la valeur nulle lorsque tous les \\(a_i\\) sont strictement égaux aux \\(\\alpha_i\\). Dans tous les autres cas, des termes positifs (le carré de différences est toujours une valeur positive) apparaissent. Donc la statistique \\(\\chi^2\\) est d’autant plus grande que les observations s’éloignent de la théorie. Calculons \\(\\chi^2_\\mathrm{obs}\\) dans notre cas37 : (chi2_obs &lt;- sum((crossbill - alpha_i)^2 / alpha_i)) # [1] 5.607074 Pour répondre à la question, il nous faut une loi de distribution statistique qui permette d’associer une probabilité au quantile \\(\\chi^2_\\mathrm{obs}\\) sous \\(H_0\\). C’est là que le statisticien Karl Pearson vient à notre secours. Il a, en effet, modélisé la distribution statistique du \\(\\chi^2\\). La loi du même nom admet un seul paramètre, les degrés de libertés (ddl) qui sont égaux au nombre de niveaux de la variable facteur étudiée \\(n\\) moins un. Ici, ddl = 2 - 1 = 1. La Fig. 8.1 représente la densité de probabilité d’une loi \\(\\chi^2\\) typique38. C’est une distribution qui démarre à zéro, passe par un maximum et est asymptotique horizontale à \\(+\\infty\\). Figure 8.1: Allure typique de la densité de probabilité de la distribution Chi2 (ici ddl = 3). 8.2.3 Seuil α du test Le raisonnement du test d’hypothèse pour répondre à notre question est le suivant. Connaissant la densité de probabilité théorique sous \\(H_0\\), nous savons que, plus le \\(\\chi^2_\\mathrm{obs}\\) est grand, moins il est plausible. Nous devons décider d’une limite à partir de laquelle nous considérerons que la valeur observée est suffisamment grande pour que \\(H_0\\) devienne trop peu plausible et nous pourrons alors la rejeter. Cette limite se définit sous la forme d’une probabilité correspondant à une zone ou aire de rejet définie dans la distribution théorique de référence sous \\(H_0\\). Cette limite s’appelle le seuil \\(\\alpha\\) du test. Choix du seuil \\(\\alpha\\) d’un test d’hypothèse. Le seuil \\(\\alpha\\) est choisi avant de réaliser le test. Il est un savant compromis entre le risque de se tromper qui diminue plus \\(\\alpha\\) est petit, et la possibilité d’obtenir le rejet de \\(H_0\\) lorsqu’elle est fausse qui augmentera avec \\(\\alpha\\). Si on veut être absolument certain du résultat, on prend \\(\\alpha = 0\\), mais dans ce cas on ne rejette jamais \\(H_0\\) et on ne tire donc jamais aucune conclusion utile. Donc, nous devons assouplir les règles et accepter un petit risque de se tromper. Généralement, les statisticiens choisissent \\(\\alpha\\) = 5% dans les cas courants, et prennent 1%, ou même 0,1% dans les cas où il faut être plus strict (par exemple, si des vies dépendent du résultat). Nous pouvons nous baser sur ces références, même si nous verrons plus loin que cette pratique est de plus en plus remise en cause dans la littérature scientifique. Poursuivons. Nous choisissons notre seuil \\(\\alpha\\) = 5%. Cela définit l’aire la plus extrême de 5% à droite de la distribution \\(\\chi^2\\) à 1 ddl comme zone de rejet (remplie en rouge sur la Fig. 8.2). Il nous suffit maintenant de voir où se place notre \\(\\chi^2_\\mathrm{obs}\\). S’il se situe dans la zone en rouge, nous rejetterons \\(H_0\\), sinon, nous ne la rejetterons pas. Figure 8.2: Densité de probabilité sous H0 (distribution Chi2 à 1 ddl), zone de rejet de 5% en rouge et position de la valeur observée (trait vertical rouge). Où se situe la limite ? Nous pouvons facilement la calculer : qchisq(0.05, df = 1, lower.tail = FALSE) # [1] 3.841459 Notre \\(\\chi^2_\\mathrm{obs}\\) = 5,61 est plus grand que cette limite à 3,84 et se situe donc dans la zone de rejet de \\(H_0\\) du test. Nous rejetons donc \\(H_0\\) ici. Nous dirons que les becs croisés à gauche sont significativement plus nombreux que ceux à droite au seuil \\(\\alpha\\) de 5% (test \\(\\chi^2\\) = 5,61, ddl = 1, valeur P = 0,018). Notez bien la façon particulière de reporter les résultats d’un test d’hypothèse ! Il nous manque encore juste un élément… qu’est-ce que cette “valeur P” de 0,018 reportée dans le résultat ? En fait, c’est la valeur de probabilité associée au test et correspond ici à l’aire à droite définie depuis le \\(\\chi^2_\\mathrm{obs}\\). Calculons-la : pchisq(5.61, df = 1, lower.tail = FALSE) # [1] 0.01785826 Le test d’hypothèse reporte la valeur P afin qu’un lecteur qui aurait choisi un autre seuil \\(\\alpha\\) pourrait effectuer immédiatement sa propre comparaison sans devoir refaire les calculs. La règle est simple : valeur P &lt; seuil \\(\\alpha\\), \\(=&gt; \\mathrm{R}H_0\\) (on rejette \\(H_0\\)), valeur P ≥ seuil \\(\\alpha\\), \\(=&gt; \\rlap{\\mathrm{R}} \\diagup H_0\\) (on ne rejette pas \\(H_0\\)). Pour finir, nous ne devons heureusement pas refaire tous les calculs à la main à chaque fois que nous voulons faire un test du \\(\\chi^2\\) dans R. La fonction chisq.test() fait tout cela pour nous39. Elle est également disponible dans les snippets à partir du menu hypothesis test : contingency ou .hc (test Chi2 univarié). chisq.test(crossbill, p = c(1/2, 1/2), rescale.p = FALSE) # # Chi-squared test for given probabilities # # data: crossbill # X-squared = 5.6071, df = 1, p-value = 0.01789 Conditions d’application Tout test d’hypothèse impose des conditions d’application qu’il faudra vérifier avant d’effectuer le test. Pour le test \\(\\chi^2\\), ce sont : échantillonnage aléatoire et observations indépendantes, aucun effectif théorique sous \\(H_0\\) nul, aucun effectif observé, si possible, inférieur à 5 (pas condition stricte). C’est bien le cas ici. 8.2.4 Effet de l’effectif étudié En inférence, la qualité des données (échantillons représentatifs) est importante, mais la quantité aussi. Plus vous pourrez mesurer d’individus, mieux c’est. Par contre, dès que la taille de l’échantillon (ici, l’effectif total mesuré) est suffisant pour rejeter \\(H_0\\), vous n’avez plus besoin d’augmenter la taille de votre échantillon40. Voyons l’effet de la taille de l’échantillon sur l’étude des becs croisés des sapins. Nous n’avons pas besoin d’un effectif plus grand que celui mesuré, car nous rejetons \\(H_0\\) ici. Qu’aurait donné notre test \\(\\chi^2\\), par contre, si l’auteur avait mesuré disons 10 fois moins d’oiseaux, les proportions restant par ailleurs identiques entre becs croisés à gauche et à droite ? # Proportions équivalentes, mais échantillon 10x plus petit (crossbill2 &lt;- as.table(c(left = 190, right = 175))) # left right # 190 175 chisq.test(crossbill2, p = c(1/2, 1/2), rescale.p = FALSE) # # Chi-squared test for given probabilities # # data: crossbill2 # X-squared = 0.61644, df = 1, p-value = 0.4324 Nous constatons que la valeur du \\(\\chi^2_{obs}\\) dépend de l’effectif. Sa valeur est plus petite ici. Par conséquent, la valeur P a également changé et elle vaut à présent 43%. Cette valeur est supérieure maintenant à notre seuil \\(\\alpha\\) de 5%. Donc, nous ne pouvons pas rejeter \\(H_0\\). Dans un pareil cas, nous conclurons que les becs croisés à gauche ne sont pas significativement plus nombreux que ceux à droite au seuil \\(\\alpha\\) de 5% (test \\(\\chi^2\\) = 0,62, ddl = 1, valeur P = 0,43). Notez, c’est important, que nous n’avons pas écrit “ne sont pas”, mais nous avons précisé “ne sont pas significativement” plus nombreux. C’est un détail très important. En effet, cela veut dire que l’on ne peut pas conclure qu’il y ait des différences sur base de l’échantillon utilisé, mais il se peut aussi que l’échantillon ne soit pas suffisamment grand pour mettre en évidence une différence. Or, nous avons analysé en réalité un plus grand échantillon (crossbill), et nous savons bien que c’est effectivement le cas. Est-ce que vous saisissez bien ce que le mot significativement veut dire, et la subtilité qui apparaît lorsqu’un test d’hypothèse ne rejette pas \\(H_0\\) ? Les conclusions tirées avec crossbill et crossbill2 et le même test d’hypothèse sont diamétralement opposées car l’un rejette et l’autre ne rejette pas \\(H_0\\). Pourtant ces deux analyses ne se contredisent pas ! Les deux interprétations sont simultanément correctes. C’est l’interprétation asymétrique du test qui permet cela, et l’adverbe significativement est indispensable pour introduire cette nuance dans le texte. 8.2.5 Test Chi2 d’indépendance Dans le cas d’un tableau de contingence à double entrée, qui croise les niveaux de deux variables qualitatives, nous pouvons effectuer également un test \\(\\chi^2\\). Celui-ci sera calculé légèrement différemment et surtout, les hypothèses testées sont différentes. A vous de jouer ! Une séance d’exercice vous est proposée en lien avec le test Chi2 d’indépendance. Vous pouvez réaliser ces exercices en parallèle à la lecture de la présente section. Ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;08b_chi2&quot;) Reprenons le jeu de données concernant le test d’une molécule potentiellement anti-cancéreuse, le timolol : timolol &lt;- tibble( traitement = c(&quot;timolol&quot;, &quot;timolol&quot;, &quot;placebo&quot;, &quot;placebo&quot;), patient = c(&quot;sain&quot;, &quot;malade&quot;, &quot;sain&quot;, &quot;malade&quot;), freq = c(44, 116, 19, 128) ) # Création du tableau de contingence timolol_table &lt;- xtabs(data = timolol, freq ~ patient + traitement) timolol_table # traitement # patient placebo timolol # malade 128 116 # sain 19 44 Nous avons ici un tableau de contingence à double entrée qui répertorie le nombre de cas attribués aléatoirement au traitement avec placebo (somme de la première colonne, soit 128 + 19 = 147 patients) et le nombre de cas qui ont reçu du timolol (116 + 44 = 160), tout autre traitement étant par ailleurs équivalent. Nous avons donc un total général de 307 patients étudiés. La répartition dans le tableau selon les ligne est, elle, tributaire des effets respectifs des deux traitements ? La clé ici est de considérer comme \\(H_0\\) un partitionnement des cas équivalent entre les deux traitements. Ceci revient au même que de dire que l’effet d’une variable (le traitement administré) est indépendant de l’effet de l’autre variable (le fait d’être guéri ou non). C’est pour cette raison qu’on parle de test \\(\\chi^2\\) d’indépendance. Les hypothèses sont : \\(H_0:\\) indépendance entre les deux variables \\(H_1:\\) dépendance entre les deux variables Toute la difficulté est de déterminer les \\(\\alpha_i\\), les effectifs qui devraient être observés sous \\(H_0\\). Si le partitionnement était identique selon tous les niveaux des deux variables, nous aurions autant de cas dans chaque cellule du tableau, soit 307/4 = 76,75. Mais n’oublions pas que nous avons attribués plus de patients au traitement timolol qu’au traitement placebo. De même, nous ne contrôlons pas le taux de guérison de la maladie qui n’est d’ailleurs généralement pas d’un patient sur 2. Il faut donc pondérer les effectifs dans les lignes et les colonnes par rapport aux totaux dans les différents niveaux des variables (en colonne pour la variable traitement, en ligne pour la variable patient). Donc, si nous indiçons les lignes avec \\(i = 1 ..m\\) et les colonnes avec \\(j = 1..n\\), nos effectifs théoriques \\(\\alpha_{i,j}\\) sous hypothèse d’indépendance entre les deux variables sont : \\[\\alpha_{i, j} =\\frac{total\\ ligne_i \\times total\\ colonne_j}{total\\ général}\\] Nous pouvons dès lors calculer le \\(\\chi^2_{obs}\\) pratiquement comme d’habitude via : \\[\\chi^2_{obs} = \\sum_{i=1}^m{\\sum_{j=1}^n{\\frac{(a_{i,j} - \\alpha_{i,j})^2}{\\alpha_{i,j}}}}\\] Enfin, nous comparons cette valeur à la distribution théorique de \\(\\chi^2\\) à \\((m - 1) \\times (n - 1)\\) degrés de liberté. Dans le cas d’un tableau 2 par 2, nous avons 1 degré de liberté. Voici le test effectué à l’aide de la fonction chisq.test() suivi de l’affichage des effectifs théoriques. Vous accédez facilement à ce code depuis le snippet Chi2 test (independence) dans le menu hypothesis tests: contingency à partir de .hc. Mais avant toute chose, nous devons choisir le seuil \\(\\alpha\\) avant de réaliser le test. Nous prendrons ici, par exemple, 1% puisque l’analyse est effectuée dans un contexte critique (maladie mortelle). (chi2. &lt;- chisq.test(timolol_table)); cat(&quot;Expected frequencies:\\n&quot;); chi2.[[&quot;expected&quot;]] # # Pearson&#39;s Chi-squared test with Yates&#39; continuity correction # # data: timolol_table # X-squared = 9.1046, df = 1, p-value = 0.00255 # Expected frequencies: # traitement # patient placebo timolol # malade 116.83388 127.16612 # sain 30.16612 32.83388 Interprétation La valeur p de 0,0026 est inférieure au seuil \\(\\alpha\\) choisi de 0,01. Donc, nous rejetons \\(H_0\\). Il n’y a pas indépendance entre les deux variables. Pour voir quels sont les effets de la dépendance entre les variables, nous devons comparer les effectifs théoriques affichés ci-dessus avec les effectifs observés. Dans le cas du placebo, sous \\(H_0\\), nous aurions du obtenir 117 malades contre 30 patients guéris. Or, nous en avons 128 malades et seulement 19 guéris. D’un autre côté, sous \\(H_0\\) nous aurions du observer 127 patients malades et 33 sains avec le timolol. Or, nous en observons 116 malades et 44 sains. Donc, les valeurs observées sont en faveur d’un meilleur effet avec le timolol. Nous pourrons dire : le timolol a un effet positif significatif sur la guérison de la maladie au seuil \\(\\alpha\\) de 1% (\\(\\chi^2\\) d’indépendance = 9,10, ddl = 1, valeur P = 0,0026). Correction de Yates Si nous calculons le \\(\\chi^2_{obs}\\) à la main, nous obtenons : alpha_ij &lt;- chi2.[[&quot;expected&quot;]] # Les a_i,j sont dans timolol_table sum((timolol_table - alpha_ij)^2 / alpha_ij) # [1] 9.978202 Cela donne 9,98. Or notre test renvoie la valeur de 9,10. A quoi est due cette différence ? Lisez bien l’intitulé du test réalisé. Il s’agit de “Pearson’s Chi-squared test with Yates’ continuity correction”. Il s’agit d’une correction introduite par R dans le cas d’un tableau 2 par 2 uniquement et qui tient compte de ce que la distribution sous \\(H_0\\) est estimée à partir des mêmes données que celle utilisées pour le test, ce qui introduit un biais ainsi corrigé. Il est donc déconseillé de désactiver cette correction, même si nous pouvons le faire en indiquant correct = FALSE (ci-dessous, juste pour vérifier notre calcul du \\(\\chi^2_{obs}\\) qui est maintenant identique, 9,98). # Test d&#39;indépendance sans correction de Yates chisq.test(timolol_table, correct = FALSE) # # Pearson&#39;s Chi-squared test # # data: timolol_table # X-squared = 9.9782, df = 1, p-value = 0.001584 Conditions d’application échantillon représentatif (échantillonnage aléatoire et individus indépendants les uns des autres), attribution des traitements aux individus de manière aléatoire, aucun effectif théorique nul, Si possible, aucun effectif observé inférieur à 5 (pas règle strict, mais voir à utiliser un test exact de Fisher ci-dessous dans ce cas). 8.2.6 Autres tests Chi2 Les test du \\(\\chi^2\\) est également utilisé, dans sa forme univariée, pour comparer les effectifs observés par rapport à des effectifs théoriques suivant un loi de distribution discrète. Ce test s’appelle un test de qualité d’ajustement (“goodness-of-fit test” en anglais). Dans ce cas, le nombre de degrés de liberté est le nombre de catégories moins le nombre de paramètres de la distribution moins un. Pour l’ajustement à une loi de distribution continue, il est possible de découper les données en classes et d’appliquer un test \\(\\chi^2\\) dessus ensuite. Il existe cependant d’autres tests considérés comme plus efficaces dans ce cas, comme le test de Komogorov-Smirnov, notamment avec les corrections introduites par Lillefors. Pour l’ajustement à une distribution normale, des tests spécialisés existent comme le test de Shapiro-Wilk. Ce dernier est disponible depuis les snippets de la SciViews Box dans le menu Hypothesis tests: distribution ou .hd, et puis Shapiro-Wilk test of normality. Gardez toujours à l’esprit que, quelle que soit la qualité d’un test d’ajustement, vous n’aurez jamais qu’une réponse binaire (oui ou non l’échantillon s’ajuste à telle distribution théorique). Les causes de dérive sont innombrables et seules des bonnes représentations graphiques (histogramme, graphe en violon, et surtout, graphique quantile-quantile) sont suffisamment riche en information pour explorer pourquoi et comment la distribution diffère d’une distribution théorique. A vous de jouer ! Employez le test d’hypothèse que vous venez d’apprendre dans votre rapport sur la biométrie humaine : sdd1_biometry-...(nom du groupe) Pour en savoir plus Le test de Chi2 avec R, Le test G est considéré comme une bonne alternative dans certains cas (voir aussi Chi-square vs. G-test). Le test exact de Fisher comme test alternatif, en particulier lorsque les effectifs sont faibles. Faites également le calcul manuellement à la calculatrice pour vérifier que vous avez bien compris.↩ Les fonctions qui permettent les calculs relatifs à la distribution \\(\\chi^2\\) dans R sont &lt;x&gt;chisq(), et les snippets correspondants dans la SciViews Box sont disponibles à partir de .ic. Leur utilisation est similaire à celle des distributions vues au module 7.↩ Avec chisq.test(), l’argument p = est la liste des probabilités attendues sous \\(H_0\\) et dont la somme vaut un. On peut aussi donner les effectifs attendus, mais il faut alors préciser rescale.p = TRUE.↩ Attention ! Vous devez fixer la taille de l’échantillon dès le départ a priori. Vous ne pouvez pas accumuler des données jusqu’à obtenir un rejet de \\(H_0\\), sans quoi votre analyse sera biaisée.↩ "],
["evaluation-par-les-pairs.html", "8.3 Evaluation par les pairs", " 8.3 Evaluation par les pairs En science, l’évaluation par les pairs (“peer-reviewing” en anglais) est le mécanisme le plus efficace pour améliorer la qualité des travaux publiés (articles scientifiques ou ouvrages plus conséquents). Par définition, les résultats publiés en science sont à la frontière de l’inconnu. Il est donc difficile de vérifier si le travail est correct. Les personnes les plus à même de le faire sont les collègues qui travaillent sur le même sujet, ou dans un domaine proche, les “pairs”. Avant d’être rendus publics, les travaux scientifiques font l’objet d’un ou plusieurs rapports par des pairs. Cette phase est la révision de l’article. Le rapport se veut constructif dans le but d’améliorer la qualité du travail. Il ne s’agit pas d’“enfoncer” les auteurs initiaux, mais le “referee” se doit d’être honnête et donc, de mettre le doigt sur les défauts et lacunes du travail également. A vous de jouer ! Vous allez vous initier au travail d’arbitrage (“referee” scientifique) sur base du rapport sur la biométrie de l’oursin violet. Vous partagerez votre dépôt Github avec un de vos collègue. Vous recevrez également un accès au dépôt de quelqu’un d’autre. Votre travail consistera à lire avec un œil critique et constructif le travail que vous recevrez. Vous ajouterez un fichier review.md à la racine du projet où vous consignerez vos remarques générales. Pour les remarques particulières directement dans le rapport, utilisez la balise de citation de Markdown (commencez le paragraphe par &gt;), par exemple : Ceci est un commentaire dans le texte. N’effacer, ni ne modifiez aucun texte directement dans ce rapport. Si vous devez suggérer l’élimination de texte, utilisez la balise Markdown qui sert à biffer ce texte sous forme de deux tildes devant et derrière (~~, ce qui donne texte biffé). Ensuite, effectuez un “commit” de vos commentaires sur le dépôt Github de votre collègue. De votre côté, lorsque vous recevrez le rapport relatif à votre propre projet, lisez les commentaires. Ne modifiez pas le fichier review.md, mais par contre, éditez le texte et éliminez les commentaires directement dans le rapport au fur et à mesure que vous le corriger en tenant compte des remarques. Vous pourrez éventuellement apporter des réponses ou des justifications aux commentaires globaux du fichier review.md. Les intructions sont détaillées ici : https://github.com/BioDataScience-Course/sdd_lesson/blob/master/sdd1_08/presentations/correction.md "],
["moyenne.html", "Module 9 Moyenne", " Module 9 Moyenne Objectifs De manière générale, pouvoir répondre à différentes questions concernant une ou deux moyennes Découvrir la distribution t de Student Comprendre le principe de la distribution d’un échantillon Appréhender l’intervalle de confiance, savoir le calculer et l’utiliser Comprendre les différentes variantes du test t de Student et être capable de l’utiliser pour résoudre des questions pratiques en biologie Connaître également le test de Wilcoxon-Mann-Withney, et pouvoir déterminer quand l’utiliser à la place du test de Student Prérequis Ce module élabore sur les notions vues au module 7 concernant les lois de distribution statistiques et sur le concept de test d’hypothèse abordé dans le module 8. Ces deux précédents modules doivent donc être maîtrisés avant d’aller plus avant ici. "],
["une-histoire-de-biere.html", "9.1 Une histoire de bière…", " 9.1 Une histoire de bière… Les belges, c’est connu, apprécient la bière. Mais ils ne sont pas les seuls, et c’est très heureux ! Car c’est en effet grâce à un certain William Sealy Gosset, brasseur et statisticien (et oui, ça ne s’invente pas) que l’un des tests d’hypothèses des plus utilisés en biologie a vu le jour : le test de “Student” qui permet de comparer des moyennes. Pour la petite histoire, Gosset a travaillé pour une certaine brasserie irlandaise du nom de Guiness au début du 20èmesiècle. C’est en étudiant la variabilité de sa bière d’un cru à l’autre que Gosset a découvert la façon dont la moyenne d’un échantillon se distribue. Il a pu dériver une formulation mathématique de cette distribution, la distribution t de Student, et à partir de là, nous verrons que de nombreuses applications en découlent. Nous pourrons, par exemple, dire si deux moyennes diffèrent significativement l’une de l’autre ou pas. Mais au fait, pourquoi, cette distribution porte-t-elle le nom de “Student” ? Visionnez la vidéo suivante (malheureusement en anglais) pour le découvrir41. Le contrat que Gosset a signé avec son employeur l’empêchait de publier des résultats scientifiques sous son vrai nom. Ainsi, il décida de publier sa trouvaille qui occupe aujourd’hui une place très importante en statistiques sous le pseudonyme de “Student” (l’étudiant). Ce n’est qu’à sa mort, en 1937, que l’on pu révéler le nom de l’auteur qui est derrière cette fantastique trouvaille. Mais au fait, de quoi s’agit-il exactement ? Nous allons le découvrir dans la section suivante. Vous pouvez activer les sous-titres en anglais via la barre de boutons en bas de la vidéo pour vous aider à comprendre l’histoire.↩ "],
["distribution-dechantillonnage.html", "9.2 Distribution d’échantillonnage", " 9.2 Distribution d’échantillonnage Afin d’appliquer directement les concepts vu durant ce module, ouvrez RStudio dans votre SciViews Box, puis exécutez l’instruction suivante dans la fenêtre console : BioDataScience::run(&quot;09a_ttest&quot;) Pour rappel, nous faisons de l’inférence sur base d’un échantillon parce que nous sommes incapables de mesurer tous les individus d’une population. Il faut au préalable que l’échantillon soit représentatif, donc réalisé dans les règles de l’art (par exemple, un échantillonnage aléatoire simple de la population). Nous pouvons calculer la moyenne d’un échantillon facilement (eq. (9.1). \\[\\begin{equation} \\bar{x}=\\sum_{i=1}^n{\\frac{x_i}{n}} \\tag{9.1} \\end{equation}\\] où \\(x\\) est une variable quantitative (donc numeric dans R) et \\(n\\) est la taille de l’échantillon, donc le nombre d’individus mesurés. On notera \\(\\bar{x}\\) la moyenne de \\(x\\), que l’on prononcera “x barre”. Nous utiliserons également l’écart type, noté \\(\\sigma_x\\) pour la population et \\(s_x\\) pour l’échantillon qui se calcule sur base de la somme des écarts à la moyenne au carré (eq. (9.2)) : \\[\\begin{equation} s_x = \\sqrt{\\sum_{i=1}^n{\\frac{(x_i - \\bar{x})^2}{n-1}}} \\tag{9.2} \\end{equation}\\] A noter que \\(s^2\\) est également appelée la variance42. En fait, ce qui nous intéresse, ce n’est pas vraiment la moyenne de l’échantillon, mais celle de la population que l’on notera \\(\\mu\\)43. D’où la question : comment varie la moyenne d’un échantillon à l’autre ? Nous pouvons répondre à cette question de manière empirique en utilisant le générateur pseudo-aléatoire de R. Partons d’une distribution théorique de la population qui soit normale, de moyenne *$ = 8 et d’écart type \\(\\sigma\\) = 2. Nous pouvons échantillonner neuf individus. Cela donne : set.seed(8431641) smpl1 &lt;- rnorm(9, mean = 8, sd = 2) smpl1 # [1] 9.138562 8.496824 9.573743 7.276562 8.300520 5.176688 4.209415 # [8] 10.700260 7.264703 mean(smpl1) # [1] 7.793031 Dans ce cas-ci, nous obtenons une moyenne de 7,8. Ce n’est pas égal à 8. Le hasard de l’échantillonnage en est responsable. La moyenne de l’échantillon tendra vers la moyenne de la population seulement lorsque \\(n \\longrightarrow \\infty\\). Réalisons un second échantillonnage fictif. mean(rnorm(9, mean = 8, sd = 2)) # [1] 8.660309 Cette fois-ci, nous obtenons une moyenne de 8,7. Nous savons que la moyenne \\(\\mu\\) qui nous intéresse est très probablement différente de la moyenne de notre échantillon, mais de conbien ? Pour le déterminer, nous devons définir comment la moyenne de l’échantillon varie d’un échantillon à l’autre, c’est ce qu’on appelle la distribution d’échantillonnage. Nous pouvons le déterminer expérimentalement en échantillonnant un grand nombre de fois. On appelle cela une méta-expérience. En pratique, c’est difficile à faire, mais avec notre ordinateur et le générateur de nombres pseudo-aléatoires de R, pas de problèmes. Donc, comment se distribue la moyenne entre, … disons dix mille échantillons différents de neufs individus tirés de la même population44 ? means_n9 &lt;- numeric(10000) # Vecteur de 10000 valeurs for (i in 1:10000) means_n9[i] &lt;- mean(rnorm(9, mean = 8, sd = 2)) chart(data = NULL, ~ means_n9) + geom_histogram(bins = 30) Nous obtenons une distribution symétrique centrée autour de 8. Elle ressemble à une distribution normale, mais ce n’en est pas une. C’est précisément ici que William Gosset intervient. Il est, en effet, arrivé à décrire cette loi de distribution de la moyenne d’échantillonnage. C’est la distribution t de Student qui admet trois paramètres : une moyenne \\(\\mu_x\\), un écart type \\(\\sigma_x\\), et des degrés de liberté ddl ou \\(\\nu\\). Les degrés de liberté sont en lien avec la taille de l’échantillon. Ils valent : \\[ddl = n-1\\] Concernant la moyenne, et l’écart type, nous pouvons les calculer sur base de notre distribution d’échantillonnage empirique contenue dans le vecteur means : mean(means_n9) # [1] 8.007725 sd(means_n9) # [1] 0.6611474 La moyenne de la distribution d’échantillonnage est donc égale à la moyenne de la population. Elle peut donc être approximée par la moyenne d’un échantillon. Quant à l’écart type, il vaut 2/3 environ, soit l’écart type de la population divisé par 3. Effectuons une autre méta-expérience toujours à partir de la même population, mais avec des échantillons plus petits, par exemple, avec \\(n = 4\\) : means_n4 &lt;- numeric(10000) # Vecteur de 10000 valeurs for (i in 1:10000) means_n4[i] &lt;- mean(rnorm(4, mean = 8, sd = 2)) chart(data = NULL, ~ means_n4) + geom_histogram(bins = 30) La distribution est plus étalée. Ses paramètres sont : mean(means_n4) # [1] 7.995668 sd(means_n4) # [1] 1.002102 La moyenne vaut toujours 8, mais cette fois-ci, l’écart type est plus grand, et il vaut 1, soit 2/2. Qu’est-ce que cela donne avec un échantillon nettement plus grand, disons \\(n = 100\\) ? means_n100 &lt;- numeric(10000) # Vecteur de 10000 valeurs for (i in 1:10000) means_n100[i] &lt;- mean(rnorm(100, mean = 8, sd = 2)) chart(data = NULL, ~ means_n100) + geom_histogram(bins = 30) mean(means_n100) # [1] 7.999136 sd(means_n100) # [1] 0.2005426 On obtient toujours 8 comme moyenne, mais cette fois-ci, l’écart type est de 0,2, soit 2/10. Pouvez-vous deviner comment l’écart type de la distribution t de Student varie sur base de ces trois méta-expériences ? Réfléchissez un petit peu avant de lire la suite. La première bonne nouvelle, c’est que la moyenne des moyennes des échantillons vaut \\(\\mu_x = \\mu\\), la moyenne de la population que nous recherchons. La seconde bonne nouvelle, c’est que la distribution des moyennes des échantillons est plus resserrée que la distribution d’origine. En fait, son écart type dépend à la fois de l’écart type de la population de départ et de \\(n\\), la taille de l’échantillon. Elle varie, en fait, comme \\(\\sigma_x = \\frac{\\sigma}{\\sqrt{n}}\\). Ainsi, avec \\(n = 9\\) nous obtenions \\(\\sigma_x = \\frac{2}{\\sqrt{9}} = \\frac{2}{3}\\) ; avec \\(n = 4\\), nous avions \\(\\sigma_x = \\frac{2}{\\sqrt{4}} = \\frac{2}{2}\\) ; enfin, avec \\(n = 100\\), nous observions \\(\\sigma_x = \\frac{2}{\\sqrt{100}} = \\frac{2}{10}\\). 9.2.1 Loi de distribution de Student On dira : \\[\\mu_x \\sim t(\\mu, \\frac{\\sigma}{\\sqrt{n}}, n-1)\\] La moyenne de l’échantillon suit une distribution t de Student avec pour moyenne, la moyenne de la population, pour écart type, l’écart type de la population divisé par la racine carrée de n, et comme degrés de liberté n moins un. La distribution t de Student dans R est représentée par &lt;x&gt;t(). Donc, qt() calcule un quantile à partir d’une probabilité, pt() une probabilité à partir d’un quantile, rt() renvoie un ou plusieurs nombres pseudo-aléatoires selon une distribution t, et dt() renvoie la densité de probabilité de la distribution. Dans la SciViews Box, vous y accédez également via les “snippets” à partie de .it pour (d)istribution: t (Student) : Le calcul est un peu plus complexe car les fonctions &lt;x&gt;t() ne considèrent que les distributions t de Student réduites (donc avec moyenne valant zéro et écart type de un). Nous devons ruser pour transformer le résultat en fonction des valeurs désirées. Mais heureusement, les “snippets” nous aident en nous prémâchant la besogne. Considérons le cas \\(n = 9\\) avec un moyenne de 8 et un écart type de 2/3. Voici quelques exemples de calculs réalisables : Quelle est la probabilité que la moyenne d’un échantillon soit égale ou supérieure à 8,5 ? .mu &lt;- 8; .s &lt;- 2/3; pt((8.5 - .mu)/.s, df = 8, lower.tail = FALSE) # [1] 0.2373656 Elle est de 24% environ. Notez que nous avons renseigné la moyenne et l’écart type de la distribution t dans .mu et .s, respectivement. Ensuite, les degrés de liberté (9 - 1) sont indiqués dans l’argument df =. Enfin, nous avons précisé lower.tail = FALSE pour obtenir l’aire à droite dans la distribution. Considérant une aire à gauche de 5%, quelle est la moyenne de l’échantillon qui la délimite ? .mu &lt;- 8; .s &lt;- 2/3; .mu + .s * qt(0.05, df = 8, lower.tail = TRUE) # [1] 6.760301 Il s’agit du quantile 6,76. Le graphique correspondant est le suivant : Figure 9.1: Une distribution de Student avec aire à gauche de 5% mise en évidence en rouge. La distribution normale équivalente est superposée en bleu clair. Nous pouvons voir sur la Fig. 9.1 que la distribution t de Student est plus resserrée en son centre, mais plus étalée aux extrémités que la distribution normale de même moyenne et écart type. Néanmoins, elle est d’autant plus proche d’une normale que les degrés de libertés sont grands. On dit qu’elle converge vers une normale lorsque \\(dll \\longrightarrow \\infty\\). En pratique, pour des degrés de liberté égaux ou supérieurs à 30, nous pourrons considérer que les deux distributions sont pratiquement confondues. Revenons à nos calculs de quantiles et probabilités. Les questions que l’on se posera seront plutôt : Quelle est la probabilité que la moyenne d’un échantillon diffère de 0,5 unités de la vraie valeur ? Au lieu de considérer l’aire à gauche ou à droite, on considèrera une aire répartie symétriquement à moitié à gauche et à moitié à droite. La réponse à la question est : # Aire à gauche de 8 -0.5 : .mu &lt;- 8; .s &lt;- 2/3 (left_area &lt;- pt((7.5 - .mu)/.s, df = 8, lower.tail = TRUE)) # [1] 0.2373656 # Aire à droite de 8 + 0.5 : (right_area &lt;- pt((8.5 - .mu)/.s, df = 8, lower.tail = FALSE)) # [1] 0.2373656 # Résultat final left_area + right_area # [1] 0.4747312 Vous avez remarqué quelque chose de particulier ? Oui, les deux aires sont identiques. C’est parce que la distribution est symétrique. On peut donc simplifier le calcul en calculant d’un seul côté et en multipliant le résultat par deux : .mu &lt;- 8; .s &lt;- 2/3 pt((7.5 - .mu)/.s, df = 8, lower.tail = TRUE) * 2 # [1] 0.4747312 Dans l’autre sens, il suffit donc de diviser la probabilité (= l’aire) par deux, parce qu’elle se répartit à parts égales à gauche et à droite dans les régions les plus extrêmes de la distribution. Ainsi, les quantiles qui définissent une aire extrême de 5% dans notre distribution sont (notez que la valeur de probabilité utilisée ici est 0,025, soit 2,5%) : # Quantile à gauche .mu &lt;- 8; .s &lt;- 2/3; .mu + .s * qt(0.025, df = 8, lower.tail = TRUE) # [1] 6.462664 # Quantile à droite .mu &lt;- 8; .s &lt;- 2/3; .mu + .s * qt(0.025, df = 8, lower.tail = FALSE) # [1] 9.537336 On pourra aussi dire que la moyenne d’un échantillon de neuf observations issu de notre population théorique de référence sera comprise entre 6,5 et 9,5 (ou 8 ± 1,5) dans 95% des cas. La Fig. 9.2 le montre graphiquement. Figure 9.2: Une distribution de Student avec aire extrême de 5% mise en évidence en rouge. 9.2.2 Intervalle de confiance Le dernier exemple que nous venons de calculer (Fig. 9.2) n’est rien d’autre que l’intervalle de confiance à 95% de la moyenne. Un intervalle de confiance à x% autour d’une valeur estimée définit une zone à gauche et à droite de la valeur estimée telle que la vraie valeur se situe x% du temps dans cet intervalle. En fait, la distribution est centrée sur \\(\\mu\\), la valeur inconnue que l’on recherche, mais l’intervalle peut être translaté sur l’axe pour se centrer sur la moyenne \\(\\bar{x}\\) d’un échantillon en particulier. Il définit alors une région sur l’axe qui comprend avec une probabilité correspondante, \\(\\mu\\) la moyenne inconnue. Avec ce nouvel outil, nous pouvons donc préciser nos estimations de la moyenne de la population \\(\\mu\\) en associant à la valeur estimée via la moyenne de l’échantillon \\(\\bar{x}\\) un intervalle de confiance. Si nous notons \\(t_p^{n-1}\\) le quantile correspondant à l’aire à gauche valant p pour une distribution t réduite de \\(n-1\\) degrés de liberté, on pourra écrire : \\[\\mathrm{IC}(1 - \\alpha) = \\mu_x \\pm t_{\\alpha/2}^{n-1} \\cdot \\sigma_x\\] On notera aussi \\(\\hat{\\mu}\\) ou “mu chapeau” comme l’estimateur de \\(\\mu\\), c’est-à-dire, la valeur que nous utilisons pour l’approximer au mieux. Ici, il s’agit de \\(\\bar{x}\\), la moyenne de notre échantillon. De même, \\(\\hat{\\sigma}\\) est l’estimateur de l’écart type de la population. La valeur que nous avons à disposition est \\(s_x\\), l’écart type de notre échantillon. Nous pourrons aussi écrire : \\[\\mathrm{IC}(1 - \\alpha) \\simeq \\hat{\\mu} \\pm t_{\\alpha/2}^{n-1} \\cdot \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\] … et en remplaçant les estimateurs : \\[\\mathrm{IC}(1 - \\alpha) \\simeq \\bar{x} \\pm t_{\\alpha/2}^{n-1} \\cdot \\frac{s_x}{\\sqrt{n}}\\] Etant donné l’importance que revet \\(\\frac{s_x}{\\sqrt{n}}\\), nous appelerons cette quantité erreur standard de x et nous la noterons \\(SE_x\\). Nous pouvons tout aussi bien écrire plus simplement : \\[\\mathrm{IC}(1 - \\alpha) \\simeq \\bar{x} \\pm t_{\\alpha/2}^{n-1} \\cdot SE_x\\] Ce qui est intéressant avec ces deux dernières formulations, c’est que l’IC est calculable sur base de notre échantillon uniquement. Analogie avec l’homme invisible qui promène son chien. Si vous avez des difficultés à comprendre l’IC, imaginez plutôt que vous recherchez l’homme invisible (c’est \\(\\mu\\)). Vous ne savez pas où il est, mais vous savez qu’il promène son chien en laisse. Or, le chien est visible (c’est \\(\\bar{x}\\) la moyenne de l’échantillon). La laisse est également invisible, mais vous connaissez sa longueur maximale (c’est votre IC). Donc, vous pouvez dire, voyant le chien que l’homme invisible est à distance maximale d’une longueur de laisse du chien. Valeur α Quel est l’impact du choix de \\(\\alpha\\) sur le calcul de l’IC ? Plus \\(\\alpha\\) sera petit, plus le risque de se tromper sera faible. Cela peut paraître intéressant, donc, de réduire \\(\\alpha\\) le plus possible. Mais alors, la longueur de l’IC augmente. Si nous poussons à l’extrême, pour \\(\\alpha\\) = 0%, nous aurons toujours un IC compris entre \\(-\\infty\\) et \\(+\\infty\\). Et cela, nous en sommes certains à 100% ! Trivial, non? Et pas très utile. Comme pour tout en statistique, nous devons accepter un certain risque de nous tromper si nous voulons obtenir des résultats utilisables. Plus ce risque est grand, plus la réponse est précise (ici, plus l’IC sera petit, voir Fig. 9.3), mais plus le risque de se tromper augmente. On cherchera alors un compromis qui se matérialise souvent par le choix de \\(\\alpha\\) = 5%. Nous nous tromperons une fois sur vingt, et nous aurons un IC généralement raisonnable pour ce prix. Naturellement, rien ne vous oblige à utiliser 5%. Vous pouvez aussi choisir 1% ou 0,1% si vous voulez limiter les risques. Figure 9.3: Une distribution de Student avec comparaison de l’IC 95% (entre les aires en rouge) et l’IC 90% (entre les aires en orange). 9.2.3 Théorème central limite (encore) Jusqu’ici, nous avons considéré une population au départ qui a une distribution normale, mais rien ne dit que ce soit le cas. Que se passe-t-il lorsque la distribution est différentes ? Ici encore, nous pouvons effectuer une méta-expérience. Considérons, par exemple, une distribution uniforme de même moyenne = 8 et écart type = 2. Sachant que l’écart type d’une distribution uniforme vaut \\(\\frac{max - min}{\\sqrt{12}}\\), voir ici, l’intervalle est de : \\(2 \\cdot \\sqrt{12} = 6,928\\). Nous avons donc : (xmin &lt;- 8 - sqrt(12)) # [1] 4.535898 (xmax &lt;- 8 + sqrt(12)) # [1] 11.4641 Vérification : sd(runif(10000, min = xmin, max = xmax)) # [1] 1.986923 Quelle est la distribution de la moyenne d’échantillonnage lorsque \\(n\\) = 4 ? set.seed(678336) m_unif_n4 &lt;- numeric(10000) # Vecteur de 10000 valeurs for (i in 1:10000) m_unif_n4[i] &lt;- mean(runif(4, min = xmin, max = xmax)) # Distribution de Student correspondante pour comparaison .mu &lt;- 8; .s &lt;- 2/2; .df &lt;- 3 # .mu, .s (sigma) and .df .x &lt;- seq(-4.5*.s + .mu, 4.5*.s + .mu, l = 1000) # Quantiles .d &lt;- function(x) dt((x - .mu)/.s, df = .df)/.s # Distribution function chart(data = NULL, ~ m_unif_n4) + geom_histogram(bins = 30) + geom_line(aes(x = .x, y = .d(.x) * 3000)) Cette distribution n’est pas une Student. Par contre, elle y ressemble plus qu’à la distribution uniforme de départ. Avec \\(n\\) = 9 elle s’en rapproche très, très fort, et pour \\(n\\) = 100, nous avons une t de Student parfaite. Figure 9.4: Distribution d’échantillonnage à partir d’une distribution uniforme, n = 9. Ajustement d’une distribution de Student équivalente par dessus l’histogramme. Figure 9.5: Une distribution de Student avec comparaison de l’IC 95% (entre les aires en rouge) et l’IC 90% (entre les aires en orange). Nous venons de montrer de manière empirique que lorsque la distribution de la population est différente d’une distribution normale, la distribution d’échantillonnage tend vers une t de Student pour un \\(n\\) grand. Ceci se démontre de manière mathématique par le fameux théorème central limite que nous avons déjà abordé et qui est si cher aux statisticiens (nous vous épargnons cette démonstration ici). Conditions de validité de l’IC L’IC sera pertinent si : l’échantillon est représentatif (par exemple, échantillonnage aléatoire), les observations au sein de l’échantillon sont indépendantes les unes des autres, la distribution de la population… est normale, alors l’IC basé sur la distribution t de Student sera exact, est approximativement normale, l’IC sera approximativement exact, est non normale, l’IC sera approximativement exact si \\(n\\) est grand. L’équation proposée est, en fait, valable pour un échantillon, et est calculé comme tel par R à l’aide des fonctions sd() pour l’écart type ou var() pour la variance. Pour la population ou pour un échantillon de taille très grande, voire infinie, nous pourrions plutôt diviser par \\(n\\) au lieu de \\(n - 1\\), … mais puisque \\(n\\) est très grand, cela ne change pas grand chose au final.↩ Notez que les lettres latines sont utilisées pour se référer aux variables et aux descripteurs statistiques telle que la moyenne pour l’échantillon, alors que les paramètres équivalents de la population, qui sont inconnus, sont représentés par des lettres grecques en statistiques.↩ Nous utilisons pour se faire une boucle for dans R qui réitère un calcul sur chaque élément d’un vecteur, ici, une séquence 1, 2, 3, …, 10000 obtenue à l’aide de l’instruction 1:10000.↩ "],
["test-de-student.html", "9.3 Test de Student", " 9.3 Test de Student Nous allons également pouvoir utiliser la distribution t de Student comme distribution de référence pour comparer une moyenne par rapport à une valeur cible ou pour comparer deux moyennes. C’est le test t de Student… ou plutôt les tests de Student puisqu’il en existe plusieurs variantes. Partons d’un exemple concret. Imaginez que vous êtes des biologistes ouest-australiens travaillant à Freemantle. Vous y étudiez le crabe Leptographus variegatus (Fabricius, 1793). C’est un crabe qui peut se trouver en populations abondantes sur les côtes rocheuses fortement battues. Il a un régime alimentaire partiellement détritivore et partiellement carnivore. Crabe Leptograpsus variegatus par Johnragla. Ce crabe est rapide et difficile à capturer… mais vous avez quand même réussi à en attraper et mesurer 200 d’entre eux, ce qui constitue un échantillon de taille raisonnable. Comme deux variétés co-existent, la variété bleue (B) et la variété orange (O) sur votre site d’étude, vous vous demandez si elles diffèrent d’un point de vue morphométrique. Naturellement, nous pouvons également supposer des différences entre mâles et femelles. Vous avez donc décidé de réaliser un échantillonnage stratifié consistant à capturer et mesurer autant de bleus que d’oranges et autant de mâles que de femelles. Vous avez donc 50 mâles bleus, 50 femelles bleues, 50 mâles oranges et 50 femelles oranges. crabs &lt;- read(&quot;crabs&quot;, package = &quot;MASS&quot;, lang = &quot;fr&quot;) skimr::skim(crabs) # Skim summary statistics # n obs: 200 # n variables: 8 # # ── Variable type:factor ───────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # sex 0 200 200 2 F: 100, M: 100, NA: 0 FALSE # species 0 200 200 2 B: 100, O: 100, NA: 0 FALSE # # ── Variable type:integer ──────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # index 0 200 200 25.5 14.47 1 13 25.5 38 50 ▇▇▇▇▇▇▇▇ # # ── Variable type:numeric ──────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # depth 0 200 200 14.03 3.42 6.1 11.4 13.9 16.6 21.6 # front 0 200 200 15.58 3.5 7.2 12.9 15.55 18.05 23.1 # length 0 200 200 32.11 7.12 14.7 27.28 32.1 37.23 47.6 # rear 0 200 200 12.74 2.57 6.5 11 12.8 14.3 20.2 # width 0 200 200 36.41 7.87 17.1 31.5 36.8 42 54.6 # hist # ▂▃▆▇▇▆▅▂ # ▂▃▇▇▇▇▅▃ # ▁▃▅▇▇▆▅▂ # ▂▃▇▆▇▃▂▁ # ▁▃▅▇▇▆▃▁ Toutes les variables qualtitatives sont des mesures effectuées sur la carapace des crabes. Nous nous posons la question suivante : Les femelles ont-elle une carapace plus large à l’arrière, en moyenne que les mâles ? Voici une comparaison graphique : chart(data = crabs, rear ~ sex) + geom_boxplot() Sur le graphique, il semble que les femelles (sex == &quot;F&quot;) tendent à avoir une carapace plus large à l’arrière -variable rear- que les mâles (sex == &quot;M&quot;), mais cette différence est-elle significative ou peut-elle être juste liée au hasard de l’échantillonnage ? Pour y répondre, nous devons élaborer un test d’hypothèse qui va confronter les hypothèses suivantes (en se basant sur les moyennes) : \\(H_0: \\overline{rear_F} = \\overline{rear_M}\\) \\(H_1: \\overline{rear_F} \\neq \\overline{rear_M}\\) Ici, nous n’avons aucune idée a priori pour \\(H_1\\) si les femelles sont sensées avoir une carapace plus large ou non que les mâles à l’arrière. Donc, nous considérons qu’elle peut être aussi bien plus grande que plus petite. On parle ici de test bilatéral car la différence peut apparaître des deux côtés. Pour ce test, nous pouvons partir de la notion d’intervalle de confiance et de notre idée de calculer les quantiles de part et d’autre de la distribution théorique à parts égales, comme dans la Fig. 9.2. Une idée serait de calculer \\(\\overline{rear_F} - \\overline{rear_M}\\), la différence des moyennes entre mesures pour les femelles et pour les mâles. Les hypothèses deviennent alors : \\(H_0: \\overline{rear_F} - \\overline{rear_M} = 0\\) \\(H_1: \\overline{rear_F} - \\overline{rear_M} \\neq 0\\) Appelons cette différence \\(\\Delta rear\\). Nous pouvons définir un intervalle de confiance pour \\(\\Delta rear\\) si nous pouvons calculer la valeur t ainsi que l’erreur standard \\(SE_{\\Delta rear}\\) associées à cette variables calculée. Après avoir interrogé des statisticiens chevronnés, ceux-ci nous proposent l’équation suivante pour \\(SE_{\\Delta rear}\\) (avec \\(n_F\\) le nombre de femelles et \\(n_M\\) le nombre de mâles) : \\[SE_{\\Delta rear} = \\sqrt{SE_{rear_F}^2 + SE_{rear_M}^2} = \\sqrt{\\frac{s_{rear_F}^2}{n_F} + \\frac{s_{rear_M}^2}{n_M}}\\] Il nous reste à déterminer les degrés de liberté associés à la distribution t. Les statisticiens nous disent qu’il s’agit de n moins deux degrés de libertés. Nous obtenons alors l’équation suivante pour l’intervalle de confiance : \\[\\mathrm{IC}(1 - \\alpha)_{\\Delta rear} \\simeq \\Delta rear \\pm t_{\\alpha/2}^{n-2} \\cdot SE_{\\Delta rear}\\] Dans notre cas, cela donne : crabs %&gt;.% group_by(., sex) %&gt;.% summarise(., mean = mean(rear), var = var(rear), n = n()) -&gt; crabs_stats crabs_stats # # A tibble: 2 x 4 # sex mean var n # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; # 1 F 13.5 7.51 100 # 2 M 12.0 4.67 100 # Calcul de Delta rear et de son intervalle de confiance à 95% (delta_rear &lt;- crabs_stats$mean[1] - crabs_stats$mean[2]) # [1] 1.497 (t &lt;- qt(0.025, nrow(crabs) - 2)) # [1] -1.972017 (se &lt;- sqrt(crabs_stats$var[1] / crabs_stats$n[1] + crabs_stats$var[2] / crabs_stats$n[2])) # [1] 0.3489874 (ic_95 &lt;- c(delta_rear + t * se, delta_rear - t * se)) # [1] 0.8087907 2.1852093 Un premier raisonnement consiste à dire que si la valeur attendue sous \\(H_0\\) est comprise dans l’intervalle de confiance, nous ne pouvons pas rejetter l’hypothèse nulle, puisqu’elle représente une des valeurs plausibles à l’intérieur l’IC. Dans le cas présent, l’intervalle de confiance à 95% sur \\(\\Delta rear\\) va de 0.81 à 2.19. Il ne contient donc pas zéro. Dans, nous pouvons rejetter \\(H_0\\) au seuil \\(\\alpha\\) de 5%. Nous pouvons effectivement interpréter le test de cette façon, mais le test t de Student se définit de manière plus classique en comparant la valeur \\(t_{obs}\\) à la distribution théorique, et en renvoyant une valeur P associée au test. Ainsi, le lecteur peut interpréter les résultats avec son propre seuil \\(\\alpha\\) éventuellement différent de celui choisi par l’auteur de l’analyse. Le raisonnement est le suivant. Sous \\(H_0\\), la distribution de \\(\\Delta rear\\) est connue. Elle suit une distribution t de Student de moyenne égale à la vraie valeur de la différence des moyennes, d’écart type égal à l’erreur standard sur cette différence, et avec \\(n - 2\\) degrés de liberté. En pratique, nous remplaçons les valeurs de la population pour la différence des moyennes et pour les erreurs standard par celles estimées par l’intermédiaire de l’échantillon. Comme dans le cas du test \\(\\chi^2\\), nous définissons les zones de rejet et de non rejet par rapport à cette distribution théorique. Dans le cas du test de Student bilatéral, l’aire \\(\\alpha\\) est répartie à moitié à gauche et à moitié à droite (Fig. ??). Nous pouvons calculer la valeur P nous-même comme ceci, sachant la valeur de \\(t_{obs} = \\frac{\\Delta rear}{SE_{\\Delta rear}}\\) parce que nous travaillons avec une distribution t réduite : (t_obs &lt;- delta_rear / se) # [1] 4.289553 (p_value &lt;- pt(t_obs, df = 198, lower.tail = FALSE) * 2) # [1] 2.797369e-05 Ne pas oublier de multiplier la probabilité obtenue par deux, car nous avons un test bilatéral qui considère une probabilité égale à gauche et à droite de la distribution ! Naturellement, R propose une fonction toute faite pour réaliser ce test afin que nous ne devions pas détailler les calculs à chaque fois. Il s’agit de la fonction t.test(). Dans la SciViews Box, le snippet équivalent est accessible depuis .hm pour hypothesis tests: means. Dans le menu qui apparaitn, vous choisissez independant Student's t-test. Les arguments de la fonction sont les suivants. Le jeu de données dans data =, une formule qui reprend le nom de la variable quantitative à gauche (rear) et celui de la variable qualitative à deux niveaux à droite (sex), l’idication du type d’hypothèse alternative, ici alternative = &quot;two-sided&quot; pour un test bilétéral, le niveau de confiance égal à \\(1 - \\alpha\\), donc conf.level = 0.95 et enfin si nous considérons les variances comme égales pour les deux sous-populations var.equal = TRUE. t.test(data = crabs, rear ~ sex, alternative = &quot;two.sided&quot;, conf.level = 0.95, var.equal = TRUE) # # Two Sample t-test # # data: rear by sex # t = 4.2896, df = 198, p-value = 2.797e-05 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # 0.8087907 2.1852093 # sample estimates: # mean in group F mean in group M # 13.487 11.990 Nous retrouvons exactement toutes les valeurs que nous avons calculées à la main. Dans le cas présent, rappelez-vous la façon d’interpréter le test. Nous comparons la valeur P à \\(\\alpha\\). Si elle est plus petit, nous rejettons \\(h_0\\), sinon, nous ne la rejettons pas. Ici, nous rejettons \\(H_0\\) et pourrons dire que la largeur à l’arrière de la carapace de L. variegatus diffère de manière significative entre les mâles et les femelles au seuil \\(\\alpha\\) de 5% (test t bilatéral, t = 4,29, ddl = 198, valeur P &lt;&lt; 10-3). Petite astuce… les mesures morphométriques sont dépendantes de la taille globale de l’animal qui varie d’un individu à l’autre, il vaut donc mieux étudier des rapports de tailles plutôt que des mesures absolues. Refaites le calcul sur base du ratio rear / length comme exercice et déterminer si la différence est plus ou moins nette entre les mâles et les femelles que dans le cas de rear seul. Pour en savoir plus Une vidéo en anglais qui explique le test t de Student un peu différemment. "],
["representation-graphique.html", "9.4 Représentation graphique", " 9.4 Représentation graphique Présentation graphique: dynamite plot + barres d’erreurs. Transformation des données pour linéariser et ou rendre symétrique autour de la moyenne. Comparaison moyenne/médiane =&gt; paramétrique versus non paramétrique. Le système judiciaire des statistiques, par Hadley Wickham. Pour terminer, bien que la moyenne est un descripteur statistique très utile, il est parfois utilisé de manière abusive. Une distribution statistique ne se résume pas à un nombre, fût-ce la moyenne. De plus, si la distribution est asymétrique, la moyenne est un mauvais choix (préférer alors la médiane, ou transformer les données pour rendre la distribution plus symétrique). La vidéo suivante détaille le problème qui peut se produire : "],
["variance.html", "Module 10 Variance", " Module 10 Variance Comparaison de deux populations (suite): Wilcoxon-Mann-Withney + comparaison au t-test. Variance, ANOVAs, test de Bartlett. Graphiques associés. Petite recherche biblio concernant l’application en pratique de ces tests à faire par les étudiants. "],
["correlation.html", "Module 11 Corrélation", " Module 11 Corrélation Suite ANOVA (ANOVA à deux facteurs) + correlation + graphes et tests. Restitution participation à l’élaboration du bookdown commun. "],
["design.html", "Module 12 Design expérimental &amp; critique statistique", " Module 12 Design expérimental &amp; critique statistique Design de l’expérience, choix du nombre de réplicas et puissance d’un test. Critique stat + “bad graphs” + pseudo-réplication. “Challenges” sur base de la critique statistique. Débriefing général. Savoir communiquer ses résultats est vital en science des données. Ce n’est pas si facile car il faut pouvoir simplifier les analyses et utiliser au mieux les visuels (c’est-à-dire, les graphiques) pour raconter une histoire qui soit à la fois captivante et compréhensible. Communiquer le fruit de ses recherches de la meilleure façon qui soit pour que les non-initiés puissent le comprendre fait partie du bagage indispensable du scientifique des données. Hans Rosling est sans nuls doute très doué pour communiquer des résultats statistiques. La vidéo suivante est un peu longue (20min) et en anglais, mais elle en vaut vraiment la peine45. De plus, il explique à quel point il est important de partager et de rassembler les données dans des grandes bases de données, et ensuite d’en tirer des études utiles pour l’humanité. C’est l’avenir des sciences des données, y compris en biologie, qu’il est en train de prédire là. Vous pouvez activer les sous-titres en anglais via la barre de boutons en bas de la vidéo.↩ "],
["graphiques-a-faire-et-ne-pas-faire.html", "12.1 Graphiques : à faire et ne pas faire", " 12.1 Graphiques : à faire et ne pas faire "],
["svbox.html", "A Installation de la SciViews Box", " A Installation de la SciViews Box La SciViews Box est une machine virtuelle (c’est-à-dire, l’équivalent d’un ordinateur complet, mais “dématérialisé” et utilisable à l’intérieur de n’importe quel autre ordinateur physique). Elle est spécialement configurée pour analyser des données et rédiger des documents scientifiques de manière professionnelle. Dans notre cas, le logiciel de gestion de la machine virtuelle, l’hyperviseur, est VirtualBox. C’est un logiciel gratuit qui existe pour Windows, MacOS et la plupart des systèmes Linux. L’avantage d’utiliser une machine virtuelle dans le contexte qui nous concerne ici est double : Elle est complètement pré-configurée et pré-testée. Comme tout le monde utilise la même machine virtuelle, les résultats obtenus chez l’un sont parfaitement reproductibles chez d’autres. L’installation est simple, mais il y a quand même quelques pièges. Suivez le guide… "],
["prerequis-8.html", "A.1 Prérequis", " A.1 Prérequis Avant d’installer la SciViews Box 2018, vérifiez que votre ordinateur répond aux conditions requises et qu’il est correctement configuré. A.1.1 Ordinateur La SciViews Box 2018, et la Science des Données en général, nécessitent un ordinateur ayant une puissance de calcul suffisante. Les tablettes et autres chromebooks sont donc exclus (sauf à être utilisés comme simples browsers web avec les calculs déportés sur un serveur, voir par exemple Chromebook Data Science). Si l’utilisation d’un serveur est une bonne idée pour l’apprentissage, ce n’est pas une solution sur le long terme pour tout le monde. En effet, vous êtes et restez dépendant du serveur que l’on a bien voulu configurer et partager avec vous (sera-t-il encore disponible après votre cours, par exemple ?). La solution proposée avec la SciViews Box vous rend complètement autonome dès le départ. Le choix d’un ordinateur ayant une capacité de calcul suffisante n’est pas aisé et les disparités en matière de performances sont énormes, voir NovaBench CPU score. La configuration de référence est la suivante, avec un score global Novabench d’environ 1000 : Processeur : à 2 ou 4 coeurs / 4 threads d’une vitesse de calcul suffisante (score CPU Novabench d’environ 500). Mémoire vive : 8Go avec un score RAM Novabench d’environ 200. Disque dur : disque rapide SSD de 256Go (score disque Novabench d’environ 75 avec vitesse d’écriture &gt;= 300Mo/s et vitesse de lecture &gt;= 400Mo/s). Affichage : 1920x1080 ou mieux. La plupart des cartes graphiques ou des coprocesseurs graphiques intégrés conviennent (pas besoin d’une bête de course si vos calculs ne nécessitent pas des instructions GPU, en tous cas). Comme base, nous considèrerons un score GPU NovaBench d’environ 200 qui correspond au processeur graphique intégré Intel HD 620. Réseau : Wifi à la norme 802.11ac. Connectique : USB 3.0 ou C pour ajouter des périphériques, HDMI ou DisplayPort pour connecter un écran externe, et une prise casque pour visionner des vidéos sans déranger les voisins. Portabilité et autonomie : 13 pouces pour un poids &lt; 1,5kg pour un ordinateur à enmener partout, sinon 15 pouces et un poids &lt;= 2kg. Autonomie d’au moins 5 à 6h. Système d’exploitation : récent et si possible 64-bit. Windows 7 ou plus convient (mais pensez à mettre-à-jour vers Windows 10), MacOS 10.10 Yosemite ou plus, ou un Linux tel Debian 8 (Jessie) ou 9 (Stretch), Ubuntu 16.04 Xenial ou supérieur, … Une configuration “standard” avec au moins 20Go de libre pour la SciViews Box (nécessairement sur le disque C: sous Windows), et une configuration non bidouillée (répertoire utilisateur et programmes standards, entre autres). Pour une configuration de base, vous pouvez aller jusqu’à diviser les scores Novabench et les valeurs (nombre de coeurs CPU, taille de la mémoire vive et du disque) par deux, et pour une configuration performante, multipliez-les par deux, et ajoutez-y éventuellement une carte graphique Nvidia performante pour des calculs GPU et un second disque de 1To pour stocker des gros jeux de données. Un “laptop” (ordinateur portable) est mieux, mais si vous n’avez pas besoin d’une solution nomade, un “desktop” convient aussi et est plus modulable. Pour tester votre système, nous vous proposons donc d’utiliser le logiciel gratuit pour un usage personnel Novabench. Vous obtiendrez un rapport, voir ci-dessous un résultat pour un PC en configuration de référence (Asus Zenbook UX330U) et un Mac en configuration optimale MacBook Pro 15 pouces mid-2015. Vous pourrez comparer à la configuration de référence : A.1.2 Activation de la virtualisation La virtualisation fait appel à un jeu d’instructions disponible sur pratiquement tous les processeurs modernes (Intel VT-x ou AMD-v). Malheureusement, elle est désactivée par défaut sur quasi tous les PC (mais les Macs sont, eux, configurés correctement en sortie d’usine). Tant que ces instructions de virtualisation ne seront pas activées, le programme d’installation de la SciViews Box va bloquer avec le message suivant: Même si vous arriviez à l’installer quand même, vous ne pourriez pas la démarrer, et verriez juste le message suivant (issu de la version précédente de la SciViews Box): Pour activer ce jeu d’instructions, il faut aller dans le BIOS, c’est-à-dire, le petit programme qui démarre votre ordinateur. Il n’y a malheureusement pas de recette unique car chaque constructeur a sa propre façon de faire. De plus, l’endroit où il faut aller dans les menus de configuration du BIOS diffère aussi d’un ordinateur à l’autre. Cependant, la procédure générale est la suivante: Redémarrer l’ordinateur, Au tout début du démarrage, il faut appuyer sur une touche ou une combinaison de touches (par exemple, DEL, F2, …). Restez à l’affût d’un message furtif qui l’indique à l’écran, Une fois entré dans le BIOS, repérez l’entrée correspondant au jeu d’instructions de virtualisation. Vous aurez plus de chances en regardant dans le menu relatif au processeur, ou dans les options avancées. Recherchez une entrée de type “Virtualisation”, “Intel Virtual Technology”, ou “Instructions AMD-v”. Activez cette option (cela n’aura aucun effet sur les logiciels que vous avez installés jusqu’ici et qui n’utilisent pas cette fonction), Sortez du BIOS en sauvegardant les modifications (suivez les instructions à l’écran), Redémarrez l’ordinateur. Si vous n’arrivez pas à entrer dans le BIOS, ou à trouver l’entrée correspondante dans celui-ci, rechercher “BIOS Virtualization” accompagné de la marque et du modèle de votre ordinateur dans votre moteur de recherche internet favori. Vous y trouverez certainement des instructions plus précises relatives à votre ordinateur. Ce site liste quelques uns de raccourcis claviers à utiliser en fonction de la marque des ordinateurs pour entrer dans le BIOS. Dans le cas où vous n’arrivez pas à activer la virtualisation sur votre PC, vous pouvez toujours installer une version 32-bit de la SciViews Box en mode d’émulation logicielle de VirtualBox. Dans ce cas, votre box tournera plus lentement et vous n’aurez pas la possibilité d’utiliser plus d’un seul coeur processeur, mais au moins, vous pourrez quand même l’utiliser. La version 64-bit “complète” se nomme svbox2018a. La version 32-bit est svbox2018b. Si vous optez pour cette dernière, adaptez l’intitulé de la machine ou des fichiers (a-&gt; b) dans la suite de ce tutoriel. Si votre ordinateur est conforme aux spécifications ci-dessus, et si la virtualisation est activée, vous êtes maintenant prêt à installer votre SciViews Box! Dans ce cas, passez directement à la section A.2. Si vous n’avez pas encore d’ordinateur et souhaitez en acheter un, voyez quelques conseils utiles ci-dessous pour faire le bon choix. A.1.3 Conseils pour acheter un PC Pour les étudiants de l’UMONS, vous ne devez pas posséder votre propre ordinateur pour suivre le cours de science des données biologiques. Des ordinateurs en configuration de référence (voir ordinateur) vous sont accessibles au cours et aux travaux pratiques, et en dehors des heures à la salle “Escher” (demander un accès au secrétariat des sciences). Toutefois, si vous pouvez acquérir un ordinateur personnel, cela vous apportera un confort et une flexibilité indéniable, à condition de bien le choisir ! Si vous suivez les directives ci-dessus, vous ne pourrez pas vous tromper. Mais comme vous n’aurez probablement pas la possibilité de tester les ordinateurs avec Novabench avant l’achat, voici quelques exemples de configurations types et leur coûts approximatifs. Attention : cette analyse est réalisée en octobre 2018. Le matériel informatique et les prix changent constamment, et les informations seront rapidement obsolètes. Les lignes directrices devraient, cependant rester valables à l’avenir pour utiliser la SciViews Box 2018. L’élément le plus important étant la vitesse de calcul du processeur, obtenez la référence du processeur de l’ordinateur que vous convoitez et recherchez-le ici. Comparer alors son score à notre configuration de référence, et faites-en de même pour les autres caractéristiques (mémoire vive, type et taille du disque dur, etc.) Prenez aussi comme référence les ordinateurs de votre salle de travaux pratiques. A l’UMONS, dans la salle “Pentagone”, les ordinateurs sont équipés d’Intel Core i5-6400T @ 2.20Ghz. Ce sont des processeurs 4 coeurs/4 threads avec un score CPU Novabench de 511. Ils possèdent également 8Go de mémoire vive, un disque dur SDD avec 190Go dédiés à Windows 10 64-bit, et un processeur graphique intégré qui affiche 1920x1080 pixels à l’écran. Aux salles “Escher” et “Turing”, ce sont des processeurs Intel Core i5-4590 @ 3.30Ghz, également 4 coeurs/4 threads affichant un score CPU Novabench de 515. Le reste de leurs configurations est similaire à celle des ordinateurs “Pentagone”. Vous pouvez également vous baser sur d’autres tests, les CPUMarks et comparer le processeur de l’ordinateur que vous voulez acheter avec les scores de vos machines des salles de travaux pratiques à partir de cette page. Dans le choix de votre ordinateur, il faut tout d’abord vous demander si vous voulez un ordinateur pour apprendre à traiter des données sur des petits tableaux, et que vous prévoyez changer dans 1 ou 2 ans (dans ce cas, une configuration de base convient), ou si vous voulez investir sur plus long terme. Visez alors plus haut. Naturellement, le prix sera un critère fondamental, également46. Justification des besoins : Processeur : l’élément le plus important. Un processeur puissant et multitâche est indispensable. Il vous faut au moins 2 coeurs et 4 threads (selon les modèles, chaque coeur peut gérer une seule tâche -ou “thread” en anglais- ou deux). Un processeur 4 coeurs/4 threads est encore mieux, et à partir de 4 coeurs/8 threads, c’est parfait. Pour la vitesse de calcul, comme indiqué plus haut, un score CPU Novabench de 500 ou mieux, ou un CPUMark de 5000 ou mieux doit être visé pour une configuration de référence ou performante. Avec un score moitié moindre, c’est encore un processeur utilisable, mais ne descendez pas en dessous pour une configuration de base. Mémoire vive : il vous faut suffisamment de mémoire pour la partager entre la machine hôte et la machine virtuelle, et garder assez de resources pour ouvrir des tableaux (moyennement) volumineux. Donc, visez 8Go de mémoire vive si possible. Pas moins de 4Go, et plus vous en avez, mieux c’est. Il existe des configurations laptops à 16Go. C’est utile ! Disque dur : ici, vous devrez peut-être faire un choix entre espace de stockage et vitesse du disque. En effet, les disques mécaniques classiques font maintenant facilement 1To, ce qui est confortable. Par contre, ils sont plus lents que les disques SSD qui sont à privilégier. Mais ces derniers sont de capacité moindre (dans des gammes de prix raisonnables), généralement 128Go ou 256Go. Des configurations plus haut de gamme combinent deux disques : un SSD rapide pour le système et un disque de 1To classique pour les données. C’est l’idéal. Si vous investissez dans un ordinateur ayant un disque dur SSD rapide mais pas assez gros pour contenir vos nombreux fichiers, photos, vidéos, morceaux de musique, etc., vous pourrez toujours compléter votre configuration avec un disque dur de 1To externe USB 3.0 pour une cinquantaine d’euros. Pensez aussi à investir dans une clé USB de 8 ou 16Gb pour transférer vos données. Faites attention de bien choisir un modèle USB 3.0 reconnaissable à son connecteur bleu, infiniment plus rapide qu’un modèle USB 2.0. Il vous en coutera une dizaine d’euros. Carte graphique et écran : la qualité de la carte graphique est moins importante ici. La plupart des configurations actuelles conviennent. Voyez plutôt la taille (et donc, le poids) qui est un critère important pour un ordinateur portable. Voulez-vous un PC de 13 ou 14 pouces plus compact et transportable, ou un 15 à 17 pouces plus confortable, mais plus lourd? Pour la résolution d’écran, ne descendez pas en dessous de 1400x900 pixels pour un travail confortable (RStudio affiche plusieurs fenêtres côte-à-côte), et vérifiez visuellement si la qualité de l’écran vous convient. Wifi et accessoires: une bonne connexion Wifi sera nécessaire pour vous connecter à Internet. La norme WiFi 802.11ac est idéale. Enfin, vérifiez les connexions proposées : USB rapide (3.0, 3.1 ou C), Thunderbold, DisplayPort, HDMI, etc. pour connecter des périphériques et des écrans externes, lecteur de cartes éventuel, etc. D’autres critères comme la qualité de construction, la robustesse, la qualité du clavier et du trackpad éventuel, l’autonomie pour un portable, … sont importants. Pensez à consulter les tests détaillés effectués par des pros avant de vous décider, par exemple, les numériques en français, PCMag ou techradar en anglais. Voici quelques configurations types qui conviennent, volontairement choisies chez différents constructeurs pour ne privilégier personne. Dans le cadre de vos études, vous allez certainement vouloir emporter votre ordinateur avec vous partout. Nous vous présentons donc des ordinateurs portables de moins de 2kg, plus adaptés à cet usage. A.1.3.1 Configurations de base Evitez autant que possible de descendre en dessous de celles-ci. Dites-vous bien que ces machines sont estampillées “bureautique”, et sont trop juste pour analyser des gros jeux de données, mais elles peuvent convenir parfaitement dans le cadre du cours de science des données biologiques. Si vous possédez déjà un PC, faites un bilan avec Novabench et décidez par vous-même si vous pouvez ou non l’utiliser de manière confortable, éventuellement en installant la SciViews Box 2018 et en testant ainsi directement. Les options existent aussi pour “booster” un ordinateur un peu juste : ajout de mémoire vive et/ou remplacement du disque dur par un disque SSD rapide. Modèle Processeur [c/t] (nova/cpu) Mémoire Disque Graphique Ecran Poids Prix Lenovo IdeaPad 320S-14IKB Core i3-7100U [2/4] (406/3798) 8Go SSD 128Go Intel HD620 14’’ (1920x1080) 1.7kg 500€ Acer Swift 3 Core i3-8130U [2/4] (579/5061) 4Go SSD 256Go Intel HD620 14’’ (1920x1080) 1.45kg 600€ MacBook Air Core i5-5350U [2/4] (363/3358) 8Go SSD 128Go Intel HD6000 13.3’’ (1440x900) 1.35kg 1000€ Avec un budget de 500€-600€, des concessions sont nécessaires. A titre d’exemple, nous reprenons deux configurations sous Windows ici. Le Lenovo choisi a 8Go de mémoire vive, mais un disque SDD de faible capacité (128Go) et un processeur un peu moins puissant. L’Acer a un plus gros disque et un meilleur processeur (toujours i3, cependant), mais n’a que 4Go de mémoire vive. Toutefois, un seul disque dur de seulement 128Go, c’est quand même fort juste sous Windows 10 qui est déjà très gourmand en espace disque à la base. Donc votre préférence ira si possible plutôt vers une configuration du type Acer Swift 3 ci-dessus47. Des versions avec processeur Core i5 et 8Go de mémoire existent. Elles sont parfaites, … mais le prix les alignent presque avec nos configurations de référence ci-dessous. Vous verrez aussi dans les tests que ces machines ne sont pas irréprochables, mais il faut mettre les “défauts” relevés en regard du prix très contenu, et relativiser. Pour la science des données, nous privilégierons des ordinateurs plus rapides, quitte a être un peu moins bien cotés dans les tests sur la qualité de l’écran (comme l’Acer, par exemple). Du côté Mac portables, nous avons le MacBook 12’’ et le MacBook Air, présenté ici. Ils sont beaucoup plus chers, mais ce sont des ordinateurs durables et bien finis qui se revendent très bien. Dans les deux cas, le processeur (même si Core i5, ou i7) est fort juste et est plus à l’aise en bureautique. Pour analyser des petits jeux de données, ça fonctionne quand même bien. Ici aussi, des concessions sont nécessaires sur la capacité du disque dur pour tirer le prix à … 1000€ tout de même ! Mais comme le MacBook Air existe depuis 2015 quasiment inchangé, vous pouvez trouver en occasion des modèles à prix équivalent aux deux autres machines, mais faites attention à éviter les modèles anciens à processeurs lents qui se vendent encore, et qui ne sont pas assez puissants ! Un modèle avec un disque de 256Go existe aussi. Attention aussi : même si ce modèle vieilli bien, il est quand même en fin de vie. Ca a un impact sur la revente. A.1.3.2 Configurations de référence Avec un budget un peu plus élevé, vous êtes nettement plus confortable : processeur assez rapide et 8Go de mémoire vive et disque SSD de 256Go. Ces laptops sont parfaits pour le cours de science des données biologiques et pour bien d’autres tâches dans le cadre de vos études. Modèle Processeur [c/t] (nova/cpu) Mémoire Disque Graphique Ecran Poids Prix HP Pavilion X360 Core i5-8250U [2/4] (801/7667) 8Go SSD 256Go GeForce MX130 14’’ (1920x1080) 1.6kg 900€ Acer Swift 5 Core i5-8250U [2/4] (801/7667) 8Go SSD 256Go Intel HD620 14’’ (1920x1080) 0.97kg 900€ MacBook Pro 13’’ Core i5-8259U [2/4] (???/10938) 8Go SSD 256Go Intel Iris+640 13’’ (2560x1600) 1.4kg 1600€ Le HP Pavilion est un portable à écran tactile représentatif de ce créneau (pour les modèles les plus puissants de la gamme en tous cas). Vous combinez un bon processeur, 8Go RAM, un disque dur rapide de 256Go, une carte graphique accélérée et un écran correct pour l’usage prévu pour un poids raisonnable. L’Acer Swift 5 est repris ici pour son poids plume et ses résultats excellents aux tests, mais sa carte graphique est en retrait par rapport au HP (élément secondaire). Un autre très bon exemple de machine portable qui convient parfaitement pour la science des données. Vivement conseillé, donc. Nous avons aussi inclu le premier MacBook Pro en version disque de 256Go à titre de comparaison (testé en version 2017) : il est plus cher mais à ce prix, vous avez tout de même un écran incomparablement meilleur, un processeur très rapide et une valeur à la revente bien plus haute. Chez Apple, restez dans la gamme MacBook Pro en laptops. Les processeurs des autres modèles les font tous entrer dans la catégorie de base. Attention aussi au prix des adaptateurs supplémentaires souvent indispensables pour les produits Apple ! N’oubliez pas de demander votre remise “éducation”, sur présentation de votre carte d’étudiant (le tarif indiqué tient compte de cette remise). A.1.3.3 Configurations performantes Un budget plus large permet d’acquérir un laptop de course qui sera utile pendant des années, et même pour un travail lourd plus tard… Dans ces configurations, pas de concessions. On veut un processeur i7 à 4 ou 6 coeurs ou équivalent, 16Go de RAM, un disque SSD d’au moins 512Go, ou mieux deux disques, une carte graphique rapide (sauf sur les ultra-portables) et un excellent écran. Modèle Processeur [c/t] (nova/cpu) Mémoire Disque Graphique Ecran Poids Prix MSI GF63 Core i7-8750H [6/12] (1400/12548) 16Go SSD 256Go + HDD 1To GeForce GTX1050Ti 15.6’’ (1920x1080) 1.86kg 1300€ Dell XPS 13 9370 Core i7-8550U [4/8] (837/8327) 16Go SSD 512Go Intel HD620 13.3’’ (1920x1080) 1.1kg 1400€ Asus ZenBook Flip Core i7-8550U [4/8] (837/8327) 16Go SSD 512Go Intel HD400 13.3’’ (1920x1080) 1.1kg 1600€ MacBook Pro 15’’ Core i7-8750H [6/12] (1400/12548) 16Go SSD 512Go Radeon Pro 555X 15.4’’ (2880x1800) 1.83kg 2700€ Le MSI est un PC dit “gamer”. Tous les laptops dans cette catégorie sont très rapides… et conviennent parfaitement bien pour la science des données, y compris pour les calculs GPU. Chez MSI, un modèle comme le GS65 Stealth obtient le label de meilleur “gamer laptop 2018” (chez techradar) et des modèles similaires sont très bien placés ailleurs. Mais nous préférons le GF63 moins cher car équipé d’une carte graphique un cran en dessous, et du coup, mieux positionné en rapport qualité/prix pour les sciences des données. Il est aussi nettement moins lourd. Chez Asus (ROG) et Lenovo entre autres, des machines quasi-équivalentes existent aussi. Attention : d’autres configurations de “laptops gamers” sont lourdes et elles chauffent beaucoup. Dans la catégorie ultraportable, on trouve aussi diverses machines plus puissantes. Le Dell XPS 13 de 13’’ est l’un des mieux classés systématiquement dans les tests un peu partout et représentatif de ce type d’ordinateurs ultra-compacts, légers, mais très performants. Poids plume oblige, on a un processeur moins puissant que sur un PC gamer (vérifiez qu’il soit suffisamment performant sur le modèle choisi, en effet, il existe plusieurs processeurs i5 et même i7 trop lents) et une carte graphique plus basique (pas grave). Choisissez un modèle avec 16Go de RAM et au moins 256Go de disque dur. Le nouveau Huawei Matebook X Pro est une splendide machine ultraportable avec un magnifique écran de 3000x2000 pixels (excusez du peu !), mais il n’est pas repris ici, car pas commercialisé en Belgique. Il est toutefois commercialisé en France… si vous acceptez d’utiliser un clavier français légèrement différent du clavier belge. Un test complet est aussi disponible. Ici, vous choisirez la version i5 avec processeur Core i5-8550U, 8Go de RAM et un disque SSD de 256Go pour une configuration de référence à 1500€, mais pourrez opter pour le i7 avec un Core i7-8250U, 16Go de RAM et un disque SDD de 512Go pour une configuration optimale pour environ 2000€. C’est l’un des meilleurs ordinateurs du moment ! Toujours dans les ultraportables, vous trouverez aussi les convertibles. Ceux équipés d’un écran tactile et qui peuvent se “retourner” pour s’utiliser comme une tablette haut de gamme. Les ordinateurs de type Microsoft Surface en sont les représentants emblématiques, mais les tests nous conduisent aussi vers l’Asus Zenbook Flip comme l’un des meilleurs (et assurément, un excellent rapport qualité/prix). Ici, on trouve des versions en 13’‘et en 15’’ mais toujours très portables et puissantes. De très bonnes machines pour analyser ses données ! A titre de comparaison, l’équivalent chez Apple est également présenté (le MacBook Pro 15’’, avec option disque de 512Go et remise “éducation”). Ce dernier est à nouveau beaucoup plus cher. Mais son écran est incomparable, sa finition est impeccable, et il tourne sous MacOS naturellement pour les afficionados ! Le modèle 2018 à 6 coeurs n’a pas encore été testé chez “les numériques”, mais voici le test du modèle 2017. C’est une excellente machine. Malheureusement, la tendance est au minimalisme pour la connectique : du USB-C et c’est tout. Cela oblige à acheter et à transporter des connecteurs supplémentaires. Les anciennes générations de MacBook Pro, “pré touch bar” se trouvent encore dans le marché de l’occasion à des prix proches de PC équivalents sous Windows. Ils sont un peu moins performants que les nouveaux, mais restent excellents, … et possèdent beaucoup plus de connecteurs intégrés (de bonnes affaires, donc). Voilà ! En espérant que ceci pourra vous aider au mieux dans le choix de votre outil informatique. Renseignez-vous au niveau des services étudiants à l’UMONS et à l’AGE : des aides existent pour les étudiants boursiers qui souhaitent acquérir un ordinateur dans le cadre de leurs études.↩ Ce ne sont que des exemples. Recherchez des configurations équivalentes chez d’autres constructeurs aussi !↩ "],
["install.html", "A.2 Installation", " A.2 Installation Vous allez devoir d’abord installer VirtualBox, un logiciel gratuit et libre qui se chargera de gérer votre machine virtuelle. Ensuite, vous installerez la SciViews Box en elle-même. Pour finir, vous aurez aussi besoin de Github Desktop. A.2.1 VirtualBox Récupérez l’installateur correspondant à votre système ici. L’installation avec tous les paramètres par défaut convient. Il se peut que vous voyiez un message vous indiquant que VirtualBox doit réinitialiser le réseau ou une autre ressource. Vérifiez que tous les documents en cours éventuels sont sauvegardés, et ensuite, vous pourrez continuer l’installation sans risques. De même, sous Windows, l’installateur de VirtualBox vous préviendra peut-être qu’il doit installer l’un ou l’autre périphérique. Vous pouvez également continuer sans craintes (précaution prise par Microsoft, mais ces périphériques fonctionnent bien). A.2.2 SciViews Box La procédure d’installation de la SciViews Box diffère selon le système d’exploitation. Reportez-vous à la sous-section correspondante pour Windows, MacOS ou Linux. A.2.2.1 Installation sous Windows Chargez l’installateur ici ou, pour les étudiants de l’UMONS, récupérez-le depuis le disque StudentTemp de la salle informatique (sous-répertoire SDD\\Software\\SciViews Box 2018). Pensez aussi à placer le fichier svbox2018a.vdi.xz dans le même répertoire que l’installateur svbox2018a_win_setup.exe. Sinon vous devrez le télécharger lors de l’installation (il pèse tout de même 2,9Gb)! Lancez l’installation. Vous verrez l’écran suivant (probablement en version française sur votre ordinateur). Vous pouvez cliquer ‘Yes’/‘Oui’. Il s’agit seulement d’une précaution de Microsoft lorsqu’il ne connait pas l’éditeur du programme à installer, comme c’est le cas ici. Si le fichier svbox2018a.vdi.xz n’est pas présent dans le même répertoire que le programme d’installation, il est à présent téléchargé (cliquez sur “Details” pour suivre l’opération): Une fois le téléchargement terminé, l’installation se poursuit. Vous verrez ensuite qu’il y a encore une opération obligatoire à lancer: la décompression du disque virtuel de la SciViews Box (svbox2018a.vdi) via ‘7z’. En cliquant ‘Finish’, cette décompression démarre toute seule. N’interrompez surtout pas la décompression du disque virtuel! Sinon, votre SciViews Box ne pourra pas démarrer et vous devrez tout recommencer à zéro en désinstallant et réinstallant complètement l’application. Losque tout est installé, vous avez une nouvelle icône sur votre bureau. Poursuivez à la section suivante pour démarrer et paramétrer votre SciViews Box. En option, vous pouvez épingler le nouveau programme dans la barre des tâches. Il sera plus facilement accessible (voir ci-dessous). A.2.2.2 Installation sous MacOS Chargez l’installateur ici ou, pour les étudiants de l’UMONS, récupérez-le depuis le disque StudentTemp de la salle informatique (sous-répertoire SDD/Software/SciViews Box 2018). Si vous le pouvez, placez le fichier svbox2018a.vdi.xz dans le dossier de téléchargements (Téléchargements ou Downloads selon la version de votre MacOS), sinon, ce fichier sera téléchargé au même emplacement (il pèse 2,9Gb)! Double-cliquez sur svbox2018a_macos_setup.dmg. Suivez simplement les instructions. Déplacez à la souris ‘SciViews Box 2018a’ vers le dossier ‘Applications’ dans la fenêtre de l’installeur (cette partie de l’installation est très rapide, donc, vous n’aurez peut-être pas l’impression que quelque chose se passe), Ensuite, toujours dans cette fenêtre, double-cliquez sur le dossier ‘Applications’ et recherchez l’entrée ‘SciViews Box 2018a’. Double-cliquez dessus, Si vous avez chargé l’installateur depuis Internet, il se peut que votre Mac indique un message et vous empêche de l’ouvrir. Dans ce cas, il faut cliquer avec le bouton droit de la souris et selectionner “Ouvrir” dans le menu contextuel tout en maintenant la touche ALT enfoncée, et ensuite cliquer “Ouvrir” dans la boite qui s’affiche. Laissez l’installation se terminer. Cela peut prendre plusieurs minutes. En option, vous pouvez aussi accrocher le programme de manière permanente dans le “Dock” pour le lancer facilement depuis cet endroit. Cliquez bouton droit et dans le menu “Options”, sélectionnez l’entrée “Garder dans le Dock”. A.2.2.3 Installation sous Linux Il est parfaitement possible d’installer la SciViews Box sous Linux. Cependant, un programme d’installation simplifié n’a pas encore été développé pour ce système. Voyez au cas par cas avec vos enseignants pour qu’ils vous expliquent comment installer la SciViews Box manuellement sous Linux. A.2.2.4 Migration et désinstallation Le disque dur virtuel de la SciViews Box est un fichier volumineux de plus de 10Go. L’installeur fait en sorte qu’il soit partagé entre plusieurs utilisateurs de l’ordinateur, et qu’il reste inchangé au cours de son utilisation. Ainsi, VirtualBox enregistrera dans vos dossiers personnels un fichier qui stocke les différences par rapport à l’état de départ de la Box. Il est donc possible de désinstaller partiellement la SciViews Box 2018 sans rien perdre. Pour cela, il suffit de désinstaller l’application (sous Windows, allez dans le panneau de configuration -&gt; Applications -&gt; SciViews Box 2018 -&gt; Désinstaller ; sous MacOS déplacez simplement l’application SciViews box 2018a depuis le dossier Applications vers la corbeille). Vous récupèrerez immédiatement près de 11Go d’espace disque. VirtualBox ne pourra plus démarrer la Box, naturellement, mais conservera vos données. Si besoin, vous pourrez réinstaller simplement l’application SciViews Box 2018a pour retrouver votre Box en l’état. Une désinstallation complète nécessite d’aller d’abord supprimer la machine virtuelle dans VirtualBox (clic bouton droit et sélection de Supprimer...) pour tous les utilisateurs qui ont créé une Box avant de désinstaller l’application principale comme ci-dessus. Si vous avez des projets créés avec des SciViews Box antérieures, deux solutions existent : Gardez-les tel quels. Faites éventuellement une désinstallation partielle de la Box. Vous pourrez toujours revenir plus tard sur ces projets après réinstallation. Migrez-les vers la nouvelle SciViews Box. Copiez vos projets depuis le répertoire shared de l’ancienne Box vers celui de la nouvelle. Dans ce cas, vous devrez vérifier que votre code fonctionne toujours sous la nouvelle Box, et l’adapter éventuellement. A.2.3 Github Desktop Dans ce cours, nous utilisons Git et Github pour gérer les différentes versions de vos projets et les partager avec vos binômes et vos enseignants. Github Desktop facilite grandement la gestion de vos projets sous Git. Ce programme gratuit est très facile à installer : son téléchargement et le lancement de son installeur ne pose pas de problèmes particuliers. Notez toutefois que ce programme n’est pas encore disponible pour Linux. A présent, tous les ligiciels requis sont installés… Il ne reste plus que quelques petites opérations de configuration à réaliser. Voyez ceci à la section suivante. "],
["configuration.html", "A.3 Configuration", " A.3 Configuration Même si la SciViews Box est pré-configurée, vous allez avoir quelques manipulations simples à réaliser pour être complètement opérationnel. Ces étapes sont détaillées ci-dessous. Nous en profiterons par la même occasion par nous familiariser avec quelques uns des outils logiciels que vous utiliserez plus tard, à commencer par le lanceur rapide SciViews Box. A.3.1 Lanceur SciViews Box L’application que vous venez d’installer est un lanceur rapide qui facilite le démarrage, la fermeture et la gestion de votre machine virtuelle SciViews Box 2018a. Démarrez-là et vous verrez la fenêtre suivante: Le message en rouge n’apparait pas systématiquement. Il signale des éléments importants. Ici, il indique que la configuration de la SciViews Box doit encore être faite, et pour cela, vous devez (1) la démarrer à l’aide du gros bouton en haut à gauche, (2) vous logger (mot de passe = sv), et (3) répondre Yes lorsqu’une boite de dialogue vous propose d’installer ‘svbox2018a v1.0.0’. Cette dernière étape est importante! Ne cliquez pas No ici, sous peine de ne pas avoir une machine virtuelle configurée comme celle de vos collègues! Le mot de passe vous sera redemandé, et ensuite, l’installation se poursuivra. Elle pourra prendre plusieurs minutes. Soyez patient. Vous pourrez ouvrir la fenêtre du terminal où s’opère le travail pour en suivre la progression, si vous le souhaitez. A la fin vous verrez la fenêtre du configurateur de la SciViews Box apparaître. A.3.2 Configurateur de la Box Prenez le temps de parcourir les différents éléments dans cette fenêtre48. La partie à gauche en haut concerne la configuration du clavier. En effet, la machine virtuelle utilisera votre clavier physique, mais elle n’a aucun moyen de déterminer de quel modèle il est. Vous allez donc l’indiquer maintenant. Utilisez la zone de texte intitulée Test area (type here) pour vérifier que la machine virtuelle interprète correctement les touches de votre clavier. Pour le changer, cliquez sur le bouton Change keyboard layout. La boite de dialogue de sélection du clavier apparait. Elle propose des configurations différentes sous forme de représentations graphiques, avec les touches caractéristiques surlignées en jaune. Vous pouvez entrer les premières lettres du type de clavier pour aller directement à la configuration correspondante dans la liste (ex.: entrez be pour un clavier belge). Si votre clavier ne se trouve pas dans les templates les plus courants, configurez-le à l’aide du bouton Other keyboard.... Fermez cette fenêtre pour retourner au configurateur lorsque vous aurez fini. Enfin, toujours concernant le clavier, la case à cocher Exchange left CTRL / CMD (Mac shortcuts) permet d’utiliser les raccourcis Mac (comme Cmd-c pour copier et Cmd-v pour coller à la place de Ctrl-c ou Ctrl-v sur un PC. Cette option n’est utile qu’aux possesseurs d’un Mac qui veulent avoir des raccourcis plus homogènes entre leur système MacOS hôte et la machine virtuelle49. Juste en dessous, vous voyez la configuration du fuseau horaire. Ici aussi, votre machine virtuelle n’a pas l’information de votre système hôte, et peut donc ne pas afficher l’heure correctement. Vous avez la possibilité de corriger cela en cliquant sur le bouton Change time zone. Vous devez débloquer la boite de dialogue (bouton Unlock en bas, puis entrer le mot de passe pour pouvoir effectuer des changements). Les trois boutons à gauche en bas servent à choisir le stylage des fenêtres, le set d’icônes et l’image d’arrière plan de votre SciViews Box. C’est ici que vous pourrez la paramétrer au mieux pour qu’elle vous plaise visuellement. A noter que, si vous double-cliquez sur les entrées dans les boites de dialogue de configuration, vous allez pouvoir prévisualiser l’effet en live. Utile pour apprécier le rendu avant de faire son choix! La zone en bas à droite permet de modifier le mot de passe. Pour rappel, il s’agit d’un mot de passe simple et peu sécure par défaut: sv. En fait, vous n’avez pas réellement besoin d’un mot de passe à l’intérieur de votre SciViews Box telle qu’elle est configurée car vous ne pouvez y accéder qu’en local à partir de l’ordinateur hôte. Par contre, il est possible d’ouvrir l’accès. A ce moment-là, il serait utile, et même indispensable, de modifier le mot de passe. Dans le cadre de votre utilisation de la SciViews Box pour ce cours, que ce soit sur les machines de la salle de T.P., ou sur votre ordinateur personnel, ne changez pas le mot de passe! Votre machine virtuelle est déjà protégée par votre système hôte puisque seul un accès local est autorisé. La zone en haut à droite permet de configurer votre compte Git. Comme vous allez utiliser Git et Github de manière intensive tout au long de ce cours, veuillez configurer cette partie du système correctement d’amblée! Les trois boutons du bas proposent de s’enregistrer sur trois systèmes distants d’hébergement de dépôts Git (si vous ne savez pas ce que c’est, imaginez juste que c’est là que vous allez pouvoir entreposer de manière sûre tous vos projets!): Github, Gitlab ou Bitbucket. Tous trois ont des avantages et des inconvénients, et ils proposent tous des utilisations gratuites dans certains cas. Durant nos cours de Science des Données à l’UMONS, nous utiliserons Github. Cette utilisation sera gratuite pour vous, et vous allez pouvoir déjà commencer à construire votre identité professionnelle sur le Net par son intermédiaire. Donc, enregistrez-vous de manière sérieuse. Choisissez un login représentatif de vos nom et prénom, pas un truc louffoque ou rigolo sur le moment, mais que vous regretterez plus tard, sachant que votre login ne pourra pas être changé ensuite! Vous allez donc vous créer un compte sur Github en cliquant sur le bouton correspondant, et en indiquant un login et un mot de passe. Nous vous demandons également d’utiliser expressément et uniquement votre adresse email UMONS ici : prénom.nom@student.umons.ac.be. En effet, ce sera, pour nous, notre seul moyen de vous identifier sans erreur sur Github lorsque nous interviendrons pour vous conseiller et/ou pour corriger vos travaux. Une fois enregistré sur le site de Github, reportez votre login et votre adresse email dans le configurateur de la SciViews Box, pour que Git puisse vous identifier correctement en local50. Une fois tout ceci effectué vous pourrez cliquer sur le bouton OK de la fenêtre du configurateur SciViews Box. La machine virtuelle devra redémarrer pour appliquer toutes les modifications de manière durable. Cliquez également OK donc dans la boite de dialogue qui apparait ensuite (sinon, elle redémarrera toute seule après 30 sec) : A.3.3 Installation des tutoriels Nous allons maintenant installer les tutoriels liés à ces cours de science des données biologiques. Vous allez apprendre par la même occasion comment ajouter des applications dans votre SciViews Box. Cela se fait en trois étapes: Télécharger l’installeur de l’application. Vous le trouverez à l’adresse http://go.sciviews.org/BioDataScience1. Assurez-vous de bien le charger dans le répertoire Downloads ou Téléchargements par défaut sur votre ordinateur51. Téléchargez, de même, la mise-à-jour vers la version 2.0 de la SciViews Box à l’adresse http://go.sciviews.org/svbox2018a2. Rentrez dans le lanceur rapide de la SciViews Box. Il repère les autoinstalleurs et les déplace dans le dossier partagé pour les rendre utilisables par la SciViews Box. Vous devez voir un message indiquant la disponibilité d’autoinstalleur(s). Vos fichiers téléchargés ont également disparus du répertoire de téléchargement à ce stade, Rentrez dans la SciViews Box depuis le lanceur. Si la Box était déjà active ou si elle est réveillée du mode veille, vous allez devoir vous délogger et relogger pour que l’installation démarre, … sinon, vous allez voir directement le message suivant qui propose d’installer ces apps (cliquez sur Yes, bien sûr, pour l’installer). Si le message n’apparait pas, voici comment se délogger (log out). Ensuite, entrez votre mot de passe comme d’habitude pour vous relogger… A ce moment, le message doit apparaitre, et l’installation doit se faire après avoir cliqué le bouton Yes. Bravo! Vous avez terminé l’installation et la configuration de votre SciViews Box. Cependant, nous allons encore effectuer une petite opération qui vous facilitera la vie, et nous vous expliquerons par la même occasion comment accéder aux fichiers respectifs de la machine virtuelle et du système hôte dans la section suivante. A.3.4 Accès aux fichiers Le disque physique de votre ordinateur hôte, et le disque virtuel de la SciViews Box sont deux choses différentes. Cela signifie que vous avez, en réalité deux ordinateurs et deux disques indépendants. Donc, vous n’accédez pas aux fichiers d’une machine à partir de l’autre52. Ce n’est pas pratique, et ce n’est pas vrai pour un dossier particuler nommé shared. Ce dossier shared est synchronisé en temps réel entre les deux systèmes. C’est donc l’endroit idéal pour échanger des données et pour faire collaborer vos deux machines. Inutile de préciser, donc, que nous travaillerons essentiellement à l’intérieur de ce dossier. Un sous-dossier, nommé projects sera utilisé pour héberger toutes nos analyses. Il est donc primordial d’y accéder facilement à la fois depuis l’ordinateur hôte et depuis la SciViews Box. Vous allez donc apprendre à retrouver ce dossier projects facilement. Sur votre ordinateur hôte, ce dossier est un peu difficile à trouver en naviguant dans l’explorateur de fichiers (ou le Finder sur le Mac). Pour cette raison, le lanceur rapide propose un bouton pour y accéder plus facilement. Une fois dans le dossier shared, nous vous conseillons d’épingler le sous-dossier projects dans les raccourcis rapides de votre explorateur de fichiers. Voici comment faire sous Windows et sous MacOS. Sous Windows, cliquez bouton droit sur projects, et sélectionnez “épingler dans Accès rapide”. Sous MacOS, vous glissez-déposez projects dans la barre latérale du Finder. Dans la SciViews Box, ce dossier est accessible depuis deux endroits: /media/sf_shared et ~/shared (~ représente le répertoire de l’utilisateur, c’est-à-dire /home/sv). Ici aussi vous pouvez épingler votre dossier projects pour en facilter l’accès: Les deux moyens d’accéder au dossier projects dans la SciViews Box et comment l’épingler sur le côté. A retenir: le dossier shared et ses sous-dossiers comme projects sont considérés un peu comme des dossiers réseau par la SciViews Box. Cela implique que certaines fonctions du système de fichiers n’y sont pas accessibles. Parmi celles-ci, la poubelle. Donc, vous ne pourrez qu’effacer complètement des items en cliquant bouton droit et sélectionnant ‘Delete’ dans le menu contextuel dans le gestionnaire de fichiers. Si vous essayer de placer des fichiers ou dossiers depuis shared dans la poubelle de la SciViews Box, cela vous sera refusé (voir copie d’écran ci-dessous). Par contre, cela fonctionne très bien depuis l’ordinateur hôte. Un tout dernier point concernant les ordinateurs de la salle de T.P. de l’UMONS. Pour des questions de performance, la machine virtuelle SciViews Box, et le dossier shared ne sont pas sur votre compte, mais directement sur le disque de l’ordinateur. Cela signifie qu’ils ne sont pas transportables vers un autre ordinateur. Vous pouvez créer une copie de shared dans mes documents ou sur une clé USB pour les transporter vers un autre ordinateur… mais nous verrons que cela n’est pas nécessaire pour tout ce que vous stockerez sur Github. En effet, vous avez accès à ces contenus depuis n’importe où via n’importe quelle connexion internet. Si jamais vous voulez retourner plus tard au configurateur de la SciViews Box, vous n’aurez qu’à cliquer sur son icône tout en haut à droite dans la barre supérieure.↩ Le Mac définit ses raccourcis claviers différemment du PC. Outre l’inversion de l’utilisation des touches Ctrl et Cmd, le Mac possède deux touches Alt, une à gauche et une à droite. Le PC a, par contre, deux touches correspondantes, mais celle de droite est nommée Alt Gr. Ces touches jouent des rôles différents: raccourcis claviers pour Alt et accès aux touches de niveau 3 et 4 pour Alt Gr. Pour les utilisateurs Mac, notez que vos deux touches Alt ont des rôles différents dans la SciViews Box comme pour un clavier PC. Enfin, VirtualBox réserve une touche clavier à son propre usage. Par défaut, c’est la touche Cmd ou Win de droite. Il est déconseillé de modifier ce choix car toutes les autres touches sont indispensables dans la SciViews box!↩ A la première utilisation de Git à l’intérieur du logiciel RStudio, votre login et votre mot de passe vous seront également redemandés. De même, vous devrez également fournir ces informations dans Github Desktop et la première fois que vous naviguerez vers https://github.com depuis le navigateur Web de votre PC hôte. Mais ensuite, vous accèderez immédiatement au service.↩ Il pourra ainsi être replacé au bon endroit et exécuté dans la Box↩ Le presse-papier est synchronisé entre les deux machines pour le texte qui y est copié.↩ "],
["svbox-use.html", "A.4 Utilisation", " A.4 Utilisation Une fois votre machine virtuelle configurée, vous vous trouvez confronté à cet écran qui montre le fond d’écran et un ensemble d’items par dessus. Nous l’appellerons le bureau de la SciViews Box. Cette machine virtuelle utilise le système d’exploitation Linux. Vous pouvez accèder au application présentes sur cette machine à partir du menu Applications en haut à gauche du bureau. Ce dernier offre un menu déroulant avec l’ensemble des applications disponibles. Ces applications sont rangé en dossier tel que Favorites, Recently Used , All, … Le “dock” en bas du bureau permet de lancer des applications rapidement et d’accéder aux fenêtres des applications en cours d’exécution tel que RStudio, Jupyter, Spyder,… Pour accèder à vos dossiers et fichiers, il suffit de cliquer sur l’icône en forme de dossier avec une image de petite maison que l’on retrouve également dans le dock. "],
["prise.html", "B Prise en main", " B Prise en main Cette annexe comprend une description détaillée des différents outils utilisés dans la cadre de cette formation. Passez à la section suivante pour découvrir les outils. (ex: B.1 RStudio). "],
["rs.html", "B.1 RStudio", " B.1 RStudio Sélectionnez l’icône RStudio dans le dock (cercle bleu avec un R blanc au centre). Un login vers RStudio apparaît. Il faut y entrer les informations suivantes : Username : sv Password : sv Cochez éventuellement Stay signed in pour éviter de devoir réentrer ces informations continuellement : RStudio s’ouvre. C’est votre interface de travail à partir de laquelle vous allez piloter R. La fenêtre principale comporte différents éléments : Une barre de menu et une barre d’outils générale en haut Un panneau à gauche intitulé Console où vous pouvez entrer des instructions dans R pour manipuler vos données Un panneau à droite en haut qui comprend plusieurs onglets, dont Environnement qui vous indique les différents items (on parle d’objets) chargés en mémoire dans R (mais pour l’instant, il n’y a encore rien). Un panneau en bas à droite comportant lui aussi plusieurs onglets. Vous devriez voir le contenu de Files au démarrage, un explorateur de fichiers simplifié relatif au contexte de travail actuel dans RStudio. Pour l’instant, aucun document de travail n’est encore ouvert. Pour en créer un, ou ouvrir un document existant, vous utilisez le menu Files, ou encore, le premier bouton de la barre d’outils générale : Le menu Session permet d’interagir directement avec R qui est lancé automatiquement en arrière plan dès que RStudio est ouvert. Par exemple, il est possible de relancer R à partir d’une entrée dans ce menu : Le menu Help propose différentes possibilités pour accéder à la documentation de R ou de RStudio. Les aide-mémoires (“cheatsheets” en anglais) sont très pratiques lors de l’apprentissage. Nous conseillons de les imprimer et de les consulter régulièrement. Le dernier bouton de la barre d’outils générale, intitulé Project permet d’ouvrir, fermer, et gérer les projets RStudio. Vous avez maintenant repéré les éléments fondamentaux de l’interface de RStudio. A ce stade vous pouvez vous familiariser avec l’aide-mémoire relatif à l’IDE RStudio. Vous verrez qu’il y a beaucoup de fonctionnalités accessibles à partir de la fenêtre principale de RStudio. Ne vous laissez pas intimider : vous les apprendrez progressivement au fur et à mesure de l’utilisation du logiciel. B.1.1 Projet dans RStudio Un projet sert, dans RStudio, à organiser son travail. Un projet va regrouper l’ensemble des jeux de données, des rapports, des présentations, des scripts d’une analyse généralement en relation avec une ou plusieurs expériences ou observations réalisés sur le terrain ou en laboratoire. Voici à quoi ressemble l’interface de RStudio lorsque vous ouvrez un projet : Notez que le nom du projet est mentionné en haut à droite. Notez également, que le répertoire de base de votre projet est le répertoire actif dans l’onglet Console (~/shared/projects/mon_premier_projet/ dans l’exemple), et que l’onglet Files affiche son contenu. Un fichier mon_premier_projet.Rproj y est placé automatiquement par RStudio. Ce fichier contient les paramètres de configuration propres à ce projet53. C’est aussi une excellente façon de repérer qu’un répertoire est la base d’un projet RStudio, en repérant ce fameux fichier .Rproj. B.1.1.1 Création d’un projet Créez votre premier projet en suivant les quatre étapes suivantes : Étape 1. Dans RStudio, Sélectionnez le bouton tout à droite dans la barre d’outils générale de RStudio qui ouvre un menu contextuel relatif aux projets. Sélectionnez y l’entrée New Project.... Étape 2. Une boite de dialogue s’ouvre. Sélectionnez New Directory pour créer votre projet dans un nouveau dossier. Il est également possible d’employer un dossier existant comme point de départ Existing Directory). Étape 3. Sélectionnez New Project tout en haut dans l’écran suivant qui vous propose également des projets particuliers (que nous n’utiliserons pas pour l’instant). Étape 4. Ensuite, RStudio vous demander quelques informations pour préconfigurer votre projet. Nommez le projet : Directory name. Indiquez ici project_test Indiquez où vous voulez le placer : Create project as subdirectory of. Sélectionnez le sous-dossier projects dans le dossier shared partagé entre la SciViews Box et la machine hôte. Sélectionnez Create a git repository Désélectionnez Use packrat with this project(il est important de ne pas sélectionner packrat, sous peine de dupliquer de nombreux packages R dans votre projet) Vous utilisez aussi le menu spécial projet pour créer un nouveau projet (New Project...), ouvrir un projet existant (Open Project...) ou encore fermer un projet (Close Project). Vous remarquez également que les derniers projets employés sont placés sous les trois options citées ci-dessus afin d’y accéder plus rapidement. Un projet ne doit bien sûr être créé qu’une seule fois ! Une fois les étapes ci-dessus effectuées, vous retournez simplement à votre projet en ouvrant le menu contextuel projets et en sélectionnant votre projet dans la liste. S’il n’y apparait pas, choisissez Open Project... et sélectionnez le fichier .Rproj relatif à votre projet. Ne créez bien évidemment jamais de projet à l’intérieur des dossiers d’un autre projet, surtout si vous utilisez Git. Sinon, RStudio va s’emméler les pinceaux ! B.1.1.2 Organisation d’un projet Le répertoire projects contient maintenant un projet RStudio intitulé project_test. Depuis la SciViews Box, il se situe dans : /home /sv /shared /projects /project_test # Le répertoire de base du projet project_test.Rproj # Fichier de configuration du projet créé par RStudio .gitignore # Fichier relatif à la gestion de version Vous devez maintenant structurer votre projet afin d’avoir différents sous-répertoires pour organiser au mieux le travail. Ceci concerne à la fois les données et les rapports d’analyse en lien avec ce projet. Cliquez sur le bouton New Folder dans la barre d’outils de l’onglet Files et appelez ce nouveau dossier data. Ajoutez également les dossiers analysis et R. Vous pouvez faire cela depuis RStudio, mais aussi depuis le système hôte si c’est plus confortable pour vous. /home /sv /shared /projects /project_test # Le répertoire de base du projet analysis # Le dossier qui comprend toutes les analyses (rapport, présentation,...) data # Le dossier qui comprend toutes les données project_test.Rproj # Fichier de configuration du projet créé par RStudio .gitignore # Fichier relatif à la gestion de version R # Le dossier qui comprend tous les scripts d&#39;analyse Vous obtenez donc un projet configuré de la manière suivante : L’organisation cohérente d’un projet est indispensable pour le bon fonctionnement et la clarté de vos analyses de données. B.1.1.3 Chemins relatifs dans un projet L’utilisation d’un projet permet de structurer de manière cohérente son travail. Vous allez maintenant devoir rendre votre projet portable. Un projet RStudio pourra être qualifié de portable s’il est possible de déplacer son répertoire de base et tout ce qu’il contient (ou le renommer) sans que les analyses qu’il contient n’en soient affectées. Ceci est utile pour copier, par exemple, le projet d’un PC à un autre, ou si vous décidez de restructurer vos fichiers sur le disque dur. La première règle est de placer tous les fichiers nécessaires dans le dossier du projet ou dans un sous-dossier. C’est ce que nous venons de faire plus haut. La seconde règle est de référencer les différents fichiers au sein du projet avec des chemins relatifs. Nous allons maintenant apprendre à faire cela. /home /sv /shared /projects /project_test # Le répertoire de base du projet analysis # Le dossier qui comprend toutes les analyses (rapport, présentation,...) rapport_test.rmd # Rapport d&#39;analyse data # Le dossier qui comprend toutes les données dataset.csv # jeu de données exemple project_test.Rproj # Fichier de configuration du projet créé par RStudio .gitignore # Fichier relatif à la gestion de version R # Le dossier qui comprend tous les scripts d&#39;analyse Les différents systèmes d’exploitations (Windows, MacOS, Linux) utilisent des conventions différentes pour les chemins d’accès aux fichiers. Dans notre cas, la machine virtuelle utilise un système d’exploitation Linux. La barre oblique (/ dite “slash” en anglais) sépare les différents dossiers imbriqués sous Linux et sous MacOS. Le système d’exploitation Windows utilise pour sa part, la barre oblique inversée (\\, dite “backslash” en anglais, mais dans R et RStudio, vous pourrez également utiliser le slash /, ce que nous vous conseillons de faire toujours pour un maximum de compatibilité entre systèmes). Par exemple, votre fichier dataset.csv se référence comme suit dans la SciViews Box, donc sous Linux : /home/sv/shared/projects/project_test/data/dataset.csv Ce chemin d’accès est le plus détaillé. Il est dit chemin d’accès absolu au fichier. Vous noterez qu’il est totalement dépendant de la structure actuelle des dossiers sur le disque. Si vous renommez project_test ou si vous le déplacez ailleurs, la référence au fichier sera cassée ! Ainsi, si vous partagez votre projet avec un collaborateur qui le place ailleurs sur son disque dur, le chemin d’accès devra être adapté sans quoi l’analyse ne pourra plus s’exécuter correctement. Décodons ce chemin d’accès : /, racine du système /home/sv/, notre dossier personnel comme utilisateur sv /home/sv/shared/, le dossier partagé entre la SciViews Box et notre PC hôte /home/sv/shared/projects/project_test/, le dossier de base de notre projet /home/sv/shared/projects/project_test/data/, le répertoire qui contient le fichier dataset.csv. Le répertoire utilisateur /home/&lt;user&gt; est différent sous MacOS (il s’appelle /Users/&lt;user&gt;) et sous Windows (il se nomme généralement C:\\Users\\&lt;user&gt;). Comme c’est un répertoire clé, et qu’il est impossible d’écrire un chemin absolu qui soit le même partout, il existe un raccourcis : le “tilde” (~) qui signifie “mon répertoire utilisateur”. Ainsi, vous pouvez aussi accéder à votre jeu de données dataset.csv comme ceci : ~/shared/projects/project_test/data/datasets.csv Ce chemin d’accès est déjà plus “portable” d’un système à l’autre et d’un utilisateur à l’autre. Il est donc à préférer. Notez que sous R, vous devez doubler les backslashs sous Windows (~\\\\Documents\\\\...). Ce n’est ni très esthétique, ni compatible avec les deux autres systèmes. Heureusement, R comprend aussi le slash comme séparateur sous Windows, de sorte que la même syntaxe peut être utilisée partout ! Nous vous conseillons donc d’utiliser aussi systématiquement les slashs sous Windows dans R ou RStudio. Si cette façon d’écrire le chemin d’accès est compatible entre les trois systèmes d’exploitation, elle ne permet toujours pas de déplacer ou de renommer notre projet. L’utilisation d’un chemin relatif permet de définir la position d’un fichier par rapport à un autre dossier qui est dit le répertoire actif. A titre d’exemple, nous voulons faire référence au jeu de données dataset.csv depuis notre rapport rapport_test.Rmd. Demandez-vous d’abord quel est le répertoire actif. Pour un fichier R Markdown ou R Notebook, c’est facile, c’est le dossier qui contient ce fichier. Dans la console R, cela peut varier selon le contexte. Si vous avez ouvert un projet, c’est le répertoire de base du projet par défaut, mais cela peut être modifié. Le répertoire actif pour R est toujours indiqué en gris à côté de l’onglet Console dans RStudio. Vous pouvez aussi interroger R à l’aide de l’instruction getwd(): getwd() Vous pouvez réaliser cela dans un chunk R dans votre document R Notebook par exemple : Une fois que vous connaissez le répertoire actif, vous naviguez à partir de celui-ci. Il existe une convention pour reculer d’un dossier dans la hiérarchie : pour cela vous indiquez .. à la place d’un nom de dossier. Voici ce que cela donne : ../data/dataset.csv Comment lit-on ceci? Tout d’abord, notez (c’est très important) que le chemin d’accès ne commence pas par / (Linux ou MacOS), ou C:/ (ou toute autre lettre, sous Windows). C’est le signe que l’on ne part pas de la racine du système de fichier, mais du répertoire actif. Ensuite, les différents éléments se décryptent comme suit : ~/shared/projects/project_test/analysis, répertoire actif au départ pour le document R Notebook .., retour en arrière d’un niveau. On est donc dans ~/shared/projects/project_test /data, naviguer dans le sous-dossier data. On est donc maintenant dans ~/shared/projects/project_test/data. C’est le répertoire qui contient le fichier qui nous intéresse /datasets.csv, le nom du fichier référencé. A noter que si le fichier se trouve déjà dans le répertoire actif, le chemin relatif se résume au nom du fichier directement ! Nulle part dans ce chemin relatif n’apparaît le nom du répertoire de projet, ni d’aucun autre répertoire parent. Ainsi, il est possible de renommer ou déplacer le projet sans casser la référence relative à n’importe quel fichier à l’intérieur de ce projet. Donc, en utilisant uniquement des références relatives, le projet reste parfaitement portable. B.1.2 Scripts R dans RStudio Un script R est une suite d’instructions qui peuvent être interprétées pour effectuer nos analyses. Ce script est stocké dans un fichier dont l’extension est .R, et que l’on placera de préférence dans le sous-dossier R de notre projet. Un script R s’ouvre dans la fenêtre d’édition de RStudio. Les parties de texte précédées d’un dièse (#) sont des commentaires. Ils ne sont jamais exécutés, mais ils permettent de structurer et d’expliquer le contenu du document (ou bien d’empêcher temporairement l’exécution d’instructions). Afin de bien documenter vos scripts, Commencez-les toujours par quelques lignes de commentaires qui contiennent un titre, le nom du ou des auteurs, la date, un copyright éventuel, … L’utilisation de sections comme à la ligne 6 ci-dessus est vivement conseillée. Ces sections sont créée à l’aide de l’entrée de menu Code -&gt; Insert Section... dans RStudio. Elles sont reprises dans le bas de la fenêtre édition pour une navigation rapide dans le script. B.1.2.1 Création d’un script R Vous avez à votre disposition plusieurs méthodes pour ouvrir un nouveau script R dans RStudio, dont deux vous sont montrées dans l’animation ci-dessous. B.1.2.2 Utilisation d’un script R Un script R est un document natif de R. Ce dernier va interpréter les intructions qui compose le script et qui ne sont pas précédées d’un dièse (cliquez sur Run dans la barre d’outils de la fenêtre d’édition, ou utilisez le raccourci clavier Ctrl+Enter ou Cmd+Enter sur MacOS pour exécuter des instructions). Un script R doit être organisé de manière cohérente afin d’être exécutable de haut en bas. Dans l’exemple ci-dessus, on commence par : Étape 1. Importer les principaux outils avec l’instruction SciViews::R. Étape 2. Utiliser l’instruction urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;) pour importer le jeu de données urchin_bio provenant du package data.io et l’assigner à urchin. On retrouve à présent urchin dans l’environnement global (Global environment dans l’onglet Environnement dans le fenêtre en haut à droite) de RStudio. Étape 3. .?urchin et View(urchin) donnent des renseignements sur le jeu de données en renvoyant vers la page d’aide du jeu de données et en ouvrant ce jeu de données dans une fenêtre de visualisation. Étape 4. Réaliser des graphiques avec la fonction chart(). Notez que les instructions exécutées dans le script sont envoyées dans la fenêtre Console en bas à gauche. B.1.3 R Markdown/R Notebook Un document R Markdown est un fichier dont l’extension est .Rmd. Il combine à la fois des instructions R (pour les analyses) et le langage Markdown (pour le texte). le R Markdown ne vous permet pas de visualiser directement le résultat final d’un rapport d’analyse54 Tout comme dans un script R, les intructions doivent être également exécutées lors de la réalisation du rapport. Une forme spéciale de document R Markdown est le R Notebook. Ce dernier est un peu un intermédiaire entre un script R et un R Markdown. Il se présente de manière très similaire à ce dernier, mais vous pouvez également exécuter le code qu’il contient ligne par ligne comme dans un script. Un document R Markdown / R Notebook se structure de la manière suivante : Un préambule Des zones d’édition Le language employé est le Markdown Des zones de code R Ces zones de codes sont appelée des chunks Le préambule est nécessairement situé au tout début du document et est balisé à l’aide de trois tirets --- sans rien d’autre sur une ligne au début et à la fin. Le préambule comporte un ensemble d’entrées de type nom: valeur qui configurent le document ou la façon dont il sera compilé en rapport final. Nous pouvons y indiquer le titre principal, le ou les auteurs, la date, … Le reste du document R Markdown est subdivisé en zones successives de texte et de code R (les fameux “chunks”) contrastés sur des fond de couleurs différentes dans RStudio. Les zones de texte des parties Markdown où vous pouvez écrire votre prose. Les chunks contiennent des instructions qui vont être interprétées pour réaliser un calcul, un graphique, un tableau, etc. Le résultat de ce traitement sera placé à cet endroit dans le rapport final. Ces chunks sont balisés en entrée par trois apostrophes inverses suivies d’accolades contenant des instructions relatives au programme à utiliser, par exemple, ```{r} pour des chunks faisant appel au logiciel R, et sont terminés par trois apostrophes inverses (```). Dans les zones Markdown, vous pouvez ajouter des balises qui permettront de formater votre texte dans la version finale de votre rapport. Par exemple, un ou plusieurs dièses (plus communément connu par sont appellation en anglais : “hastag”) en début de ligne suivi d’un espace indique que la suite correspond à un titre. Titre de niveau 1 avec un seul dièse, de niveau 2 avec deux dièses, et ainsi de suite jusqu’à 6 niveaux possibles. Dans la capture d’écran ci-dessous, nous avons remplacé tout le contenu par défaut d’un R Notebook (à part le préambule) par une série de titres de niveau 1 correspondant à la structure générale d’un rapport scientifique : Introduction Objectif Matériel et méthodes Résultats Discussion Conclusions B.1.3.1 Création d’un R Markdown/Notebook Vous avez à votre disposition deux méthodes pour ouvrir un nouveau R Notebook dans RStudio. Voyez l’animation ci-dessous. B.1.3.2 Utilisation d’un R Markdown/Notebook Afin de visualiser les résultats des chunks dans votre rapport final, Vous devez veiller à exécuter chaque chunks dans l’ordre dans un R Notebook. Ceci n’est pas nécessaire dans un R Markdown, mais dans ce cas, tous les chunks sont systématiquement recompilés à chaque génération de rapport, ce qui peut être pénible si les calculs sont longs. Pour exécuter un chunk, vous pouvez : cliquer sur le bouton “play”, sous forme d’une flèche verte pointant vers la droite, situé en haut à droite du chunk cliquer sur Run et sélectionner Run Current Chunk dans le menu déroulant qui apparait Employer le raccourci clavier Ctrl+Shift+Enter Le bouton Run propose plusieurs actions intéressantes : Exécuter la/les ligne(s) d’instruction sélectionnée(s) : Run Selected Line(s) Exécuter le chunk en entier : Run Current Chunk Exécuter tous les chunk précédents : Run All Chunk Above Redémarer la console R et exécuter tous les chunks: Restart R and Run All Chunks. Cette action est particulière intéressante pour s’assurer que le document est réellement reproductible ! … Aprés la phase d’édition du texte (et des intructions dans les chunks pour un document R Notebook), vous pouvez visualiser votre rapport final en cliquant sur le bouton Preview (Notebook) ou Knit (Markdown). Le rapport est rapidement généré avec un rendu simple et professionnel. Par défaut, ce rapport présente le texte que vous avez écrit, avec les résultats que vous avez choisi de générer via R, mais également les instructions que vous avez employée pour obtenir ces résultats. Ceci permet de mieux comprendre, directement dans le rapport, comment tout cela a été calculé. Il est possible de cacher le code (dans un document généré depuis un Notebook R), ou d’indiquer une directive de compilation dans les chunks pour éviter que le code ne s’imprime dans le rapport final. Voyez les options en cliquant sur le petit engrenage à côté de la flèche verte en haut à droite du chunk. Consultez l’aide-mémoire de R Markdown accessible à partir du menu RStudio Help -&gt; Cheatsheets -&gt; R Markdown Reference Guide, voir chunk options p.2-3 pour plus d’informations sur les nombreuses options disponibles. Par exemple, en ajoutant la directive echo=FALSE dans la balise d’entrée d’un chunk (```{r, echo=FALSE}), on empèche d’imprimer le code de ce chunk dans le rapport. Notez que sur la droite du bouton Preview ou Knit, vous avez un autre bouton représenté par un petit engrenage. Il donne accès à un menu déroulant qui vous donne la possibilité de modifier la façon de générer vos rapports. L’entrée tout en bas Output Options... permet de paramétrer la présentation du rapport. Si vous cliquez sur la petite flèche noire pointant vrs le bas juste après Preview ou Knit, vous avez un autre menu déroulant qui donne accès aux différents formats possibles : HTML, PDF, Word, etc. Essayez les différentes options pour visualiser comment votre rapport se présente dans les différents formats. N’éditer jamais à la main un fichier .Rproj. Laisser RStudio s’en occuper tout seul.↩ Les systèmes d’édition professionnels dissocient en effet le fond de la forme : vous rédiger d’abord le contenu, et ensuite, vous indiquer le style à lui appliquer.↩ "],
["github-1.html", "B.2 GitHub", " B.2 GitHub TODO "],
["github-classroom.html", "B.3 GitHub Classroom", " B.3 GitHub Classroom TODO "],
["learnr.html", "C Tutoriels “learnr”", " C Tutoriels “learnr” En complément de ce syllabus, vous allez utiliser également des tutoriels interactifs construits avec learnr. Si ce n’est déjà fait, commencez par installer ces tutoriels dans votre SciViews Box (voir A.3.3). Entrez l’instruction ci-dessous dans la fenêtre Console de RStudio suivie de la touche Entrée : BioDataScience::run() La liste des tutoriels vous est proposée (notez que ces tutoriels comme les autres outils pédagogiques sont encore en cours de développement, mais plusieurs vous seront disponibles): 1: 02a_base 2: 02b_decouverte 3: 02c_nuage_de_points 4: 02d_test ... Entrez le numéro correspondant au tutoriel que vous voulez exécuter et un document interactif apparait. Par exemple, en entrant 1 suivi de la touche Entrée, vous êtes redirigé vers le tutoriel concernant les base de R et intitulé 02a_base. La première chose à vérifier à l’ouverture du tutoriel interactif est le nom d’utilisateur (équivalent à votre username dans Github) et votre adresse email (adresse email associée à votre compte Github ou de votre compte d’étudiant). En effet, votre progression sera enregistrée, mais cela ne peut se faire que si vous renseignez ces données correctement avant de travailler dans le tutoriel “learnr”. Le learnr est un outil pédagogique mis au point afin de proposer des tutoriaux interactifs comprennant des illustrations, des questions à choix multiples, des exercices R, … Les learnr qui vous seront proposés tout au long de votre formation seront composé de la manière suivante : Objectif Introduction Une série d’exercices Conclusion Vous retrouvez d’ailleurs cette structure en haut à gauche de ce dernier. Chaque page du tutoriel est importante et nécessite votre attention. Objectifs Cette section va détailler l’ensemble des notions que vous allez apprendre à maitriser durant ce tutoriel. Dans le cadre de ce premier tutoriel, l’objectif est de découvrir les bases du language R. Introduction Cette section va vous replacer dans le contexte du tutoriel interactif avec un rappel succinct des notions théoriques indispensables afin de répondre à la série d’exercices. Cette section ne remplace pas les autres matériels pédagogiques qui vous sont proposés. Vous devez travailler dans l’ordre proposé dans le présent manuel au sein de chaque module pour vous préparer correction au learnr de test en fin de section. Une serie d’exercices Cette section peut être de longueur très variable en fonction de la difficulté et des notions à appréhender. Des zones de codes R vous sont proposées dans les exercices. Elles vous permettent d’expérimenter directement des instructions dans R depuis le document learnr. Pour exécuter ces instructions, il faut cliquer sur Run Code. Vous pouvez le faire autant de fois que vous voulez. Modifiez le code, cliquez Run Code, analysez le résultat, modifiez votre code, recliquez Run Code, etc… juqu’à ce que vous soyez satisfait du résultat. Finissez l’exercice et soumettez votre réponse en cliquant sur Submit Answer. Des boutons Hint, lorsqu’ils sont présents, vous proposent des aides si vous êtes bloqués. Les boutons Solution… montrent ce qu’il fallait entrer. N’allez pas voir directement la solution. Essayez d’abord par vous même ! Conclusion Cette section termine ce tutoriel et propose de laisser des commentaires avec l’utilisation de dièse #. Fermez le tutoriel lorsqu’il est terminé. En retournant dans RStudio, vous devez déconnecter votre process R qui est toujours occupé à servir la page learnr du tutoriel. Pour cela, placez le curseur dans la fenêtre Console et cliquez sur la touche Esc. A ce moment, vous récupérez la main dans R et pouvez à nouveau retravailler normalement dans RStudio. "],
["redaction-scientifique.html", "D Rédaction scientifique", " D Rédaction scientifique La rédaction de textes scientifiques doit respecter un certain caneva et différentes règles qui sont résumés dans cette annexe. Pour en savoir plus Recherche documentaire et aide à la création (ReDAC). L’Université de Mons met à disposition de ses étudiants un cours en ligne qui rassemble un maximum de renseignements sur la rédaction de rapports scientifiques. "],
["organisation.html", "D.1 Organisation", " D.1 Organisation Un rapport scientifique respecte généralement le schéma suivant : Table des matières (facultatif) Introduction But Matériel et méthodes Résultats Discussion Conclusion Bibliographie Annexe(s) (si nécessaire) Pour des travaux de plus grande ampleur comme les travaux de fin d’études, le schéma ci-dessus est adapté, et éventuellement divisé en chapitres, en y ajoutant généralement une partie remerciement en début de manuscrit, ainsi qu’une liste des figures, des tables, des abbréviations utilisées, voire un index en fin d’ouvrage. "],
["contenu.html", "D.2 Contenu", " D.2 Contenu Le rapport sert à restituer de façon synthétique les résultats d’une étude scientifique, et les interprétations. Le tout est remis dans le contexte de la bibliographie existante en la synthétisant dans l’introduction et en comparant les résultats avec d’autres études connexes dans la discussion. Il faut garder à l’esprit qu’un lecteur doit comprendre l’intégralité du rapport avec un minimum de connaissances a priori sur l’étude réalisée, mais avec des connaissances générales dans la spécialité. Donc, un rapport sur un sujet biologique est adressé à un lecteur biologiste pour lequel il ne faut pas rappeler les concepts de base dans sa discipline. Par contre, il faut expliquer avec suffisamment de détails comment l’étude a été réalisée dans la section “matériel et méthodes”. En général, les phrases sont simples, directes, courtes et précises (veuillez à utiliser le vocabulaire adéquat et précis). Les explications sont, autant que possible, linéaires. Evitez les renvois dans différentes autres parties du rapport, si ce n’est pour rappeler un élément évoqué plus haut, ou pour se référer à une figure ou une table. A ce sujet, les figures (dont les images, photos, schémas et graphiques) sont numérotées (Figure 1, Figure 2, …) et accompagnées d’une légende en dessous d’elles. La figure et sa légende doivent être compréhensibles telles quelles. Dans le texte, vous pourrez alors vous référer à la figure, par exemple: “Tel phénomène est observable (voir Fig. 3)”, ou “La Fig. 4 montre …”. idem pour les tableaux qui sont également numérotés (Tableau 1, Tableau 2, …) et légendés, mais au dessus du tableau. Les règles de lisibilité du tableau + légende et de renvoi vers les tableaux sont identiques que pour les figures. Les équations peuvent aussi être numérotées et des renvois de type (eq. 5) peuvent être alors utilisés. Enfin, toute affirmation doit être soit démontrée dans le rapport, soit complétée d’une citation vers un autre document scientifique qui la démontre. La partie bibliographie regroupe la liste de tous les documents qui sont ainsi cités à la fin du rapport. Veuillez à respecter les notations propres au système métrique international, les abbrévations usuelles dans la discipline, et le droit d’auteur et les licenses si vous voulez citer un passage ou reprendre une illustration provenant d’un autre auteur (sans omettre d’indiquer qui en est l’auteur). Enfin, en vue de rendre le document parfaitement reproductible, vous pouvez indiquer dans les annexes où trouver la source (le document .Rmd) et les données analysées. Vous pouvez également terminer avec un chunk qui renseigne de l’état du système R utilisé, y compris l’ensemble des packages employés. Ce chunk, présenté en annexe, contiendra l’instruction utils::sessionInfo(), ou mieux : xfun::session_info() (version courte) ou devtools::session_info() (version longue). Par exemple : xfun::session_info() R version 3.4.4 (2018-03-15) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS High Sierra 10.13.4 Locale: fr_BE.UTF-8 / fr_BE.UTF-8 / fr_BE.UTF-8 / C / fr_BE.UTF-8 / fr_BE.UTF-8 Package version: acepack_1.4.1 anytime_0.3.1 assertthat_0.2.0 backports_1.1.2 base64enc_0.1-3 BH_1.66.0.1 bindr_0.1.1 bindrcpp_0.2.2 bookdown_0.7 broom_0.5.0 callr_3.0.0 cellranger_1.1.0 chart_1.2.0 checkmate_1.8.5 cli_1.0.1 clipr_0.4.1 cluster_2.0.7-1 codetools_0.2-15 colorspace_1.3-2 compiler_3.4.4 cowplot_0.9.3 crayon_1.3.4 curl_3.2 data.io_1.2.1 data.table_1.11.4 datasets_3.4.4 DBI_1.0.0 dbplyr_1.2.2 digest_0.6.18 dplyr_0.7.8 ellipse_0.4.1 evaluate_0.12 fansi_0.3.0 flow_1.1.0 forcats_0.3.0 foreign_0.8-71 Formula_1.2-3 fs_1.2.6 ggplot2_2.2.1 ggplotify_0.0.3 ggpubr_0.1.8 ggrepel_0.8.0 ggsci_2.9 ggsignif_0.4.0 glue_1.3.0 graphics_3.4.4 grDevices_3.4.4 grid_3.4.4 gridExtra_2.3 gridGraphics_0.3-0 gtable_0.2.0 haven_1.1.2 highr_0.7 Hmisc_4.1-1 hms_0.4.2 htmlTable_1.12 htmltools_0.3.6 htmlwidgets_1.3 httr_1.3.1 igraph_1.2.2 jsonlite_1.5 knitr_1.20 labeling_0.3 lattice_0.20-35 latticeExtra_0.6-28 lazyeval_0.2.1 lubridate_1.7.4 magrittr_1.5 markdown_0.8 MASS_7.3-50 Matrix_1.2-14 methods_3.4.4 mime_0.6 modelr_0.1.2 munsell_0.5.0 nlme_3.1-137 nnet_7.3-12 nycflights13_1.0.0 openssl_1.0.2 pillar_1.3.0 pkgconfig_2.0.2 plogr_0.2.0 plyr_1.8.4 polynom_1.3.9 processx_3.2.0 proto_1.0.0 pryr_0.1.4 ps_1.1.0 purrr_0.2.5 R6_2.3.0 RApiDatetime_0.0.3 RColorBrewer_1.1-2 Rcpp_1.0.0 readr_1.1.1 readxl_1.1.0 rematch_1.0.1 reprex_0.2.1 reshape2_1.4.3 rlang_0.3.0.9000 rmarkdown_1.10 rpart_4.1-13 rprojroot_1.3-2 rstudioapi_0.8 rvcheck_0.1.0 rvest_0.3.2 scales_1.0.0 SciViews_1.1.0 selectr_0.4.1 splines_3.4.4 stats_3.4.4 stringi_1.2.4 stringr_1.3.1 survival_2.42-6 svMisc_1.1.0 tibble_1.4.2 tidyr_0.8.2 tidyselect_0.2.5 tidyverse_1.2.1 tinytex_0.9 tools_3.4.4 tsibble_0.5.3 utf8_1.1.4 utils_3.4.4 viridis_0.5.1 viridisLite_0.3.0 whisker_0.3.2 withr_2.1.2 xfun_0.4 xml2_1.2.0 yaml_2.2.0 D.2.1 Table des matières La table des matières est d’une importance capitale pour un long document (mais facultative pour un plus court rapport) afin de présenter la structure de votre oeuvre aux lecteurs. Heureusement, il n’est pas nécessaire de l’écrire manuellement. La table des matières est générée automatiquement dans un rapport R Markdown. L’instruction à ajouter dans le préambule du document R Notebook afin d’obtenir une table des matières est toc: yes (ne l’encodez pas directement, mais sélectionnez l’option Include table of contents dans les options de formattage du document accessibles à partir du bouton engrenage à droite de Preview ou Knit -&gt; Output Options...). Lorsque vous fermerez cette boite de dialogue de configuration, l’entrée ad hoc sera ajoutée pour vous dans le préambule. Vous pouvez aussi choisir de numéroter vos titres automatiquement. L’instruction à ajouter en plus de toc: yes dans le préambule du document R Notebook afin d’obtenir une table des matières avec des titres numéroté est number_sections: yes. Encore une fois, passez par la boite de dialogue de configuration, et cochez-y l’entrée Number section headings. Voyez l’animation ci-dessous pour accéder à la boite de dialogue de configuration du document R Markdown/R Notebook. D.2.2 Introduction L’introduction d’un rapport (ou d’un mémoire) a pour principal objectif de replacer l’étude scientifique réalisée dans son contexte. La règle la plus importante est qu’un lecteur n’ayant jamais entendu parler de cette étude doit comprendre l’intégralité du rapport. L’introduction doit donc permettre de : Remettre l’expérience dans son contexte, Décrire l’organisme étudié caractéristiques générales de l’organisme, distribution géographique, biotope,… Notez que l’ajout d’images ou d’une carte de distribution est un plus dans l’introduction. D.2.3 But Le but permet de synthétiser la question posée dans cette étude en fonction du contexte de l’expérience expliqué dans l’introduction. D.2.4 Matériel &amp; méthodes La section matériel &amp; méthodes permet de décrire les aspects techniques de l’étude comme le matériel employé et les méthodes mises en oeuvre (protocoles des manipulations et des mesures effectuées) afin d’acquérir les données. Cette section est également le lieu de description des techniques statistiques utilisées pour analyser les données, des programmes informatiques employés, … D.2.5 Résultats Les résultats vont généralement contenir deux parties : La description des données, via l’exploration des données récoltées (avec graphiques et/ou estimateurs statistiques) L’application des outils statistiques pertinents pour répondre à la question posée D.2.6 Discussion Cette section comprend l’interprétation biologique des résultats et la remise dans un contexte plus général, notamment en les comparant à des observations connexes réalisées par d’autres auteurs scientifiques. Il est d’une importance capitale d’avoir un regard critique sur les résultats obtenus. Cette mise en contexte aide en ce sens. D.2.7 Conclusion(s) Cette section va résumer les principales implications à retenir de notre étude et, éventuellement, proposer des perspectives afin de poursuivre la recherche dans cette thématique. D.2.8 Bibliographie (ou références) La rédaction de travaux s’appuye toujours sur une recherche bibliographique au préalable. CIl faut documenter convenablement les sources bibliographiques au sein de cette section afin d’éviter le plagiat volontaire ou involontaire. Une multitude de programmes existent pour faciliter la gestion de votre base de données bibliographique comme Mendeley, Zotero ou encore Endnote. Pour générer correctement ses références bibliographiques dans un document R Markdown/R Notebook, consulter ceci. Il s’agit d’un manuel en anglais de RStudio qui explique comment faire dans le détail. "],
["nom-des-especes.html", "D.3 Nom des espèces", " D.3 Nom des espèces Le nom complet d’une espèce en biologie suit une convention particulière, propre à la nomenclature binomiale de Linné que vous devez utiliser dans tous vos travaux. Partons de l’exemple de l’oursin violet. Il s’agit ici du nom vernaculaire en français. Mais ce nom n’est pas assez précis pour être utilisé seul dans un travail scientifique. En effet, le nom vernaculaire d’une espèce change d’une langue à l’autre. Il peut aussi varier d’une région géographique à l’autre, ou pire, il peut désigner des espèces différentes selon les endroits. Seul le nom latin fait référence ! Une espèce est classée de la manière suivante (les niveaux de classification les plus importants sont mis en gras) : Règne : Animalia Embranchement : Echinodermata Sous-Embranchement : Echinozoa Classe : Echinoidea Sous-classe : Euechinoidea Super-ordre : Echinacea Ordre : Camarodonta Infra-ordre : Echinidae Famille : Parachinidae Genre : Paracentrotus Espèce : lividus Afin de former le nom binomial de l’oursin violet, on utilise le genre et l’espèce de la classification ci-dessus : Paracentrotus lividus En toute rigueur, il faut aussi associer le nom du naturaliste qui a nommé et décrit l’espèce et l’année de la publication de la description (on parle de diagnose en biologie), et ce, uniquement la première fois qu’on cite cette espèce dans notre rapport. Paracentrotus lividus Lamarck 1816 Lors de la première citation d’une espèce, et certainement dans le titre ou le résumé, il est indispensable de spécifier le nom latin complet de l’espèce (genre espèce) qui pourra être éventuellement abbrégé par la suite en indiquant la première lettre du genre. Dans l’exemple cité, on pourra écrire ensuite P. lividus plus loin dans le texte (pour autant que cela ne prête pas à confusion, bien sûr). "],
["references.html", "Références", " Références "]
]
